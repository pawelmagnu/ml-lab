{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc572964-c993-4741-b581-488033f5acc5",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc572964-c993-4741-b581-488033f5acc5",
        "outputId": "70bd862a-75df-4369-b854-4ee6b0b8f2d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=3 in /usr/local/lib/python3.7/dist-packages (from scikeras) (4.13.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikeras) (1.0.2)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.7/dist-packages (from scikeras) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3->scikeras) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=0.21->scikeras) (3.0.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "!pip install scikeras\n",
        "import scikeras\n",
        "from scikeras.wrappers import KerasRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b24466e1-9232-46e1-88b2-5237ce7a86ac",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b24466e1-9232-46e1-88b2-5237ce7a86ac",
        "outputId": "0af27b51-dd50-4932-c285-3062daa03eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5dd53ac5-4bb5-4a0e-ba8b-b8faca221a70",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5dd53ac5-4bb5-4a0e-ba8b-b8faca221a70"
      },
      "outputs": [],
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(patience=10,min_delta=1.0,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a604b986-d9ff-4c18-83d6-14d06ca2385b",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "a604b986-d9ff-4c18-83d6-14d06ca2385b"
      },
      "outputs": [],
      "source": [
        "def get_callback(filename,name,value):\n",
        "    root_logdir = os.path.join(os.curdir, filename)\n",
        "    ts = int(time.time())\n",
        "    filen = str(ts)+'_'+str(name)+'_'+str(value)\n",
        "    return tf.keras.callbacks.TensorBoard(os.path.join(root_logdir, filen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "85f95b73-3077-4438-bf3b-69cb64073c2a",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "85f95b73-3077-4438-bf3b-69cb64073c2a"
      },
      "outputs": [],
      "source": [
        "def build_model(n_hidden, n_neurons, optimizer, learning_rate, momentum=0):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(keras.layers.Input(shape=13))\n",
        "    for i in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
        "    if optimizer == 'sgd': \n",
        "        opt = keras.optimizers.SGD(lr=learning_rate,nesterov=False)\n",
        "    elif optimizer == 'nesterov':\n",
        "        opt = keras.optimizers.SGD(lr=learning_rate,nesterov=True)\n",
        "    elif optimizer == 'momentum':\n",
        "        opt = keras.optimizers.SGD(lr=learning_rate,nesterov=False,momentum=momentum)\n",
        "    elif optimizer == 'adam':\n",
        "        opt = keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss='mse',optimizer=opt,metrics=['mse','mae'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ade9841c-290c-455b-b266-f32fbd26ece0",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ade9841c-290c-455b-b266-f32fbd26ece0"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7821be36-dba4-4066-85fe-bf904e71c1d6",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7821be36-dba4-4066-85fe-bf904e71c1d6",
        "outputId": "5abb1248-3b02-49ea-fed4-652d0299aed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 89: early stopping\n",
            "Epoch 22: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1e-06, 298.61773681640625, 13.905632972717285),\n",
              " (1e-05, 337.1399230957031, 14.659549713134766),\n",
              " (0.0001, 549.4698486328125, 21.24028205871582)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "learning = [0.000001,0.00001,0.0001]\n",
        "lr = []\n",
        "for rate in learning:\n",
        "    model = build_model(1,25,'sgd',rate)\n",
        "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','lr',rate),es],verbose=0)\n",
        "    result = (rate, history.history['loss'][-1], history.history['mae'][-1])\n",
        "    lr.append(result)\n",
        "lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ae6968e5-8578-439d-a4d2-be6e4207b0d2",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae6968e5-8578-439d-a4d2-be6e4207b0d2",
        "outputId": "06827755-3996-48be-e263-193cce892530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: early stopping\n",
            "Epoch 82: early stopping\n",
            "Epoch 100: early stopping\n",
            "Epoch 36: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 23378.01953125, 72.62944793701172),\n",
              " (1, 404.4757080078125, 16.748228073120117),\n",
              " (2, 229.35877990722656, 11.249618530273438),\n",
              " (3, 411.98870849609375, 16.886764526367188)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "hidden = [0,1,2,3]\n",
        "hl = []\n",
        "for hid in hidden:\n",
        "    model = build_model(hid,25,'sgd',1e-05)\n",
        "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','hl',hid),es],verbose=0)\n",
        "    result = (hid, history.history['loss'][-1], history.history['mae'][-1])\n",
        "    hl.append(result)\n",
        "hl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "edaf6ff1-c500-42b8-99e0-58b4ee977c1b",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edaf6ff1-c500-42b8-99e0-58b4ee977c1b",
        "outputId": "249c306c-7a04-4fa8-da83-0099ebcce7fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46: early stopping\n",
            "Epoch 67: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(5, 288.40496826171875, 13.205202102661133),\n",
              " (25, 407.7099609375, 16.8358211517334),\n",
              " (125, 288.2408752441406, 13.254331588745117)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "neurons = [5,25,125]\n",
        "nn = []\n",
        "for neuron in neurons:\n",
        "    model = build_model(1,neuron,'sgd',1e-05)\n",
        "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','nn',neuron),es],verbose=0)\n",
        "    result = (neuron, history.history['loss'][-1], history.history['mae'][-1])\n",
        "    nn.append(result)\n",
        "nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "547a394b-5c4c-4516-9551-fb495169e75f",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "547a394b-5c4c-4516-9551-fb495169e75f",
        "outputId": "41ef9e04-3a56-4d63-e453-33415e650b30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sgd', 368.9898986816406, 15.68124771118164),\n",
              " ('nesterov', 474.77978515625, 18.92207145690918),\n",
              " ('momentum', 313.5940856933594, 13.945269584655762),\n",
              " ('adam', 5499.271484375, 50.31266784667969)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "opti = ['sgd','nesterov','momentum','adam']\n",
        "opt = []\n",
        "for optimizer in opti:\n",
        "    model = build_model(1,25,optimizer,1e-05,0.5)\n",
        "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','opt',optimizer),es],verbose=0)\n",
        "    result = (optimizer, history.history['loss'][-1], history.history['mae'][-1])\n",
        "    opt.append(result)\n",
        "opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "76781afb-5280-49eb-a375-65a98a2b82fc",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76781afb-5280-49eb-a375-65a98a2b82fc",
        "outputId": "a6570e39-9c17-408b-fa7f-9f5def687c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99: early stopping\n",
            "Epoch 46: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.1, 290.4100341796875, 13.356572151184082),\n",
              " (0.5, 393.7789306640625, 16.28229331970215),\n",
              " (0.9, 451.1329650878906, 18.036165237426758)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "moment = [0.1,0.5,0.9]\n",
        "mom = []\n",
        "for momentum in moment:\n",
        "    model = build_model(1,25,'momentum',1e-05,momentum)\n",
        "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','mom',momentum),es],verbose=0)\n",
        "    result = (momentum, history.history['loss'][-1], history.history['mae'][-1])\n",
        "    mom.append(result)\n",
        "mom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "dbad199a-b738-417a-8ebe-32df82f777a3",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dbad199a-b738-417a-8ebe-32df82f777a3"
      },
      "outputs": [],
      "source": [
        "with open('lr.pkl','wb') as f:\n",
        "    pickle.dump(lr,f)\n",
        "with open('hl.pkl','wb') as f:\n",
        "    pickle.dump(hl,f)\n",
        "with open('nn.pkl','wb') as f:\n",
        "    pickle.dump(nn,f)\n",
        "with open('opt.pkl','wb') as f:\n",
        "    pickle.dump(opt,f)\n",
        "with open('mom.pkl','wb') as f:\n",
        "    pickle.dump(mom,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6ea6dcdd-b76b-40bf-ab6f-8045c648223e",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "6ea6dcdd-b76b-40bf-ab6f-8045c648223e"
      },
      "outputs": [],
      "source": [
        "param_distribs = {\n",
        "\"model__n_hidden\": [0,1,2,3],\n",
        "\"model__n_neurons\": [5,25,125],\n",
        "\"model__learning_rate\": [0.000001,0.00001,0.0001],\n",
        "\"model__optimizer\": ['sgd','nesterov', 'momentum','adam'],\n",
        "\"model__momentum\": [0.1,0.5,0.9]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "86721a4e-529f-4d6f-b160-82ddc10818c5",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "86721a4e-529f-4d6f-b160-82ddc10818c5"
      },
      "outputs": [],
      "source": [
        "keras_reg = KerasRegressor(build_model, callbacks=[es])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4fb9da51-b3e8-420d-b627-9777a1c37548",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fb9da51-b3e8-420d-b627-9777a1c37548",
        "outputId": "d533b295-a1dc-4973-f154-0c1fbc5f4eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 56ms/step - loss: 510.0782 - mse: 510.0782 - mae: 19.8547 - val_loss: 327.7150 - val_mse: 327.7150 - val_mae: 16.1038\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 471.9808 - mse: 471.9808 - mae: 18.8708 - val_loss: 329.8615 - val_mse: 329.8615 - val_mae: 16.0789\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 445.7855 - mse: 445.7855 - mae: 18.0715 - val_loss: 317.0804 - val_mse: 317.0804 - val_mae: 15.4875\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 433.0125 - mse: 433.0125 - mae: 17.6360 - val_loss: 314.4315 - val_mse: 314.4315 - val_mae: 15.5062\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 418.3333 - mse: 418.3333 - mae: 17.2059 - val_loss: 287.6233 - val_mse: 287.6233 - val_mae: 14.3559\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 407.9185 - mse: 407.9185 - mae: 16.9596 - val_loss: 291.8539 - val_mse: 291.8539 - val_mae: 14.6518\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 397.2549 - mse: 397.2549 - mae: 16.4733 - val_loss: 280.2048 - val_mse: 280.2048 - val_mae: 13.9511\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 401.2432 - mse: 401.2432 - mae: 16.7033 - val_loss: 282.1749 - val_mse: 282.1749 - val_mae: 14.1419\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 376.5662 - mse: 376.5662 - mae: 15.7844 - val_loss: 275.3293 - val_mse: 275.3293 - val_mae: 14.1283\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 403.5771 - mse: 403.5771 - mae: 16.9705 - val_loss: 267.8712 - val_mse: 267.8712 - val_mae: 13.5942\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 365.9253 - mse: 365.9253 - mae: 15.5824 - val_loss: 267.5854 - val_mse: 267.5854 - val_mae: 13.5906\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 370.8924 - mse: 370.8924 - mae: 15.7771 - val_loss: 242.0033 - val_mse: 242.0033 - val_mae: 12.6172\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 338.8614 - mse: 338.8614 - mae: 14.6632 - val_loss: 264.5946 - val_mse: 264.5946 - val_mae: 13.7870\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 353.4474 - mse: 353.4474 - mae: 15.3595 - val_loss: 238.2844 - val_mse: 238.2844 - val_mae: 12.4665\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 335.3766 - mse: 335.3766 - mae: 14.5650 - val_loss: 232.8445 - val_mse: 232.8445 - val_mae: 12.1676\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 335.1751 - mse: 335.1751 - mae: 14.5596 - val_loss: 238.1500 - val_mse: 238.1500 - val_mae: 12.4833\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 334.0725 - mse: 334.0725 - mae: 14.4191 - val_loss: 233.9503 - val_mse: 233.9503 - val_mae: 12.2292\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 330.7891 - mse: 330.7891 - mae: 14.3245 - val_loss: 231.8185 - val_mse: 231.8185 - val_mae: 12.0576\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 328.1929 - mse: 328.1929 - mae: 14.2247 - val_loss: 232.6169 - val_mse: 232.6169 - val_mae: 12.1521\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 329.6829 - mse: 329.6829 - mae: 14.2360 - val_loss: 234.8259 - val_mse: 234.8259 - val_mae: 12.3325\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 328.2345 - mse: 328.2345 - mae: 14.2959 - val_loss: 229.9910 - val_mse: 229.9910 - val_mae: 12.0105\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 330.0374 - mse: 330.0374 - mae: 14.2631 - val_loss: 231.8925 - val_mse: 231.8925 - val_mae: 12.0733\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 327.1134 - mse: 327.1134 - mae: 14.1117 - val_loss: 232.7791 - val_mse: 232.7791 - val_mae: 12.2358\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 325.9888 - mse: 325.9888 - mae: 14.0930 - val_loss: 228.4366 - val_mse: 228.4366 - val_mae: 11.9052\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 326.8098 - mse: 326.8098 - mae: 14.1603 - val_loss: 230.6953 - val_mse: 230.6953 - val_mae: 12.0799\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 325.8672 - mse: 325.8672 - mae: 14.1152 - val_loss: 232.1516 - val_mse: 232.1516 - val_mae: 12.2206\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 326.4432 - mse: 326.4432 - mae: 14.1371 - val_loss: 225.7630 - val_mse: 225.7630 - val_mae: 11.6060\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 330.6783 - mse: 330.6783 - mae: 14.3018 - val_loss: 226.3740 - val_mse: 226.3740 - val_mae: 11.6830\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 325.2683 - mse: 325.2683 - mae: 14.0297 - val_loss: 232.6634 - val_mse: 232.6634 - val_mae: 12.2557\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 324.5252 - mse: 324.5252 - mae: 14.0762 - val_loss: 224.8318 - val_mse: 224.8318 - val_mae: 11.5065\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 324.3365 - mse: 324.3365 - mae: 14.0289 - val_loss: 225.6365 - val_mse: 225.6365 - val_mae: 11.5467\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 324.7887 - mse: 324.7887 - mae: 13.9318 - val_loss: 234.3565 - val_mse: 234.3565 - val_mae: 12.3799\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 326.3009 - mse: 326.3009 - mae: 14.0990 - val_loss: 231.1310 - val_mse: 231.1310 - val_mae: 12.1210\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 322.8039 - mse: 322.8039 - mae: 13.9866 - val_loss: 229.2457 - val_mse: 229.2457 - val_mae: 11.9790\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 323.4306 - mse: 323.4306 - mae: 14.0330 - val_loss: 225.8468 - val_mse: 225.8468 - val_mae: 11.6727\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 324.3210 - mse: 324.3210 - mae: 14.0474 - val_loss: 224.6321 - val_mse: 224.6321 - val_mae: 11.5240\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 323.2278 - mse: 323.2278 - mae: 14.0013 - val_loss: 224.2093 - val_mse: 224.2093 - val_mae: 11.4561\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 324.4556 - mse: 324.4556 - mae: 14.0133 - val_loss: 228.7344 - val_mse: 228.7344 - val_mae: 11.7753\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 325.6258 - mse: 325.6258 - mae: 14.0733 - val_loss: 228.0413 - val_mse: 228.0413 - val_mae: 11.8793\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 324.7815 - mse: 324.7815 - mae: 14.1490 - val_loss: 227.8598 - val_mse: 227.8598 - val_mae: 11.8730\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 323.8671 - mse: 323.8671 - mae: 14.0377 - val_loss: 225.7841 - val_mse: 225.7841 - val_mae: 11.6608\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 322.7298 - mse: 322.7298 - mae: 13.9637 - val_loss: 225.5042 - val_mse: 225.5042 - val_mae: 11.6360\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 323.2656 - mse: 323.2656 - mae: 13.9645 - val_loss: 224.3540 - val_mse: 224.3540 - val_mae: 11.5076\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 322.6051 - mse: 322.6051 - mae: 13.9153 - val_loss: 225.0488 - val_mse: 225.0488 - val_mae: 11.6034\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 321.9189 - mse: 321.9189 - mae: 13.9682 - val_loss: 225.5086 - val_mse: 225.5086 - val_mae: 11.6446\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 322.1965 - mse: 322.1965 - mae: 13.9579 - val_loss: 224.0765 - val_mse: 224.0765 - val_mae: 11.4141\n",
            "Epoch 46: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 1271.6661 - mse: 1271.6661 - mae: 24.7646 - val_loss: 315.2581 - val_mse: 315.2581 - val_mae: 15.5960\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 440.9491 - mse: 440.9491 - mae: 17.5469 - val_loss: 287.0763 - val_mse: 287.0763 - val_mae: 14.2767\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 427.8048 - mse: 427.8048 - mae: 17.0387 - val_loss: 286.3997 - val_mse: 286.3997 - val_mae: 14.3102\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 422.3765 - mse: 422.3765 - mae: 16.8749 - val_loss: 285.3933 - val_mse: 285.3933 - val_mae: 14.2683\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.5194 - mse: 421.5194 - mae: 16.8195 - val_loss: 281.2551 - val_mse: 281.2551 - val_mae: 13.9023\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.0280 - mse: 421.0280 - mae: 16.7734 - val_loss: 280.3517 - val_mse: 280.3517 - val_mae: 13.8270\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 420.3327 - mse: 420.3327 - mae: 16.7256 - val_loss: 281.0923 - val_mse: 281.0923 - val_mae: 13.9343\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 422.5028 - mse: 422.5028 - mae: 16.7808 - val_loss: 283.7935 - val_mse: 283.7935 - val_mae: 14.1828\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 417.2372 - mse: 417.2372 - mae: 16.6078 - val_loss: 288.8313 - val_mse: 288.8313 - val_mae: 14.5380\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 417.4158 - mse: 417.4158 - mae: 16.6642 - val_loss: 286.3887 - val_mse: 286.3887 - val_mae: 14.3717\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 418.4656 - mse: 418.4656 - mae: 16.7123 - val_loss: 279.4449 - val_mse: 279.4449 - val_mae: 13.8012\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 417.1438 - mse: 417.1438 - mae: 16.6086 - val_loss: 283.3441 - val_mse: 283.3441 - val_mae: 14.1693\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 416.6584 - mse: 416.6584 - mae: 16.6215 - val_loss: 281.9165 - val_mse: 281.9165 - val_mae: 14.0508\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 418.1605 - mse: 418.1605 - mae: 16.7158 - val_loss: 279.4825 - val_mse: 279.4825 - val_mae: 13.8113\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 415.9425 - mse: 415.9425 - mae: 16.5658 - val_loss: 280.0923 - val_mse: 280.0923 - val_mae: 13.8757\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 416.0912 - mse: 416.0912 - mae: 16.5624 - val_loss: 279.8239 - val_mse: 279.8239 - val_mae: 13.8519\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 417.1494 - mse: 417.1494 - mae: 16.5849 - val_loss: 287.5470 - val_mse: 287.5470 - val_mae: 14.4684\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 418.5743 - mse: 418.5743 - mae: 16.6828 - val_loss: 279.5944 - val_mse: 279.5944 - val_mae: 13.8288\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 415.5572 - mse: 415.5572 - mae: 16.5101 - val_loss: 282.1790 - val_mse: 282.1790 - val_mae: 14.0777\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 416.3837 - mse: 416.3837 - mae: 16.5875 - val_loss: 284.4726 - val_mse: 284.4726 - val_mae: 14.2495\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 414.8126 - mse: 414.8126 - mae: 16.5746 - val_loss: 279.8365 - val_mse: 279.8365 - val_mae: 13.8559\n",
            "Epoch 21: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 523.6891 - mse: 523.6891 - mae: 18.7810 - val_loss: 334.5630 - val_mse: 334.5630 - val_mae: 15.3467\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 308.6869 - mse: 308.6869 - mae: 14.2052 - val_loss: 267.6744 - val_mse: 267.6744 - val_mae: 12.8947\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 262.1619 - mse: 262.1619 - mae: 12.7281 - val_loss: 203.3920 - val_mse: 203.3920 - val_mae: 10.9981\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 198.6542 - mse: 198.6542 - mae: 10.5950 - val_loss: 160.7829 - val_mse: 160.7829 - val_mae: 9.3482\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 164.6873 - mse: 164.6873 - mae: 9.3283 - val_loss: 145.0349 - val_mse: 145.0349 - val_mae: 8.6014\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 158.7353 - mse: 158.7353 - mae: 9.1225 - val_loss: 136.3226 - val_mse: 136.3226 - val_mae: 8.2331\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 154.5663 - mse: 154.5663 - mae: 8.9477 - val_loss: 156.3708 - val_mse: 156.3708 - val_mae: 9.4758\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 154.4781 - mse: 154.4781 - mae: 9.0009 - val_loss: 132.3943 - val_mse: 132.3943 - val_mae: 8.0835\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 147.9917 - mse: 147.9917 - mae: 8.5695 - val_loss: 132.5080 - val_mse: 132.5080 - val_mae: 8.0842\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 147.1955 - mse: 147.1955 - mae: 8.6143 - val_loss: 131.1747 - val_mse: 131.1747 - val_mae: 7.9864\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 147.1128 - mse: 147.1128 - mae: 8.5635 - val_loss: 129.6494 - val_mse: 129.6494 - val_mae: 7.9635\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 148.3597 - mse: 148.3597 - mae: 8.6111 - val_loss: 149.5252 - val_mse: 149.5252 - val_mae: 9.0337\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 155.8669 - mse: 155.8669 - mae: 9.0289 - val_loss: 131.0293 - val_mse: 131.0293 - val_mae: 7.9116\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 148.0634 - mse: 148.0634 - mae: 8.5633 - val_loss: 128.4116 - val_mse: 128.4116 - val_mae: 7.9992\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 144.9148 - mse: 144.9148 - mae: 8.4908 - val_loss: 128.6734 - val_mse: 128.6734 - val_mae: 7.9419\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 145.8469 - mse: 145.8469 - mae: 8.3810 - val_loss: 130.4294 - val_mse: 130.4294 - val_mae: 8.1788\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 145.8179 - mse: 145.8179 - mae: 8.6030 - val_loss: 129.4504 - val_mse: 129.4504 - val_mae: 7.9553\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 144.3088 - mse: 144.3088 - mae: 8.4180 - val_loss: 127.5799 - val_mse: 127.5799 - val_mae: 7.9511\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 143.6773 - mse: 143.6773 - mae: 8.4264 - val_loss: 130.7281 - val_mse: 130.7281 - val_mae: 7.9396\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 143.5729 - mse: 143.5729 - mae: 8.3236 - val_loss: 128.6730 - val_mse: 128.6730 - val_mae: 7.8819\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 145.5984 - mse: 145.5984 - mae: 8.4909 - val_loss: 127.7746 - val_mse: 127.7746 - val_mae: 7.9470\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 142.9239 - mse: 142.9239 - mae: 8.3223 - val_loss: 127.4522 - val_mse: 127.4522 - val_mae: 7.8963\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 141.1613 - mse: 141.1613 - mae: 8.3736 - val_loss: 136.4136 - val_mse: 136.4136 - val_mae: 8.2913\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 145.5075 - mse: 145.5075 - mae: 8.3403 - val_loss: 137.6667 - val_mse: 137.6667 - val_mae: 8.6485\n",
            "Epoch 24: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7145 - mse: 598.7145 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 750.5964 - mse: 750.5964 - mae: 23.5581 - val_loss: 658.9481 - val_mse: 658.9481 - val_mae: 22.1139\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 750.5106 - mse: 750.5106 - mae: 23.5569 - val_loss: 658.8378 - val_mse: 658.8378 - val_mae: 22.1125\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 750.4256 - mse: 750.4256 - mae: 23.5558 - val_loss: 658.7266 - val_mse: 658.7266 - val_mae: 22.1111\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 750.3388 - mse: 750.3388 - mae: 23.5547 - val_loss: 658.6168 - val_mse: 658.6168 - val_mae: 22.1097\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 750.2540 - mse: 750.2540 - mae: 23.5535 - val_loss: 658.5071 - val_mse: 658.5071 - val_mae: 22.1083\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 750.1675 - mse: 750.1675 - mae: 23.5524 - val_loss: 658.3990 - val_mse: 658.3990 - val_mae: 22.1070\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 750.0862 - mse: 750.0862 - mae: 23.5513 - val_loss: 658.2892 - val_mse: 658.2892 - val_mae: 22.1056\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 750.0041 - mse: 750.0041 - mae: 23.5502 - val_loss: 658.1786 - val_mse: 658.1786 - val_mae: 22.1042\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 749.9174 - mse: 749.9174 - mae: 23.5490 - val_loss: 658.0714 - val_mse: 658.0714 - val_mae: 22.1028\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 749.8326 - mse: 749.8326 - mae: 23.5479 - val_loss: 657.9653 - val_mse: 657.9653 - val_mae: 22.1015\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 749.7520 - mse: 749.7520 - mae: 23.5468 - val_loss: 657.8561 - val_mse: 657.8561 - val_mae: 22.1001\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 749.6671 - mse: 749.6671 - mae: 23.5457 - val_loss: 657.7483 - val_mse: 657.7483 - val_mae: 22.0988\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 749.5837 - mse: 749.5837 - mae: 23.5446 - val_loss: 657.6413 - val_mse: 657.6413 - val_mae: 22.0974\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 749.5026 - mse: 749.5026 - mae: 23.5435 - val_loss: 657.5327 - val_mse: 657.5327 - val_mae: 22.0960\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 749.4181 - mse: 749.4181 - mae: 23.5424 - val_loss: 657.4243 - val_mse: 657.4243 - val_mae: 22.0947\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 749.3348 - mse: 749.3348 - mae: 23.5412 - val_loss: 657.3173 - val_mse: 657.3173 - val_mae: 22.0933\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 749.2524 - mse: 749.2524 - mae: 23.5401 - val_loss: 657.2105 - val_mse: 657.2105 - val_mae: 22.0920\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 749.1678 - mse: 749.1678 - mae: 23.5390 - val_loss: 657.1047 - val_mse: 657.1047 - val_mae: 22.0906\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 749.0870 - mse: 749.0870 - mae: 23.5379 - val_loss: 656.9950 - val_mse: 656.9950 - val_mae: 22.0892\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 749.0031 - mse: 749.0031 - mae: 23.5368 - val_loss: 656.8864 - val_mse: 656.8864 - val_mae: 22.0878\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 748.9189 - mse: 748.9189 - mae: 23.5356 - val_loss: 656.7802 - val_mse: 656.7802 - val_mae: 22.0865\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 748.8361 - mse: 748.8361 - mae: 23.5345 - val_loss: 656.6735 - val_mse: 656.6735 - val_mae: 22.0851\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 748.7527 - mse: 748.7527 - mae: 23.5334 - val_loss: 656.5665 - val_mse: 656.5665 - val_mae: 22.0838\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 748.6702 - mse: 748.6702 - mae: 23.5323 - val_loss: 656.4586 - val_mse: 656.4586 - val_mae: 22.0824\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 748.5869 - mse: 748.5869 - mae: 23.5312 - val_loss: 656.3530 - val_mse: 656.3530 - val_mae: 22.0811\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 748.5048 - mse: 748.5048 - mae: 23.5301 - val_loss: 656.2461 - val_mse: 656.2461 - val_mae: 22.0797\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 748.4221 - mse: 748.4221 - mae: 23.5290 - val_loss: 656.1390 - val_mse: 656.1390 - val_mae: 22.0783\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 748.3408 - mse: 748.3408 - mae: 23.5279 - val_loss: 656.0292 - val_mse: 656.0292 - val_mae: 22.0770\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 748.2534 - mse: 748.2534 - mae: 23.5267 - val_loss: 655.9230 - val_mse: 655.9230 - val_mae: 22.0756\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 748.1706 - mse: 748.1706 - mae: 23.5256 - val_loss: 655.8148 - val_mse: 655.8148 - val_mae: 22.0742\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 748.0880 - mse: 748.0880 - mae: 23.5245 - val_loss: 655.7062 - val_mse: 655.7062 - val_mae: 22.0729\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 748.0038 - mse: 748.0038 - mae: 23.5233 - val_loss: 655.5989 - val_mse: 655.5989 - val_mae: 22.0715\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 747.9193 - mse: 747.9193 - mae: 23.5222 - val_loss: 655.4937 - val_mse: 655.4937 - val_mae: 22.0701\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 747.8381 - mse: 747.8381 - mae: 23.5211 - val_loss: 655.3851 - val_mse: 655.3851 - val_mae: 22.0688\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 747.7582 - mse: 747.7582 - mae: 23.5200 - val_loss: 655.2734 - val_mse: 655.2734 - val_mae: 22.0674\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 747.6694 - mse: 747.6694 - mae: 23.5189 - val_loss: 655.1689 - val_mse: 655.1689 - val_mae: 22.0660\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 747.5891 - mse: 747.5891 - mae: 23.5178 - val_loss: 655.0629 - val_mse: 655.0629 - val_mae: 22.0647\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 747.5060 - mse: 747.5060 - mae: 23.5167 - val_loss: 654.9564 - val_mse: 654.9564 - val_mae: 22.0633\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 747.4225 - mse: 747.4225 - mae: 23.5156 - val_loss: 654.8505 - val_mse: 654.8505 - val_mae: 22.0620\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 747.3412 - mse: 747.3412 - mae: 23.5145 - val_loss: 654.7425 - val_mse: 654.7425 - val_mae: 22.0606\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 747.2588 - mse: 747.2588 - mae: 23.5133 - val_loss: 654.6340 - val_mse: 654.6340 - val_mae: 22.0592\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 747.1745 - mse: 747.1745 - mae: 23.5122 - val_loss: 654.5276 - val_mse: 654.5276 - val_mae: 22.0579\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 747.0942 - mse: 747.0942 - mae: 23.5112 - val_loss: 654.4196 - val_mse: 654.4196 - val_mae: 22.0565\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 747.0113 - mse: 747.0113 - mae: 23.5101 - val_loss: 654.3130 - val_mse: 654.3130 - val_mae: 22.0551\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 746.9269 - mse: 746.9269 - mae: 23.5089 - val_loss: 654.2083 - val_mse: 654.2083 - val_mae: 22.0538\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 746.8447 - mse: 746.8447 - mae: 23.5079 - val_loss: 654.1028 - val_mse: 654.1028 - val_mae: 22.0524\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 746.7628 - mse: 746.7628 - mae: 23.5067 - val_loss: 653.9958 - val_mse: 653.9958 - val_mae: 22.0511\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 746.6819 - mse: 746.6819 - mae: 23.5056 - val_loss: 653.8867 - val_mse: 653.8867 - val_mae: 22.0497\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 746.5985 - mse: 746.5985 - mae: 23.5045 - val_loss: 653.7796 - val_mse: 653.7796 - val_mae: 22.0483\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 746.5159 - mse: 746.5159 - mae: 23.5034 - val_loss: 653.6740 - val_mse: 653.6740 - val_mae: 22.0470\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 746.4337 - mse: 746.4337 - mae: 23.5023 - val_loss: 653.5670 - val_mse: 653.5670 - val_mae: 22.0456\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 746.3500 - mse: 746.3500 - mae: 23.5012 - val_loss: 653.4609 - val_mse: 653.4609 - val_mae: 22.0442\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 746.2692 - mse: 746.2692 - mae: 23.5001 - val_loss: 653.3531 - val_mse: 653.3531 - val_mae: 22.0429\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 746.1849 - mse: 746.1849 - mae: 23.4990 - val_loss: 653.2479 - val_mse: 653.2479 - val_mae: 22.0415\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 746.1027 - mse: 746.1027 - mae: 23.4979 - val_loss: 653.1417 - val_mse: 653.1417 - val_mae: 22.0402\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 746.0212 - mse: 746.0212 - mae: 23.4968 - val_loss: 653.0351 - val_mse: 653.0351 - val_mae: 22.0388\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.9390 - mse: 745.9390 - mae: 23.4957 - val_loss: 652.9301 - val_mse: 652.9301 - val_mae: 22.0375\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 745.8574 - mse: 745.8574 - mae: 23.4946 - val_loss: 652.8242 - val_mse: 652.8242 - val_mae: 22.0361\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.7759 - mse: 745.7759 - mae: 23.4935 - val_loss: 652.7178 - val_mse: 652.7178 - val_mae: 22.0347\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.6936 - mse: 745.6936 - mae: 23.4924 - val_loss: 652.6133 - val_mse: 652.6133 - val_mae: 22.0334\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.6119 - mse: 745.6119 - mae: 23.4913 - val_loss: 652.5095 - val_mse: 652.5095 - val_mae: 22.0321\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.5321 - mse: 745.5321 - mae: 23.4902 - val_loss: 652.4036 - val_mse: 652.4036 - val_mae: 22.0307\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.4504 - mse: 745.4504 - mae: 23.4891 - val_loss: 652.2968 - val_mse: 652.2968 - val_mae: 22.0294\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.3659 - mse: 745.3659 - mae: 23.4880 - val_loss: 652.1934 - val_mse: 652.1934 - val_mae: 22.0280\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 745.2872 - mse: 745.2872 - mae: 23.4869 - val_loss: 652.0860 - val_mse: 652.0860 - val_mae: 22.0266\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.2050 - mse: 745.2050 - mae: 23.4859 - val_loss: 651.9784 - val_mse: 651.9784 - val_mae: 22.0253\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.1208 - mse: 745.1208 - mae: 23.4847 - val_loss: 651.8740 - val_mse: 651.8740 - val_mae: 22.0239\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 745.0412 - mse: 745.0412 - mae: 23.4836 - val_loss: 651.7676 - val_mse: 651.7676 - val_mae: 22.0226\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 744.9578 - mse: 744.9578 - mae: 23.4825 - val_loss: 651.6641 - val_mse: 651.6641 - val_mae: 22.0212\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 744.8795 - mse: 744.8795 - mae: 23.4815 - val_loss: 651.5583 - val_mse: 651.5583 - val_mae: 22.0199\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 744.7961 - mse: 744.7961 - mae: 23.4803 - val_loss: 651.4540 - val_mse: 651.4540 - val_mae: 22.0185\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 744.7179 - mse: 744.7179 - mae: 23.4793 - val_loss: 651.3461 - val_mse: 651.3461 - val_mae: 22.0172\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 744.6327 - mse: 744.6327 - mae: 23.4782 - val_loss: 651.2411 - val_mse: 651.2411 - val_mae: 22.0158\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 744.5522 - mse: 744.5522 - mae: 23.4770 - val_loss: 651.1358 - val_mse: 651.1358 - val_mae: 22.0145\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 744.4696 - mse: 744.4696 - mae: 23.4760 - val_loss: 651.0320 - val_mse: 651.0320 - val_mae: 22.0131\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 744.3906 - mse: 744.3906 - mae: 23.4749 - val_loss: 650.9250 - val_mse: 650.9250 - val_mae: 22.0117\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 744.3088 - mse: 744.3088 - mae: 23.4738 - val_loss: 650.8179 - val_mse: 650.8179 - val_mae: 22.0104\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 744.2239 - mse: 744.2239 - mae: 23.4726 - val_loss: 650.7163 - val_mse: 650.7163 - val_mae: 22.0091\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 744.1458 - mse: 744.1458 - mae: 23.4716 - val_loss: 650.6107 - val_mse: 650.6107 - val_mae: 22.0077\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 744.0643 - mse: 744.0643 - mae: 23.4705 - val_loss: 650.5068 - val_mse: 650.5068 - val_mae: 22.0064\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 743.9839 - mse: 743.9839 - mae: 23.4694 - val_loss: 650.4026 - val_mse: 650.4026 - val_mae: 22.0050\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 743.9043 - mse: 743.9043 - mae: 23.4683 - val_loss: 650.2956 - val_mse: 650.2956 - val_mae: 22.0037\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 743.8200 - mse: 743.8200 - mae: 23.4672 - val_loss: 650.1903 - val_mse: 650.1903 - val_mae: 22.0023\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 743.7385 - mse: 743.7385 - mae: 23.4661 - val_loss: 650.0854 - val_mse: 650.0854 - val_mae: 22.0009\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 743.6573 - mse: 743.6573 - mae: 23.4650 - val_loss: 649.9797 - val_mse: 649.9797 - val_mae: 21.9996\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 743.5750 - mse: 743.5750 - mae: 23.4639 - val_loss: 649.8731 - val_mse: 649.8731 - val_mae: 21.9982\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 743.4959 - mse: 743.4959 - mae: 23.4628 - val_loss: 649.7657 - val_mse: 649.7657 - val_mae: 21.9968\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 743.4127 - mse: 743.4127 - mae: 23.4617 - val_loss: 649.6617 - val_mse: 649.6617 - val_mae: 21.9955\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 743.3344 - mse: 743.3344 - mae: 23.4606 - val_loss: 649.5555 - val_mse: 649.5555 - val_mae: 21.9941\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 743.2501 - mse: 743.2501 - mae: 23.4595 - val_loss: 649.4529 - val_mse: 649.4529 - val_mae: 21.9928\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 743.1704 - mse: 743.1704 - mae: 23.4584 - val_loss: 649.3486 - val_mse: 649.3486 - val_mae: 21.9915\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 743.0886 - mse: 743.0886 - mae: 23.4573 - val_loss: 649.2441 - val_mse: 649.2441 - val_mae: 21.9901\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 743.0121 - mse: 743.0121 - mae: 23.4562 - val_loss: 649.1370 - val_mse: 649.1370 - val_mae: 21.9887\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 742.9262 - mse: 742.9262 - mae: 23.4551 - val_loss: 649.0363 - val_mse: 649.0363 - val_mae: 21.9874\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 742.8450 - mse: 742.8450 - mae: 23.4540 - val_loss: 648.9346 - val_mse: 648.9346 - val_mae: 21.9861\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 742.7697 - mse: 742.7697 - mae: 23.4530 - val_loss: 648.8257 - val_mse: 648.8257 - val_mae: 21.9847\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 742.6853 - mse: 742.6853 - mae: 23.4519 - val_loss: 648.7217 - val_mse: 648.7217 - val_mae: 21.9834\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 742.6033 - mse: 742.6033 - mae: 23.4508 - val_loss: 648.6187 - val_mse: 648.6187 - val_mae: 21.9820\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 742.5234 - mse: 742.5234 - mae: 23.4497 - val_loss: 648.5133 - val_mse: 648.5133 - val_mae: 21.9807\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 742.4435 - mse: 742.4435 - mae: 23.4486 - val_loss: 648.4072 - val_mse: 648.4072 - val_mae: 21.9793\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 31ms/step - loss: 16302.2627 - mse: 16302.2627 - mae: 92.0910 - val_loss: 16446.5039 - val_mse: 16446.5039 - val_mae: 94.0426\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 16299.5342 - mse: 16299.5342 - mae: 92.0827 - val_loss: 16443.6523 - val_mse: 16443.6523 - val_mae: 94.0340\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 16296.8984 - mse: 16296.8984 - mae: 92.0745 - val_loss: 16440.7715 - val_mse: 16440.7715 - val_mae: 94.0253\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16294.2119 - mse: 16294.2119 - mae: 92.0663 - val_loss: 16437.9102 - val_mse: 16437.9102 - val_mae: 94.0166\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16291.5547 - mse: 16291.5547 - mae: 92.0580 - val_loss: 16435.0312 - val_mse: 16435.0312 - val_mae: 94.0080\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16288.8232 - mse: 16288.8232 - mae: 92.0498 - val_loss: 16432.1895 - val_mse: 16432.1895 - val_mae: 93.9994\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16286.1426 - mse: 16286.1426 - mae: 92.0416 - val_loss: 16429.3418 - val_mse: 16429.3418 - val_mae: 93.9908\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16283.4941 - mse: 16283.4941 - mae: 92.0334 - val_loss: 16426.4805 - val_mse: 16426.4805 - val_mae: 93.9821\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16280.7861 - mse: 16280.7861 - mae: 92.0251 - val_loss: 16423.6367 - val_mse: 16423.6367 - val_mae: 93.9735\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16278.1348 - mse: 16278.1348 - mae: 92.0169 - val_loss: 16420.7578 - val_mse: 16420.7578 - val_mae: 93.9648\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16275.4258 - mse: 16275.4258 - mae: 92.0086 - val_loss: 16417.9082 - val_mse: 16417.9082 - val_mae: 93.9562\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16272.7637 - mse: 16272.7637 - mae: 92.0004 - val_loss: 16415.0586 - val_mse: 16415.0586 - val_mae: 93.9476\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16270.1396 - mse: 16270.1396 - mae: 91.9922 - val_loss: 16412.1660 - val_mse: 16412.1660 - val_mae: 93.9388\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16267.4219 - mse: 16267.4219 - mae: 91.9839 - val_loss: 16409.3125 - val_mse: 16409.3125 - val_mae: 93.9302\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 16264.7275 - mse: 16264.7275 - mae: 91.9757 - val_loss: 16406.4668 - val_mse: 16406.4668 - val_mae: 93.9216\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 16262.0430 - mse: 16262.0430 - mae: 91.9675 - val_loss: 16403.6484 - val_mse: 16403.6484 - val_mae: 93.9131\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 16259.3779 - mse: 16259.3779 - mae: 91.9593 - val_loss: 16400.8203 - val_mse: 16400.8203 - val_mae: 93.9045\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16256.7432 - mse: 16256.7432 - mae: 91.9512 - val_loss: 16397.9473 - val_mse: 16397.9473 - val_mae: 93.8959\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16254.0566 - mse: 16254.0566 - mae: 91.9429 - val_loss: 16395.0605 - val_mse: 16395.0605 - val_mae: 93.8871\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 16251.3682 - mse: 16251.3682 - mae: 91.9346 - val_loss: 16392.1875 - val_mse: 16392.1875 - val_mae: 93.8785\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16248.6660 - mse: 16248.6660 - mae: 91.9264 - val_loss: 16389.3340 - val_mse: 16389.3340 - val_mae: 93.8698\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16246.0127 - mse: 16246.0127 - mae: 91.9182 - val_loss: 16386.4668 - val_mse: 16386.4668 - val_mae: 93.8611\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 16243.3076 - mse: 16243.3076 - mae: 91.9100 - val_loss: 16383.6357 - val_mse: 16383.6357 - val_mae: 93.8526\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16240.6689 - mse: 16240.6689 - mae: 91.9018 - val_loss: 16380.7959 - val_mse: 16380.7959 - val_mae: 93.8440\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16237.9883 - mse: 16237.9883 - mae: 91.8936 - val_loss: 16377.9756 - val_mse: 16377.9756 - val_mae: 93.8354\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16235.3125 - mse: 16235.3125 - mae: 91.8855 - val_loss: 16375.1787 - val_mse: 16375.1787 - val_mae: 93.8270\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16232.7041 - mse: 16232.7041 - mae: 91.8773 - val_loss: 16372.3252 - val_mse: 16372.3252 - val_mae: 93.8183\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16230.0654 - mse: 16230.0654 - mae: 91.8691 - val_loss: 16369.4521 - val_mse: 16369.4521 - val_mae: 93.8096\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16227.3574 - mse: 16227.3574 - mae: 91.8609 - val_loss: 16366.6455 - val_mse: 16366.6455 - val_mae: 93.8011\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16224.6953 - mse: 16224.6953 - mae: 91.8528 - val_loss: 16363.8418 - val_mse: 16363.8418 - val_mae: 93.7926\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16222.0771 - mse: 16222.0771 - mae: 91.8446 - val_loss: 16360.9883 - val_mse: 16360.9883 - val_mae: 93.7840\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16219.4424 - mse: 16219.4424 - mae: 91.8364 - val_loss: 16358.1309 - val_mse: 16358.1309 - val_mae: 93.7753\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16216.7461 - mse: 16216.7461 - mae: 91.8282 - val_loss: 16355.2988 - val_mse: 16355.2988 - val_mae: 93.7668\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16214.0938 - mse: 16214.0938 - mae: 91.8200 - val_loss: 16352.4629 - val_mse: 16352.4629 - val_mae: 93.7582\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16211.4385 - mse: 16211.4385 - mae: 91.8119 - val_loss: 16349.6377 - val_mse: 16349.6377 - val_mae: 93.7496\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16208.7559 - mse: 16208.7559 - mae: 91.8037 - val_loss: 16346.8291 - val_mse: 16346.8291 - val_mae: 93.7411\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16206.0732 - mse: 16206.0732 - mae: 91.7955 - val_loss: 16344.0186 - val_mse: 16344.0186 - val_mae: 93.7326\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16203.4414 - mse: 16203.4414 - mae: 91.7873 - val_loss: 16341.1484 - val_mse: 16341.1484 - val_mae: 93.7239\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16200.8086 - mse: 16200.8086 - mae: 91.7791 - val_loss: 16338.2490 - val_mse: 16338.2490 - val_mae: 93.7151\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16198.1016 - mse: 16198.1016 - mae: 91.7708 - val_loss: 16335.3867 - val_mse: 16335.3867 - val_mae: 93.7064\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16195.3916 - mse: 16195.3916 - mae: 91.7625 - val_loss: 16332.5742 - val_mse: 16332.5742 - val_mae: 93.6979\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16192.7305 - mse: 16192.7305 - mae: 91.7544 - val_loss: 16329.7578 - val_mse: 16329.7578 - val_mae: 93.6894\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16190.1152 - mse: 16190.1152 - mae: 91.7462 - val_loss: 16326.9062 - val_mse: 16326.9062 - val_mae: 93.6807\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16187.4336 - mse: 16187.4336 - mae: 91.7380 - val_loss: 16324.0693 - val_mse: 16324.0693 - val_mae: 93.6721\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 16184.7939 - mse: 16184.7939 - mae: 91.7299 - val_loss: 16321.2197 - val_mse: 16321.2197 - val_mae: 93.6635\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16182.0771 - mse: 16182.0771 - mae: 91.7216 - val_loss: 16318.4121 - val_mse: 16318.4121 - val_mae: 93.6550\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16179.4561 - mse: 16179.4561 - mae: 91.7134 - val_loss: 16315.5547 - val_mse: 16315.5547 - val_mae: 93.6463\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16176.7939 - mse: 16176.7939 - mae: 91.7052 - val_loss: 16312.6875 - val_mse: 16312.6875 - val_mae: 93.6376\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16174.1543 - mse: 16174.1543 - mae: 91.6970 - val_loss: 16309.8174 - val_mse: 16309.8174 - val_mae: 93.6289\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16171.4014 - mse: 16171.4014 - mae: 91.6887 - val_loss: 16307.0195 - val_mse: 16307.0195 - val_mae: 93.6204\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16168.7754 - mse: 16168.7754 - mae: 91.6806 - val_loss: 16304.1797 - val_mse: 16304.1797 - val_mae: 93.6118\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16166.0918 - mse: 16166.0918 - mae: 91.6724 - val_loss: 16301.3330 - val_mse: 16301.3330 - val_mae: 93.6032\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16163.4736 - mse: 16163.4736 - mae: 91.6642 - val_loss: 16298.4580 - val_mse: 16298.4580 - val_mae: 93.5945\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16160.7881 - mse: 16160.7881 - mae: 91.6559 - val_loss: 16295.6289 - val_mse: 16295.6289 - val_mae: 93.5859\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16158.1172 - mse: 16158.1172 - mae: 91.6478 - val_loss: 16292.8311 - val_mse: 16292.8311 - val_mae: 93.5774\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16155.5127 - mse: 16155.5127 - mae: 91.6396 - val_loss: 16289.9805 - val_mse: 16289.9805 - val_mae: 93.5688\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16152.8340 - mse: 16152.8340 - mae: 91.6314 - val_loss: 16287.1387 - val_mse: 16287.1387 - val_mae: 93.5602\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16150.1768 - mse: 16150.1768 - mae: 91.6232 - val_loss: 16284.3145 - val_mse: 16284.3145 - val_mae: 93.5516\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 16147.5215 - mse: 16147.5215 - mae: 91.6150 - val_loss: 16281.5088 - val_mse: 16281.5088 - val_mae: 93.5431\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16144.8848 - mse: 16144.8848 - mae: 91.6069 - val_loss: 16278.6914 - val_mse: 16278.6914 - val_mae: 93.5345\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16142.2949 - mse: 16142.2949 - mae: 91.5988 - val_loss: 16275.8359 - val_mse: 16275.8359 - val_mae: 93.5258\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16139.5732 - mse: 16139.5732 - mae: 91.5905 - val_loss: 16273.0771 - val_mse: 16273.0771 - val_mae: 93.5175\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16136.9414 - mse: 16136.9414 - mae: 91.5824 - val_loss: 16270.3057 - val_mse: 16270.3057 - val_mae: 93.5091\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16134.3701 - mse: 16134.3701 - mae: 91.5743 - val_loss: 16267.4502 - val_mse: 16267.4502 - val_mae: 93.5004\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16131.6221 - mse: 16131.6221 - mae: 91.5661 - val_loss: 16264.6787 - val_mse: 16264.6787 - val_mae: 93.4921\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16129.0176 - mse: 16129.0176 - mae: 91.5580 - val_loss: 16261.8525 - val_mse: 16261.8525 - val_mae: 93.4835\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16126.3623 - mse: 16126.3623 - mae: 91.5497 - val_loss: 16259.0381 - val_mse: 16259.0381 - val_mae: 93.4750\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16123.7041 - mse: 16123.7041 - mae: 91.5415 - val_loss: 16256.2266 - val_mse: 16256.2266 - val_mae: 93.4665\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16121.0469 - mse: 16121.0469 - mae: 91.5333 - val_loss: 16253.4023 - val_mse: 16253.4023 - val_mae: 93.4579\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16118.4111 - mse: 16118.4111 - mae: 91.5252 - val_loss: 16250.5977 - val_mse: 16250.5977 - val_mae: 93.4494\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16115.7832 - mse: 16115.7832 - mae: 91.5170 - val_loss: 16247.7754 - val_mse: 16247.7754 - val_mae: 93.4409\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16113.1045 - mse: 16113.1045 - mae: 91.5088 - val_loss: 16244.9951 - val_mse: 16244.9951 - val_mae: 93.4325\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16110.4951 - mse: 16110.4951 - mae: 91.5007 - val_loss: 16242.1963 - val_mse: 16242.1963 - val_mae: 93.4240\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16107.8164 - mse: 16107.8164 - mae: 91.4925 - val_loss: 16239.4004 - val_mse: 16239.4004 - val_mae: 93.4155\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 16105.1953 - mse: 16105.1953 - mae: 91.4844 - val_loss: 16236.5645 - val_mse: 16236.5645 - val_mae: 93.4069\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16102.5127 - mse: 16102.5127 - mae: 91.4761 - val_loss: 16233.7607 - val_mse: 16233.7607 - val_mae: 93.3984\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16099.8477 - mse: 16099.8477 - mae: 91.4679 - val_loss: 16230.9668 - val_mse: 16230.9668 - val_mae: 93.3900\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16097.2188 - mse: 16097.2188 - mae: 91.4598 - val_loss: 16228.1582 - val_mse: 16228.1582 - val_mae: 93.3814\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16094.5625 - mse: 16094.5625 - mae: 91.4516 - val_loss: 16225.3760 - val_mse: 16225.3760 - val_mae: 93.3730\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16091.9551 - mse: 16091.9551 - mae: 91.4435 - val_loss: 16222.5303 - val_mse: 16222.5303 - val_mae: 93.3644\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16089.3242 - mse: 16089.3242 - mae: 91.4352 - val_loss: 16219.6699 - val_mse: 16219.6699 - val_mae: 93.3557\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16086.6006 - mse: 16086.6006 - mae: 91.4270 - val_loss: 16216.8896 - val_mse: 16216.8896 - val_mae: 93.3473\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16083.9795 - mse: 16083.9795 - mae: 91.4188 - val_loss: 16214.1113 - val_mse: 16214.1113 - val_mae: 93.3389\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16081.3613 - mse: 16081.3613 - mae: 91.4108 - val_loss: 16211.3379 - val_mse: 16211.3379 - val_mae: 93.3305\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16078.6924 - mse: 16078.6924 - mae: 91.4026 - val_loss: 16208.5996 - val_mse: 16208.5996 - val_mae: 93.3221\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16076.1016 - mse: 16076.1016 - mae: 91.3946 - val_loss: 16205.8018 - val_mse: 16205.8018 - val_mae: 93.3137\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16073.5010 - mse: 16073.5010 - mae: 91.3864 - val_loss: 16202.9619 - val_mse: 16202.9619 - val_mae: 93.3050\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16070.8184 - mse: 16070.8184 - mae: 91.3782 - val_loss: 16200.1572 - val_mse: 16200.1572 - val_mae: 93.2965\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16068.1729 - mse: 16068.1729 - mae: 91.3700 - val_loss: 16197.3623 - val_mse: 16197.3623 - val_mae: 93.2880\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16065.5176 - mse: 16065.5176 - mae: 91.3618 - val_loss: 16194.5566 - val_mse: 16194.5566 - val_mae: 93.2795\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16062.8877 - mse: 16062.8877 - mae: 91.3537 - val_loss: 16191.7188 - val_mse: 16191.7188 - val_mae: 93.2710\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16060.2441 - mse: 16060.2441 - mae: 91.3455 - val_loss: 16188.9102 - val_mse: 16188.9102 - val_mae: 93.2624\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16057.6133 - mse: 16057.6133 - mae: 91.3373 - val_loss: 16186.1240 - val_mse: 16186.1240 - val_mae: 93.2540\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16054.9209 - mse: 16054.9209 - mae: 91.3292 - val_loss: 16183.3896 - val_mse: 16183.3896 - val_mae: 93.2457\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 16052.3711 - mse: 16052.3711 - mae: 91.3211 - val_loss: 16180.5400 - val_mse: 16180.5400 - val_mae: 93.2370\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 16049.6650 - mse: 16049.6650 - mae: 91.3129 - val_loss: 16177.7461 - val_mse: 16177.7461 - val_mae: 93.2285\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16047.0898 - mse: 16047.0898 - mae: 91.3047 - val_loss: 16174.9111 - val_mse: 16174.9111 - val_mae: 93.2200\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16044.3623 - mse: 16044.3623 - mae: 91.2965 - val_loss: 16172.1572 - val_mse: 16172.1572 - val_mae: 93.2116\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16041.7871 - mse: 16041.7871 - mae: 91.2884 - val_loss: 16169.3516 - val_mse: 16169.3516 - val_mae: 93.2031\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16039.1152 - mse: 16039.1152 - mae: 91.2803 - val_loss: 16166.6104 - val_mse: 16166.6104 - val_mae: 93.1947\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 36ms/step - loss: 498.3453 - mse: 498.3453 - mae: 15.8995 - val_loss: 176.0839 - val_mse: 176.0839 - val_mae: 11.1497\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 258.5614 - mse: 258.5614 - mae: 12.9481 - val_loss: 163.1781 - val_mse: 163.1781 - val_mae: 10.5825\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 244.6259 - mse: 244.6259 - mae: 12.3924 - val_loss: 153.4243 - val_mse: 153.4243 - val_mae: 10.0502\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 233.6992 - mse: 233.6992 - mae: 11.9032 - val_loss: 145.6243 - val_mse: 145.6243 - val_mae: 9.5592\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 223.6359 - mse: 223.6359 - mae: 11.4337 - val_loss: 138.0595 - val_mse: 138.0595 - val_mae: 9.1605\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.5944 - mse: 215.5944 - mae: 11.0828 - val_loss: 132.2214 - val_mse: 132.2214 - val_mae: 8.8582\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 208.7141 - mse: 208.7141 - mae: 10.7734 - val_loss: 127.5702 - val_mse: 127.5702 - val_mae: 8.4639\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 203.5351 - mse: 203.5351 - mae: 10.5250 - val_loss: 124.0507 - val_mse: 124.0507 - val_mae: 8.2182\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 198.5636 - mse: 198.5636 - mae: 10.3205 - val_loss: 122.1904 - val_mse: 122.1904 - val_mae: 8.0850\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 194.5379 - mse: 194.5379 - mae: 10.1518 - val_loss: 119.4164 - val_mse: 119.4164 - val_mae: 7.9342\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 191.5782 - mse: 191.5782 - mae: 9.9973 - val_loss: 116.3182 - val_mse: 116.3182 - val_mae: 7.7387\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 189.5530 - mse: 189.5530 - mae: 9.9171 - val_loss: 114.1659 - val_mse: 114.1659 - val_mae: 7.5960\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 187.2971 - mse: 187.2971 - mae: 9.8004 - val_loss: 117.0635 - val_mse: 117.0635 - val_mae: 7.8626\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 187.6150 - mse: 187.6150 - mae: 9.9386 - val_loss: 110.9705 - val_mse: 110.9705 - val_mae: 7.3722\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 183.7025 - mse: 183.7025 - mae: 9.6869 - val_loss: 110.1095 - val_mse: 110.1095 - val_mae: 7.2948\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.9996 - mse: 182.9996 - mae: 9.6846 - val_loss: 109.5373 - val_mse: 109.5373 - val_mae: 7.2433\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.4632 - mse: 182.4632 - mae: 9.5968 - val_loss: 109.5597 - val_mse: 109.5597 - val_mae: 7.2685\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 180.9645 - mse: 180.9645 - mae: 9.5711 - val_loss: 109.7165 - val_mse: 109.7165 - val_mae: 7.3122\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 179.8073 - mse: 179.8073 - mae: 9.4940 - val_loss: 111.6722 - val_mse: 111.6722 - val_mae: 7.5220\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 180.3877 - mse: 180.3877 - mae: 9.5737 - val_loss: 110.9033 - val_mse: 110.9033 - val_mae: 7.4566\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 178.8440 - mse: 178.8440 - mae: 9.6125 - val_loss: 108.0590 - val_mse: 108.0590 - val_mae: 7.0594\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 178.9648 - mse: 178.9648 - mae: 9.4921 - val_loss: 107.7275 - val_mse: 107.7275 - val_mae: 7.0826\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 177.9634 - mse: 177.9634 - mae: 9.4382 - val_loss: 109.0630 - val_mse: 109.0630 - val_mae: 7.2740\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 177.7723 - mse: 177.7723 - mae: 9.4879 - val_loss: 108.1131 - val_mse: 108.1131 - val_mae: 7.1539\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 177.3527 - mse: 177.3527 - mae: 9.5041 - val_loss: 108.3251 - val_mse: 108.3251 - val_mae: 7.1906\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 176.9287 - mse: 176.9287 - mae: 9.4314 - val_loss: 109.2195 - val_mse: 109.2195 - val_mae: 7.3160\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 177.3478 - mse: 177.3478 - mae: 9.4660 - val_loss: 107.2381 - val_mse: 107.2381 - val_mae: 7.0398\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 178.1914 - mse: 178.1914 - mae: 9.4693 - val_loss: 106.9247 - val_mse: 106.9247 - val_mae: 6.9518\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 177.2240 - mse: 177.2240 - mae: 9.4053 - val_loss: 110.0351 - val_mse: 110.0351 - val_mae: 7.3862\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 176.2466 - mse: 176.2466 - mae: 9.4716 - val_loss: 106.7282 - val_mse: 106.7282 - val_mae: 6.9054\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 176.6167 - mse: 176.6167 - mae: 9.4273 - val_loss: 106.5208 - val_mse: 106.5208 - val_mae: 6.8784\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 176.4272 - mse: 176.4272 - mae: 9.2677 - val_loss: 109.0117 - val_mse: 109.0117 - val_mae: 7.2338\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 176.4640 - mse: 176.4640 - mae: 9.4292 - val_loss: 108.3252 - val_mse: 108.3252 - val_mae: 7.1425\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 175.5710 - mse: 175.5710 - mae: 9.4032 - val_loss: 107.8326 - val_mse: 107.8326 - val_mae: 7.0700\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 175.6802 - mse: 175.6802 - mae: 9.3462 - val_loss: 108.3395 - val_mse: 108.3395 - val_mae: 7.1385\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 176.0067 - mse: 176.0067 - mae: 9.4234 - val_loss: 107.1395 - val_mse: 107.1395 - val_mae: 6.9661\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 175.3054 - mse: 175.3054 - mae: 9.3524 - val_loss: 106.7994 - val_mse: 106.7994 - val_mae: 6.9070\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 175.7307 - mse: 175.7307 - mae: 9.3351 - val_loss: 106.4876 - val_mse: 106.4876 - val_mae: 6.8656\n",
            "Epoch 38: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 580.9429 - mse: 580.9429 - mae: 20.2834 - val_loss: 300.7255 - val_mse: 300.7255 - val_mae: 14.9633\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 437.6780 - mse: 437.6780 - mae: 17.3303 - val_loss: 290.5272 - val_mse: 290.5272 - val_mae: 14.5560\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 430.9803 - mse: 430.9803 - mae: 17.0431 - val_loss: 290.0232 - val_mse: 290.0232 - val_mae: 14.4607\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 428.5042 - mse: 428.5042 - mae: 16.9747 - val_loss: 289.4101 - val_mse: 289.4101 - val_mae: 14.4051\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 427.6775 - mse: 427.6775 - mae: 16.9534 - val_loss: 288.3765 - val_mse: 288.3765 - val_mae: 14.3536\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 427.1454 - mse: 427.1454 - mae: 16.9359 - val_loss: 286.8312 - val_mse: 286.8312 - val_mae: 14.3047\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 426.3268 - mse: 426.3268 - mae: 16.8862 - val_loss: 286.5927 - val_mse: 286.5927 - val_mae: 14.2756\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 425.9113 - mse: 425.9113 - mae: 16.8504 - val_loss: 287.6252 - val_mse: 287.6252 - val_mae: 14.2630\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 425.0378 - mse: 425.0378 - mae: 16.8566 - val_loss: 287.1714 - val_mse: 287.1714 - val_mae: 14.2297\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 424.6950 - mse: 424.6950 - mae: 16.8438 - val_loss: 286.6965 - val_mse: 286.6965 - val_mae: 14.1970\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 424.2744 - mse: 424.2744 - mae: 16.8312 - val_loss: 285.9519 - val_mse: 285.9519 - val_mae: 14.1716\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 423.8189 - mse: 423.8189 - mae: 16.7954 - val_loss: 286.1782 - val_mse: 286.1782 - val_mae: 14.1561\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 423.4466 - mse: 423.4466 - mae: 16.7949 - val_loss: 285.9461 - val_mse: 285.9461 - val_mae: 14.1389\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 423.4799 - mse: 423.4799 - mae: 16.7917 - val_loss: 285.2214 - val_mse: 285.2214 - val_mae: 14.1142\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 422.8629 - mse: 422.8629 - mae: 16.7571 - val_loss: 285.4461 - val_mse: 285.4461 - val_mae: 14.1039\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 422.5232 - mse: 422.5232 - mae: 16.7839 - val_loss: 284.0732 - val_mse: 284.0732 - val_mae: 14.0725\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 422.4487 - mse: 422.4487 - mae: 16.7263 - val_loss: 284.8774 - val_mse: 284.8774 - val_mae: 14.0694\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 422.3378 - mse: 422.3378 - mae: 16.7439 - val_loss: 284.7036 - val_mse: 284.7036 - val_mae: 14.0546\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 422.0168 - mse: 422.0168 - mae: 16.7298 - val_loss: 284.6650 - val_mse: 284.6650 - val_mae: 14.0438\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.9910 - mse: 421.9910 - mae: 16.7537 - val_loss: 283.9266 - val_mse: 283.9266 - val_mae: 14.0157\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 421.6730 - mse: 421.6730 - mae: 16.7583 - val_loss: 282.4659 - val_mse: 282.4659 - val_mae: 13.9797\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.6727 - mse: 421.6727 - mae: 16.7013 - val_loss: 282.7810 - val_mse: 282.7810 - val_mae: 13.9685\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.3478 - mse: 421.3478 - mae: 16.7061 - val_loss: 283.2709 - val_mse: 283.2709 - val_mae: 13.9701\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 421.3438 - mse: 421.3438 - mae: 16.7393 - val_loss: 283.1178 - val_mse: 283.1178 - val_mae: 13.9570\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 421.1485 - mse: 421.1485 - mae: 16.7272 - val_loss: 282.6213 - val_mse: 282.6213 - val_mae: 13.9361\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.1491 - mse: 421.1491 - mae: 16.7127 - val_loss: 282.9848 - val_mse: 282.9848 - val_mae: 13.9392\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.0849 - mse: 421.0849 - mae: 16.7235 - val_loss: 281.8393 - val_mse: 281.8393 - val_mae: 13.8985\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 421.2554 - mse: 421.2554 - mae: 16.7235 - val_loss: 281.9209 - val_mse: 281.9209 - val_mae: 13.8932\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 420.7356 - mse: 420.7356 - mae: 16.6960 - val_loss: 282.0634 - val_mse: 282.0634 - val_mae: 13.8911\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.6651 - mse: 420.6651 - mae: 16.6953 - val_loss: 282.5509 - val_mse: 282.5509 - val_mae: 13.9048\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 420.8253 - mse: 420.8253 - mae: 16.7327 - val_loss: 281.3746 - val_mse: 281.3746 - val_mae: 13.8570\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 420.7927 - mse: 420.7927 - mae: 16.6952 - val_loss: 282.7739 - val_mse: 282.7739 - val_mae: 13.9115\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.5447 - mse: 420.5447 - mae: 16.7207 - val_loss: 282.5999 - val_mse: 282.5999 - val_mae: 13.9018\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.5091 - mse: 420.5091 - mae: 16.7186 - val_loss: 282.6335 - val_mse: 282.6335 - val_mae: 13.9023\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 420.4101 - mse: 420.4101 - mae: 16.7215 - val_loss: 282.4755 - val_mse: 282.4755 - val_mae: 13.8927\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 420.3452 - mse: 420.3452 - mae: 16.7411 - val_loss: 281.4146 - val_mse: 281.4146 - val_mae: 13.8333\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.4673 - mse: 420.4673 - mae: 16.7061 - val_loss: 281.4939 - val_mse: 281.4939 - val_mae: 13.8359\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 420.4192 - mse: 420.4192 - mae: 16.7035 - val_loss: 281.4188 - val_mse: 281.4188 - val_mae: 13.8304\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 420.2407 - mse: 420.2407 - mae: 16.7166 - val_loss: 281.2937 - val_mse: 281.2937 - val_mae: 13.8216\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 420.2480 - mse: 420.2480 - mae: 16.6834 - val_loss: 282.1728 - val_mse: 282.1728 - val_mae: 13.8696\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 420.2769 - mse: 420.2769 - mae: 16.7542 - val_loss: 280.9637 - val_mse: 280.9637 - val_mae: 13.7984\n",
            "Epoch 41: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 538.8386 - mse: 538.8386 - mae: 21.0488 - val_loss: 459.8853 - val_mse: 459.8853 - val_mae: 18.9598\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 462.2241 - mse: 462.2241 - mae: 18.9176 - val_loss: 448.8090 - val_mse: 448.8090 - val_mae: 18.5723\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 458.2438 - mse: 458.2438 - mae: 18.7599 - val_loss: 457.1783 - val_mse: 457.1783 - val_mae: 18.5916\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 447.7928 - mse: 447.7928 - mae: 18.3023 - val_loss: 438.6548 - val_mse: 438.6548 - val_mae: 18.0581\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 437.9349 - mse: 437.9349 - mae: 18.0320 - val_loss: 438.4936 - val_mse: 438.4936 - val_mae: 17.9973\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 455.2294 - mse: 455.2294 - mae: 18.4994 - val_loss: 456.1324 - val_mse: 456.1324 - val_mae: 18.8089\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 465.7155 - mse: 465.7155 - mae: 18.9798 - val_loss: 459.6138 - val_mse: 459.6138 - val_mae: 18.9615\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 446.3547 - mse: 446.3547 - mae: 18.3074 - val_loss: 459.4137 - val_mse: 459.4137 - val_mae: 18.9688\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 464.1589 - mse: 464.1589 - mae: 18.9265 - val_loss: 452.4082 - val_mse: 452.4082 - val_mae: 18.6244\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 461.4037 - mse: 461.4037 - mae: 18.7859 - val_loss: 452.2383 - val_mse: 452.2383 - val_mae: 18.6192\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 460.5005 - mse: 460.5005 - mae: 18.7460 - val_loss: 450.8806 - val_mse: 450.8806 - val_mae: 18.5614\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 461.3554 - mse: 461.3554 - mae: 18.7474 - val_loss: 458.7127 - val_mse: 458.7127 - val_mae: 19.1160\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 462.7935 - mse: 462.7935 - mae: 18.8699 - val_loss: 450.6008 - val_mse: 450.6008 - val_mae: 18.5719\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 462.6787 - mse: 462.6787 - mae: 18.8854 - val_loss: 450.4192 - val_mse: 450.4192 - val_mae: 18.5732\n",
            "Epoch 14: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 2740.0828 - mse: 2740.0828 - mae: 36.3132 - val_loss: 2067.5767 - val_mse: 2067.5767 - val_mae: 32.1843\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1776.4594 - mse: 1776.4594 - mae: 31.4833 - val_loss: 1418.3237 - val_mse: 1418.3237 - val_mae: 28.5008\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1275.2632 - mse: 1275.2632 - mae: 28.2506 - val_loss: 1056.3998 - val_mse: 1056.3998 - val_mae: 25.9026\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 989.7515 - mse: 989.7515 - mae: 25.9001 - val_loss: 840.9255 - val_mse: 840.9255 - val_mae: 23.9782\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 817.9467 - mse: 817.9467 - mae: 24.1621 - val_loss: 702.9092 - val_mse: 702.9092 - val_mae: 22.5050\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 707.5389 - mse: 707.5389 - mae: 22.8471 - val_loss: 610.9418 - val_mse: 610.9418 - val_mae: 21.3310\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 633.9506 - mse: 633.9506 - mae: 21.8086 - val_loss: 547.6523 - val_mse: 547.6523 - val_mae: 20.3870\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 584.1175 - mse: 584.1175 - mae: 21.0200 - val_loss: 502.8277 - val_mse: 502.8277 - val_mae: 19.6543\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 548.9865 - mse: 548.9865 - mae: 20.3961 - val_loss: 470.5258 - val_mse: 470.5258 - val_mae: 19.0630\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 523.9214 - mse: 523.9214 - mae: 19.8982 - val_loss: 446.0624 - val_mse: 446.0624 - val_mae: 18.5731\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 505.0429 - mse: 505.0429 - mae: 19.4950 - val_loss: 427.6323 - val_mse: 427.6323 - val_mae: 18.1624\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 490.6055 - mse: 490.6055 - mae: 19.1613 - val_loss: 413.0490 - val_mse: 413.0490 - val_mae: 17.8167\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 479.1383 - mse: 479.1383 - mae: 18.8786 - val_loss: 401.6271 - val_mse: 401.6271 - val_mae: 17.5143\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 470.1363 - mse: 470.1363 - mae: 18.6342 - val_loss: 392.4231 - val_mse: 392.4231 - val_mae: 17.2567\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 462.8379 - mse: 462.8379 - mae: 18.4299 - val_loss: 384.8684 - val_mse: 384.8684 - val_mae: 17.0458\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 457.0443 - mse: 457.0443 - mae: 18.2592 - val_loss: 378.4721 - val_mse: 378.4721 - val_mae: 16.8646\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 452.2538 - mse: 452.2538 - mae: 18.1203 - val_loss: 373.2752 - val_mse: 373.2752 - val_mae: 16.7106\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 448.3713 - mse: 448.3713 - mae: 17.9981 - val_loss: 368.8236 - val_mse: 368.8236 - val_mae: 16.5788\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 445.1230 - mse: 445.1230 - mae: 17.8929 - val_loss: 364.8895 - val_mse: 364.8895 - val_mae: 16.4662\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 442.2516 - mse: 442.2516 - mae: 17.8023 - val_loss: 361.5746 - val_mse: 361.5746 - val_mae: 16.3697\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 439.7484 - mse: 439.7484 - mae: 17.7224 - val_loss: 358.5515 - val_mse: 358.5515 - val_mae: 16.2826\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 437.4550 - mse: 437.4550 - mae: 17.6487 - val_loss: 355.6961 - val_mse: 355.6961 - val_mae: 16.2062\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 435.2979 - mse: 435.2979 - mae: 17.5833 - val_loss: 353.0076 - val_mse: 353.0076 - val_mae: 16.1395\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 433.2904 - mse: 433.2904 - mae: 17.5264 - val_loss: 350.5063 - val_mse: 350.5063 - val_mae: 16.0748\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 431.4134 - mse: 431.4134 - mae: 17.4696 - val_loss: 348.0005 - val_mse: 348.0005 - val_mae: 16.0136\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 429.5930 - mse: 429.5930 - mae: 17.4187 - val_loss: 345.7247 - val_mse: 345.7247 - val_mae: 15.9630\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 427.8087 - mse: 427.8087 - mae: 17.3715 - val_loss: 343.5220 - val_mse: 343.5220 - val_mae: 15.9148\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 425.8537 - mse: 425.8537 - mae: 17.3190 - val_loss: 341.1374 - val_mse: 341.1374 - val_mae: 15.8648\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 423.7063 - mse: 423.7063 - mae: 17.2623 - val_loss: 338.5915 - val_mse: 338.5915 - val_mae: 15.8121\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.4164 - mse: 421.4164 - mae: 17.2036 - val_loss: 336.0048 - val_mse: 336.0048 - val_mae: 15.7569\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 419.1862 - mse: 419.1862 - mae: 17.1474 - val_loss: 333.4227 - val_mse: 333.4227 - val_mae: 15.7009\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 416.9557 - mse: 416.9557 - mae: 17.0886 - val_loss: 330.8140 - val_mse: 330.8140 - val_mae: 15.6422\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 414.6719 - mse: 414.6719 - mae: 17.0305 - val_loss: 328.2923 - val_mse: 328.2923 - val_mae: 15.5827\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 412.2546 - mse: 412.2546 - mae: 16.9673 - val_loss: 325.7219 - val_mse: 325.7219 - val_mae: 15.5181\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 409.5805 - mse: 409.5805 - mae: 16.8898 - val_loss: 322.8489 - val_mse: 322.8489 - val_mae: 15.4364\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 406.8410 - mse: 406.8410 - mae: 16.8052 - val_loss: 320.2892 - val_mse: 320.2892 - val_mae: 15.3575\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.5619 - mse: 404.5619 - mae: 16.7318 - val_loss: 317.9868 - val_mse: 317.9868 - val_mae: 15.2840\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 402.6552 - mse: 402.6552 - mae: 16.6667 - val_loss: 316.0294 - val_mse: 316.0294 - val_mae: 15.2194\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 401.0722 - mse: 401.0722 - mae: 16.6098 - val_loss: 314.2124 - val_mse: 314.2124 - val_mae: 15.1613\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 399.6736 - mse: 399.6736 - mae: 16.5631 - val_loss: 312.5718 - val_mse: 312.5718 - val_mae: 15.1084\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 398.3440 - mse: 398.3440 - mae: 16.5197 - val_loss: 311.0715 - val_mse: 311.0715 - val_mae: 15.0597\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 397.2139 - mse: 397.2139 - mae: 16.4803 - val_loss: 309.6457 - val_mse: 309.6457 - val_mae: 15.0150\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 396.1802 - mse: 396.1802 - mae: 16.4481 - val_loss: 308.3083 - val_mse: 308.3083 - val_mae: 14.9746\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 395.2535 - mse: 395.2535 - mae: 16.4196 - val_loss: 307.0329 - val_mse: 307.0329 - val_mae: 14.9356\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 394.3964 - mse: 394.3964 - mae: 16.3939 - val_loss: 305.9047 - val_mse: 305.9047 - val_mae: 14.8993\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 393.5439 - mse: 393.5439 - mae: 16.3676 - val_loss: 304.7767 - val_mse: 304.7767 - val_mae: 14.8661\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 392.8459 - mse: 392.8459 - mae: 16.3470 - val_loss: 303.6273 - val_mse: 303.6273 - val_mae: 14.8344\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 392.0396 - mse: 392.0396 - mae: 16.3246 - val_loss: 302.6717 - val_mse: 302.6717 - val_mae: 14.8064\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 391.3341 - mse: 391.3341 - mae: 16.3060 - val_loss: 301.7709 - val_mse: 301.7709 - val_mae: 14.7796\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 390.6838 - mse: 390.6838 - mae: 16.2884 - val_loss: 300.8654 - val_mse: 300.8654 - val_mae: 14.7539\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 390.0618 - mse: 390.0618 - mae: 16.2716 - val_loss: 299.9572 - val_mse: 299.9572 - val_mae: 14.7288\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 389.4757 - mse: 389.4757 - mae: 16.2566 - val_loss: 299.0422 - val_mse: 299.0422 - val_mae: 14.7057\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 388.8739 - mse: 388.8739 - mae: 16.2417 - val_loss: 298.2002 - val_mse: 298.2002 - val_mae: 14.6855\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 388.2979 - mse: 388.2979 - mae: 16.2294 - val_loss: 297.2289 - val_mse: 297.2289 - val_mae: 14.6620\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 387.6896 - mse: 387.6896 - mae: 16.2108 - val_loss: 296.4103 - val_mse: 296.4103 - val_mae: 14.6426\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 387.1624 - mse: 387.1624 - mae: 16.1982 - val_loss: 295.6363 - val_mse: 295.6363 - val_mae: 14.6232\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 386.6255 - mse: 386.6255 - mae: 16.1843 - val_loss: 294.7718 - val_mse: 294.7718 - val_mae: 14.6027\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 386.0860 - mse: 386.0860 - mae: 16.1702 - val_loss: 294.1156 - val_mse: 294.1156 - val_mae: 14.5866\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 385.5962 - mse: 385.5962 - mae: 16.1599 - val_loss: 293.3614 - val_mse: 293.3614 - val_mae: 14.5670\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 385.1136 - mse: 385.1136 - mae: 16.1458 - val_loss: 292.6327 - val_mse: 292.6327 - val_mae: 14.5483\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 384.6608 - mse: 384.6608 - mae: 16.1330 - val_loss: 291.9412 - val_mse: 291.9412 - val_mae: 14.5299\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 384.1934 - mse: 384.1934 - mae: 16.1223 - val_loss: 291.2570 - val_mse: 291.2570 - val_mae: 14.5115\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 383.7526 - mse: 383.7526 - mae: 16.1093 - val_loss: 290.5962 - val_mse: 290.5962 - val_mae: 14.4943\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 383.3195 - mse: 383.3195 - mae: 16.0968 - val_loss: 290.0562 - val_mse: 290.0562 - val_mae: 14.4807\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 382.9190 - mse: 382.9190 - mae: 16.0874 - val_loss: 289.4169 - val_mse: 289.4169 - val_mae: 14.4637\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 382.4866 - mse: 382.4866 - mae: 16.0757 - val_loss: 288.7877 - val_mse: 288.7877 - val_mae: 14.4465\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 382.0993 - mse: 382.0993 - mae: 16.0656 - val_loss: 288.1830 - val_mse: 288.1830 - val_mae: 14.4299\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 381.6850 - mse: 381.6850 - mae: 16.0548 - val_loss: 287.6587 - val_mse: 287.6587 - val_mae: 14.4150\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 381.3468 - mse: 381.3468 - mae: 16.0480 - val_loss: 287.1111 - val_mse: 287.1111 - val_mae: 14.3995\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 380.9228 - mse: 380.9228 - mae: 16.0336 - val_loss: 286.5421 - val_mse: 286.5421 - val_mae: 14.3830\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 380.5603 - mse: 380.5603 - mae: 16.0246 - val_loss: 285.9710 - val_mse: 285.9710 - val_mae: 14.3658\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 380.2084 - mse: 380.2084 - mae: 16.0110 - val_loss: 285.4904 - val_mse: 285.4904 - val_mae: 14.3517\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 379.8655 - mse: 379.8655 - mae: 16.0039 - val_loss: 285.0026 - val_mse: 285.0026 - val_mae: 14.3370\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 379.5133 - mse: 379.5133 - mae: 15.9916 - val_loss: 284.6018 - val_mse: 284.6018 - val_mae: 14.3259\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 379.1714 - mse: 379.1714 - mae: 15.9848 - val_loss: 284.1338 - val_mse: 284.1338 - val_mae: 14.3118\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 378.8742 - mse: 378.8742 - mae: 15.9755 - val_loss: 283.6417 - val_mse: 283.6417 - val_mae: 14.2964\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 378.5265 - mse: 378.5265 - mae: 15.9668 - val_loss: 283.1955 - val_mse: 283.1955 - val_mae: 14.2825\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 378.2109 - mse: 378.2109 - mae: 15.9593 - val_loss: 282.7578 - val_mse: 282.7578 - val_mae: 14.2691\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 377.8954 - mse: 377.8954 - mae: 15.9496 - val_loss: 282.3181 - val_mse: 282.3181 - val_mae: 14.2557\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 377.6171 - mse: 377.6171 - mae: 15.9424 - val_loss: 281.9049 - val_mse: 281.9049 - val_mae: 14.2436\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 377.3256 - mse: 377.3256 - mae: 15.9365 - val_loss: 281.3818 - val_mse: 281.3818 - val_mae: 14.2274\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.9730 - mse: 376.9730 - mae: 15.9230 - val_loss: 280.9753 - val_mse: 280.9753 - val_mae: 14.2151\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.6863 - mse: 376.6863 - mae: 15.9165 - val_loss: 280.5729 - val_mse: 280.5729 - val_mae: 14.2027\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.4110 - mse: 376.4110 - mae: 15.9080 - val_loss: 280.2159 - val_mse: 280.2159 - val_mae: 14.1920\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 376.1771 - mse: 376.1771 - mae: 15.8997 - val_loss: 279.8439 - val_mse: 279.8439 - val_mae: 14.1808\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 375.8878 - mse: 375.8878 - mae: 15.8931 - val_loss: 279.5360 - val_mse: 279.5360 - val_mae: 14.1720\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 375.6239 - mse: 375.6239 - mae: 15.8884 - val_loss: 279.1140 - val_mse: 279.1140 - val_mae: 14.1579\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 375.3615 - mse: 375.3615 - mae: 15.8794 - val_loss: 278.7714 - val_mse: 278.7714 - val_mae: 14.1473\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 375.1368 - mse: 375.1368 - mae: 15.8728 - val_loss: 278.3861 - val_mse: 278.3861 - val_mae: 14.1342\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.8578 - mse: 374.8578 - mae: 15.8675 - val_loss: 278.0103 - val_mse: 278.0103 - val_mae: 14.1212\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.6218 - mse: 374.6218 - mae: 15.8575 - val_loss: 277.6372 - val_mse: 277.6372 - val_mae: 14.1091\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 374.3863 - mse: 374.3863 - mae: 15.8513 - val_loss: 277.2891 - val_mse: 277.2891 - val_mae: 14.0974\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 374.1431 - mse: 374.1431 - mae: 15.8418 - val_loss: 276.9348 - val_mse: 276.9348 - val_mae: 14.0850\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 373.9012 - mse: 373.9012 - mae: 15.8337 - val_loss: 276.6342 - val_mse: 276.6342 - val_mae: 14.0753\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 373.6639 - mse: 373.6639 - mae: 15.8296 - val_loss: 276.3016 - val_mse: 276.3016 - val_mae: 14.0640\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 373.4424 - mse: 373.4424 - mae: 15.8225 - val_loss: 275.9799 - val_mse: 275.9799 - val_mae: 14.0523\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 373.2363 - mse: 373.2363 - mae: 15.8143 - val_loss: 275.7275 - val_mse: 275.7275 - val_mae: 14.0439\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 373.0266 - mse: 373.0266 - mae: 15.8140 - val_loss: 275.4058 - val_mse: 275.4058 - val_mae: 14.0324\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 372.8145 - mse: 372.8145 - mae: 15.8059 - val_loss: 275.1033 - val_mse: 275.1033 - val_mae: 14.0214\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 372.6250 - mse: 372.6250 - mae: 15.7957 - val_loss: 274.8277 - val_mse: 274.8277 - val_mae: 14.0116\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 1344.8054 - mse: 1344.8054 - mae: 28.3117 - val_loss: 1162.0614 - val_mse: 1162.0614 - val_mae: 25.9379\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1068.7159 - mse: 1068.7159 - mae: 25.7750 - val_loss: 924.3562 - val_mse: 924.3562 - val_mae: 23.6196\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 892.2636 - mse: 892.2636 - mae: 23.8826 - val_loss: 767.6924 - val_mse: 767.6924 - val_mae: 21.8757\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 776.0338 - mse: 776.0338 - mae: 22.4884 - val_loss: 660.9979 - val_mse: 660.9979 - val_mae: 20.6224\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 696.3116 - mse: 696.3116 - mae: 21.4677 - val_loss: 585.5120 - val_mse: 585.5120 - val_mae: 19.6641\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 639.3087 - mse: 639.3087 - mae: 20.7203 - val_loss: 530.8522 - val_mse: 530.8522 - val_mae: 18.9254\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 597.7719 - mse: 597.7719 - mae: 20.1481 - val_loss: 489.6588 - val_mse: 489.6588 - val_mae: 18.3660\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 566.6884 - mse: 566.6884 - mae: 19.7135 - val_loss: 458.9146 - val_mse: 458.9146 - val_mae: 17.9412\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 543.4366 - mse: 543.4366 - mae: 19.3806 - val_loss: 435.4163 - val_mse: 435.4163 - val_mae: 17.6025\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 525.6279 - mse: 525.6279 - mae: 19.1154 - val_loss: 416.1619 - val_mse: 416.1619 - val_mae: 17.3260\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 511.0471 - mse: 511.0471 - mae: 18.8921 - val_loss: 400.3915 - val_mse: 400.3915 - val_mae: 17.0827\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 498.9721 - mse: 498.9721 - mae: 18.7022 - val_loss: 386.6861 - val_mse: 386.6861 - val_mae: 16.8569\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 488.0515 - mse: 488.0515 - mae: 18.5240 - val_loss: 374.6195 - val_mse: 374.6195 - val_mae: 16.6419\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 478.5156 - mse: 478.5156 - mae: 18.3597 - val_loss: 364.1305 - val_mse: 364.1305 - val_mae: 16.4402\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 470.2559 - mse: 470.2559 - mae: 18.2063 - val_loss: 355.2672 - val_mse: 355.2672 - val_mae: 16.2609\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 463.3680 - mse: 463.3680 - mae: 18.0763 - val_loss: 347.7101 - val_mse: 347.7101 - val_mae: 16.0998\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 457.3717 - mse: 457.3717 - mae: 17.9699 - val_loss: 341.0604 - val_mse: 341.0604 - val_mae: 15.9659\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 451.7935 - mse: 451.7935 - mae: 17.8656 - val_loss: 334.8851 - val_mse: 334.8851 - val_mae: 15.8482\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 446.5476 - mse: 446.5476 - mae: 17.7639 - val_loss: 329.6779 - val_mse: 329.6779 - val_mae: 15.7459\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 442.0120 - mse: 442.0120 - mae: 17.6721 - val_loss: 325.3617 - val_mse: 325.3617 - val_mae: 15.6495\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 438.3283 - mse: 438.3283 - mae: 17.5915 - val_loss: 321.5338 - val_mse: 321.5338 - val_mae: 15.5575\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 435.1205 - mse: 435.1205 - mae: 17.5160 - val_loss: 318.0774 - val_mse: 318.0774 - val_mae: 15.4687\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 432.2943 - mse: 432.2943 - mae: 17.4455 - val_loss: 315.0947 - val_mse: 315.0947 - val_mae: 15.3907\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 429.7986 - mse: 429.7986 - mae: 17.3841 - val_loss: 312.6264 - val_mse: 312.6264 - val_mae: 15.3230\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 427.5631 - mse: 427.5631 - mae: 17.3293 - val_loss: 310.1373 - val_mse: 310.1373 - val_mae: 15.2531\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 425.4254 - mse: 425.4254 - mae: 17.2738 - val_loss: 307.7964 - val_mse: 307.7964 - val_mae: 15.1879\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 423.3449 - mse: 423.3449 - mae: 17.2190 - val_loss: 305.2800 - val_mse: 305.2800 - val_mae: 15.1165\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.3426 - mse: 421.3426 - mae: 17.1662 - val_loss: 302.9771 - val_mse: 302.9771 - val_mae: 15.0472\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 419.2899 - mse: 419.2899 - mae: 17.1121 - val_loss: 300.6848 - val_mse: 300.6848 - val_mae: 14.9764\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 417.3473 - mse: 417.3473 - mae: 17.0595 - val_loss: 298.6415 - val_mse: 298.6415 - val_mae: 14.9094\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 415.5610 - mse: 415.5610 - mae: 17.0076 - val_loss: 296.6777 - val_mse: 296.6777 - val_mae: 14.8456\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 413.8767 - mse: 413.8767 - mae: 16.9587 - val_loss: 294.9746 - val_mse: 294.9746 - val_mae: 14.7864\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 412.1641 - mse: 412.1641 - mae: 16.9127 - val_loss: 293.2339 - val_mse: 293.2339 - val_mae: 14.7257\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 410.5027 - mse: 410.5027 - mae: 16.8654 - val_loss: 291.5320 - val_mse: 291.5320 - val_mae: 14.6703\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 408.9274 - mse: 408.9274 - mae: 16.8178 - val_loss: 290.0850 - val_mse: 290.0850 - val_mae: 14.6229\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 407.4277 - mse: 407.4277 - mae: 16.7754 - val_loss: 288.6716 - val_mse: 288.6716 - val_mae: 14.5790\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 405.9862 - mse: 405.9862 - mae: 16.7343 - val_loss: 287.1877 - val_mse: 287.1877 - val_mae: 14.5320\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 404.5733 - mse: 404.5733 - mae: 16.6932 - val_loss: 285.7093 - val_mse: 285.7093 - val_mae: 14.4837\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 403.2343 - mse: 403.2343 - mae: 16.6560 - val_loss: 284.2681 - val_mse: 284.2681 - val_mae: 14.4360\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 401.9625 - mse: 401.9625 - mae: 16.6171 - val_loss: 282.9380 - val_mse: 282.9380 - val_mae: 14.3902\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 400.7294 - mse: 400.7294 - mae: 16.5836 - val_loss: 281.5895 - val_mse: 281.5895 - val_mae: 14.3437\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 399.4917 - mse: 399.4917 - mae: 16.5487 - val_loss: 280.3016 - val_mse: 280.3016 - val_mae: 14.2981\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 398.3803 - mse: 398.3803 - mae: 16.5141 - val_loss: 279.1472 - val_mse: 279.1472 - val_mae: 14.2581\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 397.3693 - mse: 397.3693 - mae: 16.4888 - val_loss: 278.0235 - val_mse: 278.0235 - val_mae: 14.2176\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 396.3963 - mse: 396.3963 - mae: 16.4633 - val_loss: 276.8225 - val_mse: 276.8225 - val_mae: 14.1743\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 395.3290 - mse: 395.3290 - mae: 16.4263 - val_loss: 275.6965 - val_mse: 275.6965 - val_mae: 14.1342\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 394.3566 - mse: 394.3566 - mae: 16.3987 - val_loss: 274.5344 - val_mse: 274.5344 - val_mae: 14.0915\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 393.4128 - mse: 393.4128 - mae: 16.3712 - val_loss: 273.5096 - val_mse: 273.5096 - val_mae: 14.0530\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 392.4293 - mse: 392.4293 - mae: 16.3411 - val_loss: 272.6287 - val_mse: 272.6287 - val_mae: 14.0203\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 391.4673 - mse: 391.4673 - mae: 16.3188 - val_loss: 271.5398 - val_mse: 271.5398 - val_mae: 13.9819\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 390.5194 - mse: 390.5194 - mae: 16.2917 - val_loss: 270.4315 - val_mse: 270.4315 - val_mae: 13.9425\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 389.6693 - mse: 389.6693 - mae: 16.2670 - val_loss: 269.4651 - val_mse: 269.4651 - val_mae: 13.9077\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 388.6811 - mse: 388.6811 - mae: 16.2405 - val_loss: 268.5558 - val_mse: 268.5558 - val_mae: 13.8735\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 387.8853 - mse: 387.8853 - mae: 16.2171 - val_loss: 267.5923 - val_mse: 267.5923 - val_mae: 13.8379\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 387.0589 - mse: 387.0589 - mae: 16.1900 - val_loss: 266.8149 - val_mse: 266.8149 - val_mae: 13.8076\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 386.3653 - mse: 386.3653 - mae: 16.1720 - val_loss: 266.0545 - val_mse: 266.0545 - val_mae: 13.7779\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 385.6092 - mse: 385.6092 - mae: 16.1461 - val_loss: 265.3165 - val_mse: 265.3165 - val_mae: 13.7497\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 384.8999 - mse: 384.8999 - mae: 16.1271 - val_loss: 264.5653 - val_mse: 264.5653 - val_mae: 13.7207\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 384.2836 - mse: 384.2836 - mae: 16.1052 - val_loss: 263.9039 - val_mse: 263.9039 - val_mae: 13.6941\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 383.6335 - mse: 383.6335 - mae: 16.0873 - val_loss: 263.2426 - val_mse: 263.2426 - val_mae: 13.6672\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 383.0640 - mse: 383.0640 - mae: 16.0700 - val_loss: 262.5164 - val_mse: 262.5164 - val_mae: 13.6382\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 382.4103 - mse: 382.4103 - mae: 16.0495 - val_loss: 261.7490 - val_mse: 261.7490 - val_mae: 13.6077\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 381.8287 - mse: 381.8287 - mae: 16.0293 - val_loss: 261.0435 - val_mse: 261.0435 - val_mae: 13.5809\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 381.2867 - mse: 381.2867 - mae: 16.0120 - val_loss: 260.3895 - val_mse: 260.3895 - val_mae: 13.5560\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 380.8122 - mse: 380.8122 - mae: 15.9914 - val_loss: 259.7798 - val_mse: 259.7798 - val_mae: 13.5340\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 380.2321 - mse: 380.2321 - mae: 15.9762 - val_loss: 259.3047 - val_mse: 259.3047 - val_mae: 13.5168\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 379.7353 - mse: 379.7353 - mae: 15.9625 - val_loss: 258.7197 - val_mse: 258.7197 - val_mae: 13.4945\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 379.2908 - mse: 379.2908 - mae: 15.9466 - val_loss: 258.2248 - val_mse: 258.2248 - val_mae: 13.4763\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 378.8384 - mse: 378.8384 - mae: 15.9338 - val_loss: 257.7659 - val_mse: 257.7659 - val_mae: 13.4594\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 378.3767 - mse: 378.3767 - mae: 15.9144 - val_loss: 257.2778 - val_mse: 257.2778 - val_mae: 13.4419\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 377.9392 - mse: 377.9392 - mae: 15.9050 - val_loss: 256.7163 - val_mse: 256.7163 - val_mae: 13.4200\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 377.5368 - mse: 377.5368 - mae: 15.8854 - val_loss: 256.2967 - val_mse: 256.2967 - val_mae: 13.4050\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 377.1165 - mse: 377.1165 - mae: 15.8725 - val_loss: 255.8771 - val_mse: 255.8771 - val_mae: 13.3904\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 376.7120 - mse: 376.7120 - mae: 15.8596 - val_loss: 255.5717 - val_mse: 255.5717 - val_mae: 13.3822\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.3517 - mse: 376.3517 - mae: 15.8522 - val_loss: 255.1836 - val_mse: 255.1836 - val_mae: 13.3690\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.0483 - mse: 376.0483 - mae: 15.8370 - val_loss: 254.7643 - val_mse: 254.7643 - val_mae: 13.3548\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 375.6252 - mse: 375.6252 - mae: 15.8253 - val_loss: 254.4975 - val_mse: 254.4975 - val_mae: 13.3475\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 375.3024 - mse: 375.3024 - mae: 15.8208 - val_loss: 254.0209 - val_mse: 254.0209 - val_mae: 13.3291\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 374.9406 - mse: 374.9406 - mae: 15.8034 - val_loss: 253.6817 - val_mse: 253.6817 - val_mae: 13.3170\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.6835 - mse: 374.6835 - mae: 15.7946 - val_loss: 253.3773 - val_mse: 253.3773 - val_mae: 13.3074\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.3654 - mse: 374.3654 - mae: 15.7864 - val_loss: 252.9666 - val_mse: 252.9666 - val_mae: 13.2917\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.0330 - mse: 374.0330 - mae: 15.7697 - val_loss: 252.6686 - val_mse: 252.6686 - val_mae: 13.2820\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 373.7346 - mse: 373.7346 - mae: 15.7620 - val_loss: 252.4139 - val_mse: 252.4139 - val_mae: 13.2736\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 373.4225 - mse: 373.4225 - mae: 15.7522 - val_loss: 252.1227 - val_mse: 252.1227 - val_mae: 13.2629\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 373.1638 - mse: 373.1638 - mae: 15.7391 - val_loss: 251.8417 - val_mse: 251.8417 - val_mae: 13.2535\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 372.8522 - mse: 372.8522 - mae: 15.7345 - val_loss: 251.5227 - val_mse: 251.5227 - val_mae: 13.2426\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 372.5625 - mse: 372.5625 - mae: 15.7220 - val_loss: 251.2723 - val_mse: 251.2723 - val_mae: 13.2358\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 372.3113 - mse: 372.3113 - mae: 15.7174 - val_loss: 251.0578 - val_mse: 251.0578 - val_mae: 13.2302\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 372.0475 - mse: 372.0475 - mae: 15.7088 - val_loss: 250.7345 - val_mse: 250.7345 - val_mae: 13.2185\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 371.7585 - mse: 371.7585 - mae: 15.7015 - val_loss: 250.2986 - val_mse: 250.2986 - val_mae: 13.2011\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 371.4899 - mse: 371.4899 - mae: 15.6880 - val_loss: 249.9921 - val_mse: 249.9921 - val_mae: 13.1909\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 371.2695 - mse: 371.2695 - mae: 15.6803 - val_loss: 249.7111 - val_mse: 249.7111 - val_mae: 13.1812\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 370.9774 - mse: 370.9774 - mae: 15.6666 - val_loss: 249.5476 - val_mse: 249.5476 - val_mae: 13.1786\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 370.7505 - mse: 370.7505 - mae: 15.6624 - val_loss: 249.3068 - val_mse: 249.3068 - val_mae: 13.1708\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 370.4720 - mse: 370.4720 - mae: 15.6571 - val_loss: 248.9653 - val_mse: 248.9653 - val_mae: 13.1565\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 370.2284 - mse: 370.2284 - mae: 15.6453 - val_loss: 248.6538 - val_mse: 248.6538 - val_mae: 13.1443\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 369.9831 - mse: 369.9831 - mae: 15.6346 - val_loss: 248.4769 - val_mse: 248.4769 - val_mae: 13.1402\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 369.7905 - mse: 369.7905 - mae: 15.6327 - val_loss: 248.2026 - val_mse: 248.2026 - val_mae: 13.1298\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 369.5761 - mse: 369.5761 - mae: 15.6233 - val_loss: 247.9373 - val_mse: 247.9373 - val_mae: 13.1190\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 369.3287 - mse: 369.3287 - mae: 15.6115 - val_loss: 247.6513 - val_mse: 247.6513 - val_mae: 13.1082\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: 1423.9529 - mse: 1423.9529 - mae: 27.8336 - val_loss: 1160.5831 - val_mse: 1160.5831 - val_mae: 26.2117\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1134.8182 - mse: 1134.8182 - mae: 25.4859 - val_loss: 932.2388 - val_mse: 932.2388 - val_mae: 24.1899\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 947.7444 - mse: 947.7444 - mae: 23.7609 - val_loss: 783.0972 - val_mse: 783.0972 - val_mae: 22.7050\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 820.3138 - mse: 820.3138 - mae: 22.4556 - val_loss: 678.4903 - val_mse: 678.4903 - val_mae: 21.5163\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 728.6835 - mse: 728.6835 - mae: 21.4520 - val_loss: 604.6562 - val_mse: 604.6562 - val_mae: 20.6245\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 661.9294 - mse: 661.9294 - mae: 20.6594 - val_loss: 551.4911 - val_mse: 551.4911 - val_mae: 19.9087\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 611.9042 - mse: 611.9042 - mae: 20.0078 - val_loss: 512.0173 - val_mse: 512.0173 - val_mae: 19.3187\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 572.9371 - mse: 572.9371 - mae: 19.4539 - val_loss: 481.2824 - val_mse: 481.2824 - val_mae: 18.7850\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 541.7225 - mse: 541.7225 - mae: 18.9621 - val_loss: 456.2465 - val_mse: 456.2465 - val_mae: 18.2890\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 515.1027 - mse: 515.1027 - mae: 18.5306 - val_loss: 436.2195 - val_mse: 436.2195 - val_mae: 17.8562\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 494.0576 - mse: 494.0576 - mae: 18.1551 - val_loss: 420.3135 - val_mse: 420.3135 - val_mae: 17.4925\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 475.9386 - mse: 475.9386 - mae: 17.8118 - val_loss: 406.4100 - val_mse: 406.4100 - val_mae: 17.1523\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 460.1776 - mse: 460.1776 - mae: 17.4971 - val_loss: 394.7128 - val_mse: 394.7128 - val_mae: 16.8493\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 446.8538 - mse: 446.8538 - mae: 17.2141 - val_loss: 384.9920 - val_mse: 384.9920 - val_mae: 16.5931\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 435.5876 - mse: 435.5876 - mae: 16.9670 - val_loss: 377.0282 - val_mse: 377.0282 - val_mae: 16.3662\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 426.1310 - mse: 426.1310 - mae: 16.7667 - val_loss: 370.3723 - val_mse: 370.3723 - val_mae: 16.1759\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 418.1032 - mse: 418.1032 - mae: 16.6006 - val_loss: 364.5706 - val_mse: 364.5706 - val_mae: 16.0215\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 411.0980 - mse: 411.0980 - mae: 16.4538 - val_loss: 359.7555 - val_mse: 359.7555 - val_mae: 15.8999\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 404.8702 - mse: 404.8702 - mae: 16.3266 - val_loss: 355.7717 - val_mse: 355.7717 - val_mae: 15.7965\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 399.3997 - mse: 399.3997 - mae: 16.2211 - val_loss: 352.1472 - val_mse: 352.1472 - val_mae: 15.6941\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 394.8164 - mse: 394.8164 - mae: 16.1222 - val_loss: 348.9694 - val_mse: 348.9694 - val_mae: 15.6005\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 390.2991 - mse: 390.2991 - mae: 16.0255 - val_loss: 346.0464 - val_mse: 346.0464 - val_mae: 15.5136\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 386.2071 - mse: 386.2071 - mae: 15.9340 - val_loss: 343.2807 - val_mse: 343.2807 - val_mae: 15.4350\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 382.1979 - mse: 382.1979 - mae: 15.8491 - val_loss: 340.6847 - val_mse: 340.6847 - val_mae: 15.3594\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 378.2765 - mse: 378.2765 - mae: 15.7648 - val_loss: 338.0185 - val_mse: 338.0185 - val_mae: 15.2806\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.6609 - mse: 374.6609 - mae: 15.6875 - val_loss: 335.2215 - val_mse: 335.2215 - val_mae: 15.1967\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 371.0013 - mse: 371.0013 - mae: 15.6161 - val_loss: 332.3434 - val_mse: 332.3434 - val_mae: 15.1124\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 367.3854 - mse: 367.3854 - mae: 15.5438 - val_loss: 329.4510 - val_mse: 329.4510 - val_mae: 15.0261\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 363.8494 - mse: 363.8494 - mae: 15.4677 - val_loss: 326.6920 - val_mse: 326.6920 - val_mae: 14.9475\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 360.5232 - mse: 360.5232 - mae: 15.3962 - val_loss: 324.0740 - val_mse: 324.0740 - val_mae: 14.8693\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 357.2344 - mse: 357.2344 - mae: 15.3303 - val_loss: 321.6765 - val_mse: 321.6765 - val_mae: 14.7968\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 354.0993 - mse: 354.0993 - mae: 15.2631 - val_loss: 319.3453 - val_mse: 319.3453 - val_mae: 14.7278\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 351.1282 - mse: 351.1282 - mae: 15.1973 - val_loss: 316.9740 - val_mse: 316.9740 - val_mae: 14.6581\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 348.3215 - mse: 348.3215 - mae: 15.1335 - val_loss: 314.7838 - val_mse: 314.7838 - val_mae: 14.5911\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 345.6533 - mse: 345.6533 - mae: 15.0730 - val_loss: 312.7162 - val_mse: 312.7162 - val_mae: 14.5270\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 343.0400 - mse: 343.0400 - mae: 15.0118 - val_loss: 310.7585 - val_mse: 310.7585 - val_mae: 14.4616\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 340.3589 - mse: 340.3589 - mae: 14.9462 - val_loss: 308.7983 - val_mse: 308.7983 - val_mae: 14.3958\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 337.8774 - mse: 337.8774 - mae: 14.8852 - val_loss: 306.9639 - val_mse: 306.9639 - val_mae: 14.3354\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 335.5437 - mse: 335.5437 - mae: 14.8277 - val_loss: 305.3280 - val_mse: 305.3280 - val_mae: 14.2804\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 333.2617 - mse: 333.2617 - mae: 14.7701 - val_loss: 303.7706 - val_mse: 303.7706 - val_mae: 14.2263\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 331.1216 - mse: 331.1216 - mae: 14.7094 - val_loss: 302.2435 - val_mse: 302.2435 - val_mae: 14.1737\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 329.1021 - mse: 329.1021 - mae: 14.6538 - val_loss: 300.7256 - val_mse: 300.7256 - val_mae: 14.1210\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 327.1083 - mse: 327.1083 - mae: 14.6029 - val_loss: 299.2512 - val_mse: 299.2512 - val_mae: 14.0702\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 325.2281 - mse: 325.2281 - mae: 14.5509 - val_loss: 297.7780 - val_mse: 297.7780 - val_mae: 14.0204\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 323.3649 - mse: 323.3649 - mae: 14.5002 - val_loss: 296.3936 - val_mse: 296.3936 - val_mae: 13.9746\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 321.6375 - mse: 321.6375 - mae: 14.4487 - val_loss: 295.0657 - val_mse: 295.0657 - val_mae: 13.9312\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 319.9954 - mse: 319.9954 - mae: 14.4074 - val_loss: 293.7979 - val_mse: 293.7979 - val_mae: 13.8892\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 318.3520 - mse: 318.3520 - mae: 14.3609 - val_loss: 292.5643 - val_mse: 292.5643 - val_mae: 13.8502\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 316.8070 - mse: 316.8070 - mae: 14.3183 - val_loss: 291.4410 - val_mse: 291.4410 - val_mae: 13.8149\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 315.2906 - mse: 315.2906 - mae: 14.2720 - val_loss: 290.3961 - val_mse: 290.3961 - val_mae: 13.7812\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 313.8380 - mse: 313.8380 - mae: 14.2312 - val_loss: 289.3441 - val_mse: 289.3441 - val_mae: 13.7469\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 312.4938 - mse: 312.4938 - mae: 14.1927 - val_loss: 288.3916 - val_mse: 288.3916 - val_mae: 13.7142\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 311.1380 - mse: 311.1380 - mae: 14.1563 - val_loss: 287.5677 - val_mse: 287.5677 - val_mae: 13.6836\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 309.8436 - mse: 309.8436 - mae: 14.1216 - val_loss: 286.8166 - val_mse: 286.8166 - val_mae: 13.6542\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 308.6458 - mse: 308.6458 - mae: 14.0831 - val_loss: 286.0393 - val_mse: 286.0393 - val_mae: 13.6252\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 307.5792 - mse: 307.5792 - mae: 14.0543 - val_loss: 285.2717 - val_mse: 285.2717 - val_mae: 13.5994\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 306.5560 - mse: 306.5560 - mae: 14.0215 - val_loss: 284.5781 - val_mse: 284.5781 - val_mae: 13.5764\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 305.6117 - mse: 305.6117 - mae: 13.9953 - val_loss: 283.9865 - val_mse: 283.9865 - val_mae: 13.5557\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 304.7187 - mse: 304.7187 - mae: 13.9679 - val_loss: 283.3936 - val_mse: 283.3936 - val_mae: 13.5357\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 303.8435 - mse: 303.8435 - mae: 13.9417 - val_loss: 282.8641 - val_mse: 282.8641 - val_mae: 13.5164\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 303.0494 - mse: 303.0494 - mae: 13.9196 - val_loss: 282.3460 - val_mse: 282.3460 - val_mae: 13.4984\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 302.2588 - mse: 302.2588 - mae: 13.8952 - val_loss: 281.8882 - val_mse: 281.8882 - val_mae: 13.4814\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 301.5052 - mse: 301.5052 - mae: 13.8729 - val_loss: 281.4003 - val_mse: 281.4003 - val_mae: 13.4633\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 300.8181 - mse: 300.8181 - mae: 13.8537 - val_loss: 280.9615 - val_mse: 280.9615 - val_mae: 13.4470\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 300.1324 - mse: 300.1324 - mae: 13.8336 - val_loss: 280.5529 - val_mse: 280.5529 - val_mae: 13.4314\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 299.4742 - mse: 299.4742 - mae: 13.8134 - val_loss: 280.1215 - val_mse: 280.1215 - val_mae: 13.4170\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 298.8617 - mse: 298.8617 - mae: 13.7971 - val_loss: 279.7444 - val_mse: 279.7444 - val_mae: 13.4035\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 298.2810 - mse: 298.2810 - mae: 13.7820 - val_loss: 279.3293 - val_mse: 279.3293 - val_mae: 13.3881\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 297.7176 - mse: 297.7176 - mae: 13.7613 - val_loss: 278.9623 - val_mse: 278.9623 - val_mae: 13.3749\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 297.1651 - mse: 297.1651 - mae: 13.7489 - val_loss: 278.6243 - val_mse: 278.6243 - val_mae: 13.3608\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 296.6122 - mse: 296.6122 - mae: 13.7308 - val_loss: 278.2927 - val_mse: 278.2927 - val_mae: 13.3481\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 296.1060 - mse: 296.1060 - mae: 13.7155 - val_loss: 277.9449 - val_mse: 277.9449 - val_mae: 13.3358\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 295.6257 - mse: 295.6257 - mae: 13.7006 - val_loss: 277.6255 - val_mse: 277.6255 - val_mae: 13.3237\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 295.1188 - mse: 295.1188 - mae: 13.6868 - val_loss: 277.2952 - val_mse: 277.2952 - val_mae: 13.3125\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 294.6499 - mse: 294.6499 - mae: 13.6754 - val_loss: 276.9480 - val_mse: 276.9480 - val_mae: 13.3003\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 294.2134 - mse: 294.2134 - mae: 13.6613 - val_loss: 276.6264 - val_mse: 276.6264 - val_mae: 13.2894\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 293.7328 - mse: 293.7328 - mae: 13.6478 - val_loss: 276.2953 - val_mse: 276.2953 - val_mae: 13.2783\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 293.3177 - mse: 293.3177 - mae: 13.6346 - val_loss: 276.0181 - val_mse: 276.0181 - val_mae: 13.2712\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 292.8861 - mse: 292.8861 - mae: 13.6274 - val_loss: 275.7049 - val_mse: 275.7049 - val_mae: 13.2600\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 292.4585 - mse: 292.4585 - mae: 13.6121 - val_loss: 275.3915 - val_mse: 275.3915 - val_mae: 13.2493\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 292.0779 - mse: 292.0779 - mae: 13.5995 - val_loss: 275.1085 - val_mse: 275.1085 - val_mae: 13.2400\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 291.6751 - mse: 291.6751 - mae: 13.5867 - val_loss: 274.8508 - val_mse: 274.8508 - val_mae: 13.2318\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 291.2772 - mse: 291.2772 - mae: 13.5734 - val_loss: 274.5865 - val_mse: 274.5865 - val_mae: 13.2244\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 290.8982 - mse: 290.8982 - mae: 13.5624 - val_loss: 274.3279 - val_mse: 274.3279 - val_mae: 13.2169\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 290.5552 - mse: 290.5552 - mae: 13.5527 - val_loss: 274.0412 - val_mse: 274.0412 - val_mae: 13.2071\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 290.2004 - mse: 290.2004 - mae: 13.5416 - val_loss: 273.7379 - val_mse: 273.7379 - val_mae: 13.1971\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 289.8490 - mse: 289.8490 - mae: 13.5309 - val_loss: 273.5166 - val_mse: 273.5166 - val_mae: 13.1894\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 289.4749 - mse: 289.4749 - mae: 13.5170 - val_loss: 273.2588 - val_mse: 273.2588 - val_mae: 13.1810\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 289.1407 - mse: 289.1407 - mae: 13.5057 - val_loss: 273.0069 - val_mse: 273.0069 - val_mae: 13.1722\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 288.8421 - mse: 288.8421 - mae: 13.4980 - val_loss: 272.7905 - val_mse: 272.7905 - val_mae: 13.1640\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 288.4799 - mse: 288.4799 - mae: 13.4833 - val_loss: 272.5636 - val_mse: 272.5636 - val_mae: 13.1573\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 288.1786 - mse: 288.1786 - mae: 13.4749 - val_loss: 272.3447 - val_mse: 272.3447 - val_mae: 13.1516\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 287.8641 - mse: 287.8641 - mae: 13.4682 - val_loss: 272.1192 - val_mse: 272.1192 - val_mae: 13.1443\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 287.5839 - mse: 287.5839 - mae: 13.4594 - val_loss: 271.8449 - val_mse: 271.8449 - val_mae: 13.1341\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 287.2713 - mse: 287.2713 - mae: 13.4485 - val_loss: 271.6090 - val_mse: 271.6090 - val_mae: 13.1259\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 286.9690 - mse: 286.9690 - mae: 13.4379 - val_loss: 271.4156 - val_mse: 271.4156 - val_mae: 13.1185\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 286.6728 - mse: 286.6728 - mae: 13.4269 - val_loss: 271.2067 - val_mse: 271.2067 - val_mae: 13.1116\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 286.3875 - mse: 286.3875 - mae: 13.4162 - val_loss: 270.9962 - val_mse: 270.9962 - val_mae: 13.1044\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 286.1147 - mse: 286.1147 - mae: 13.4079 - val_loss: 270.8040 - val_mse: 270.8040 - val_mae: 13.0980\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 285.8586 - mse: 285.8586 - mae: 13.3989 - val_loss: 270.5953 - val_mse: 270.5953 - val_mae: 13.0900\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 20ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 908.8613 - mse: 908.8613 - mae: 23.5314 - val_loss: 885.1147 - val_mse: 885.1147 - val_mae: 22.7132\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 900.3298 - mse: 900.3298 - mae: 23.4130 - val_loss: 875.9252 - val_mse: 875.9252 - val_mae: 22.5886\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 891.7601 - mse: 891.7601 - mae: 23.2953 - val_loss: 866.9262 - val_mse: 866.9262 - val_mae: 22.4657\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 883.4443 - mse: 883.4443 - mae: 23.1788 - val_loss: 858.0089 - val_mse: 858.0089 - val_mae: 22.3438\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 875.3414 - mse: 875.3414 - mae: 23.0652 - val_loss: 849.2676 - val_mse: 849.2676 - val_mae: 22.2238\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 867.1307 - mse: 867.1307 - mae: 22.9521 - val_loss: 840.7571 - val_mse: 840.7571 - val_mae: 22.1071\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 859.2940 - mse: 859.2940 - mae: 22.8419 - val_loss: 832.2686 - val_mse: 832.2686 - val_mae: 21.9907\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 851.4801 - mse: 851.4801 - mae: 22.7323 - val_loss: 823.8793 - val_mse: 823.8793 - val_mae: 21.8754\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 843.7428 - mse: 843.7428 - mae: 22.6247 - val_loss: 815.6874 - val_mse: 815.6874 - val_mae: 21.7626\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 836.1478 - mse: 836.1478 - mae: 22.5185 - val_loss: 807.7195 - val_mse: 807.7195 - val_mae: 21.6526\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 828.8749 - mse: 828.8749 - mae: 22.4146 - val_loss: 799.7179 - val_mse: 799.7179 - val_mae: 21.5433\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 821.4957 - mse: 821.4957 - mae: 22.3114 - val_loss: 791.9827 - val_mse: 791.9827 - val_mae: 21.4363\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 814.2957 - mse: 814.2957 - mae: 22.2107 - val_loss: 784.3260 - val_mse: 784.3260 - val_mae: 21.3294\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 807.3492 - mse: 807.3492 - mae: 22.1099 - val_loss: 776.6945 - val_mse: 776.6945 - val_mae: 21.2235\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 800.1911 - mse: 800.1911 - mae: 22.0099 - val_loss: 769.3061 - val_mse: 769.3061 - val_mae: 21.1198\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 793.2914 - mse: 793.2914 - mae: 21.9113 - val_loss: 761.9446 - val_mse: 761.9446 - val_mae: 21.0162\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 786.5981 - mse: 786.5981 - mae: 21.8145 - val_loss: 754.4871 - val_mse: 754.4871 - val_mae: 20.9122\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 779.8471 - mse: 779.8471 - mae: 21.7174 - val_loss: 747.1973 - val_mse: 747.1973 - val_mae: 20.8095\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 773.1591 - mse: 773.1591 - mae: 21.6216 - val_loss: 740.1314 - val_mse: 740.1314 - val_mae: 20.7087\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 766.6311 - mse: 766.6311 - mae: 21.5271 - val_loss: 733.1531 - val_mse: 733.1531 - val_mae: 20.6087\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 760.1833 - mse: 760.1833 - mae: 21.4331 - val_loss: 726.2950 - val_mse: 726.2950 - val_mae: 20.5106\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 753.8370 - mse: 753.8370 - mae: 21.3406 - val_loss: 719.4743 - val_mse: 719.4743 - val_mae: 20.4122\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 747.6225 - mse: 747.6225 - mae: 21.2476 - val_loss: 712.6309 - val_mse: 712.6309 - val_mae: 20.3133\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 741.3237 - mse: 741.3237 - mae: 21.1538 - val_loss: 705.9517 - val_mse: 705.9517 - val_mae: 20.2157\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 735.1672 - mse: 735.1672 - mae: 21.0626 - val_loss: 699.4425 - val_mse: 699.4425 - val_mae: 20.1198\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 729.1220 - mse: 729.1220 - mae: 20.9724 - val_loss: 693.0188 - val_mse: 693.0188 - val_mae: 20.0247\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 723.2100 - mse: 723.2100 - mae: 20.8828 - val_loss: 686.6054 - val_mse: 686.6054 - val_mae: 19.9306\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 717.3220 - mse: 717.3220 - mae: 20.7943 - val_loss: 680.3286 - val_mse: 680.3286 - val_mae: 19.8371\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 711.5941 - mse: 711.5941 - mae: 20.7074 - val_loss: 674.0770 - val_mse: 674.0770 - val_mae: 19.7440\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 705.7278 - mse: 705.7278 - mae: 20.6204 - val_loss: 667.9872 - val_mse: 667.9872 - val_mae: 19.6521\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 700.1475 - mse: 700.1475 - mae: 20.5340 - val_loss: 661.8441 - val_mse: 661.8441 - val_mae: 19.5587\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 694.5435 - mse: 694.5435 - mae: 20.4483 - val_loss: 655.7672 - val_mse: 655.7672 - val_mae: 19.4659\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 689.0456 - mse: 689.0456 - mae: 20.3629 - val_loss: 649.8025 - val_mse: 649.8025 - val_mae: 19.3745\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 683.5198 - mse: 683.5198 - mae: 20.2784 - val_loss: 644.0194 - val_mse: 644.0194 - val_mae: 19.2850\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 678.2028 - mse: 678.2028 - mae: 20.1951 - val_loss: 638.2616 - val_mse: 638.2616 - val_mae: 19.1953\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 672.7576 - mse: 672.7576 - mae: 20.1113 - val_loss: 632.7032 - val_mse: 632.7032 - val_mae: 19.1075\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 667.6225 - mse: 667.6225 - mae: 20.0292 - val_loss: 626.9608 - val_mse: 626.9608 - val_mae: 19.0181\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 662.3301 - mse: 662.3301 - mae: 19.9471 - val_loss: 621.3336 - val_mse: 621.3336 - val_mae: 18.9297\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 657.2180 - mse: 657.2180 - mae: 19.8657 - val_loss: 615.7731 - val_mse: 615.7731 - val_mae: 18.8418\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 652.1336 - mse: 652.1336 - mae: 19.7855 - val_loss: 610.3511 - val_mse: 610.3511 - val_mae: 18.7554\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 647.1265 - mse: 647.1265 - mae: 19.7064 - val_loss: 605.0962 - val_mse: 605.0962 - val_mae: 18.6714\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 642.2476 - mse: 642.2476 - mae: 19.6292 - val_loss: 599.9282 - val_mse: 599.9282 - val_mae: 18.5893\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 637.4431 - mse: 637.4431 - mae: 19.5543 - val_loss: 594.8464 - val_mse: 594.8464 - val_mae: 18.5087\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 632.8572 - mse: 632.8572 - mae: 19.4806 - val_loss: 589.6893 - val_mse: 589.6893 - val_mae: 18.4282\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 628.1103 - mse: 628.1103 - mae: 19.4067 - val_loss: 584.7678 - val_mse: 584.7678 - val_mae: 18.3501\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 623.6267 - mse: 623.6267 - mae: 19.3344 - val_loss: 579.8917 - val_mse: 579.8917 - val_mae: 18.2726\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 619.1727 - mse: 619.1727 - mae: 19.2644 - val_loss: 575.0969 - val_mse: 575.0969 - val_mae: 18.1964\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 614.7589 - mse: 614.7589 - mae: 19.1941 - val_loss: 570.3759 - val_mse: 570.3759 - val_mae: 18.1215\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 610.4644 - mse: 610.4644 - mae: 19.1255 - val_loss: 565.7411 - val_mse: 565.7411 - val_mae: 18.0473\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 606.2257 - mse: 606.2257 - mae: 19.0576 - val_loss: 561.1573 - val_mse: 561.1573 - val_mae: 17.9741\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 602.0043 - mse: 602.0043 - mae: 18.9912 - val_loss: 556.6738 - val_mse: 556.6738 - val_mae: 17.9022\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 597.8754 - mse: 597.8754 - mae: 18.9247 - val_loss: 552.2105 - val_mse: 552.2105 - val_mae: 17.8304\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 593.7806 - mse: 593.7806 - mae: 18.8593 - val_loss: 547.8292 - val_mse: 547.8292 - val_mae: 17.7595\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 589.7232 - mse: 589.7232 - mae: 18.7946 - val_loss: 543.5464 - val_mse: 543.5464 - val_mae: 17.6904\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 585.7883 - mse: 585.7883 - mae: 18.7298 - val_loss: 539.1891 - val_mse: 539.1891 - val_mae: 17.6202\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 581.7470 - mse: 581.7470 - mae: 18.6650 - val_loss: 534.9826 - val_mse: 534.9826 - val_mae: 17.5516\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 577.8893 - mse: 577.8893 - mae: 18.6010 - val_loss: 530.8028 - val_mse: 530.8028 - val_mae: 17.4832\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 574.0663 - mse: 574.0663 - mae: 18.5380 - val_loss: 526.5668 - val_mse: 526.5668 - val_mae: 17.4140\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 570.1522 - mse: 570.1522 - mae: 18.4752 - val_loss: 522.5621 - val_mse: 522.5621 - val_mae: 17.3477\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 566.4848 - mse: 566.4848 - mae: 18.4127 - val_loss: 518.4482 - val_mse: 518.4482 - val_mae: 17.2796\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 562.7132 - mse: 562.7132 - mae: 18.3502 - val_loss: 514.3923 - val_mse: 514.3923 - val_mae: 17.2118\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 559.0196 - mse: 559.0196 - mae: 18.2884 - val_loss: 510.4670 - val_mse: 510.4670 - val_mae: 17.1453\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 555.4420 - mse: 555.4420 - mae: 18.2273 - val_loss: 506.5420 - val_mse: 506.5420 - val_mae: 17.0790\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 551.8914 - mse: 551.8914 - mae: 18.1676 - val_loss: 502.6468 - val_mse: 502.6468 - val_mae: 17.0139\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 548.3814 - mse: 548.3814 - mae: 18.1079 - val_loss: 498.8546 - val_mse: 498.8546 - val_mae: 16.9501\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 544.9833 - mse: 544.9833 - mae: 18.0496 - val_loss: 495.0254 - val_mse: 495.0254 - val_mae: 16.8863\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 541.5603 - mse: 541.5603 - mae: 17.9910 - val_loss: 491.2978 - val_mse: 491.2978 - val_mae: 16.8233\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 538.2379 - mse: 538.2379 - mae: 17.9339 - val_loss: 487.6299 - val_mse: 487.6299 - val_mae: 16.7613\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 534.9508 - mse: 534.9508 - mae: 17.8781 - val_loss: 484.1516 - val_mse: 484.1516 - val_mae: 16.7020\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 531.7680 - mse: 531.7680 - mae: 17.8233 - val_loss: 480.6504 - val_mse: 480.6504 - val_mae: 16.6427\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 528.6016 - mse: 528.6016 - mae: 17.7686 - val_loss: 477.2614 - val_mse: 477.2614 - val_mae: 16.5852\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 525.5984 - mse: 525.5984 - mae: 17.7164 - val_loss: 473.7594 - val_mse: 473.7594 - val_mae: 16.5259\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 522.4845 - mse: 522.4845 - mae: 17.6631 - val_loss: 470.5042 - val_mse: 470.5042 - val_mae: 16.4700\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 519.5585 - mse: 519.5585 - mae: 17.6121 - val_loss: 467.1824 - val_mse: 467.1824 - val_mae: 16.4139\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 516.6343 - mse: 516.6343 - mae: 17.5618 - val_loss: 463.9478 - val_mse: 463.9478 - val_mae: 16.3594\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 513.7305 - mse: 513.7305 - mae: 17.5127 - val_loss: 460.7704 - val_mse: 460.7704 - val_mae: 16.3060\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 510.8743 - mse: 510.8743 - mae: 17.4631 - val_loss: 457.6232 - val_mse: 457.6232 - val_mae: 16.2533\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 508.0564 - mse: 508.0564 - mae: 17.4157 - val_loss: 454.5904 - val_mse: 454.5904 - val_mae: 16.2024\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 505.3406 - mse: 505.3406 - mae: 17.3693 - val_loss: 451.5317 - val_mse: 451.5317 - val_mae: 16.1510\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 502.6067 - mse: 502.6067 - mae: 17.3225 - val_loss: 448.5502 - val_mse: 448.5502 - val_mae: 16.1012\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 499.9549 - mse: 499.9549 - mae: 17.2779 - val_loss: 445.6180 - val_mse: 445.6180 - val_mae: 16.0527\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 497.3252 - mse: 497.3252 - mae: 17.2328 - val_loss: 442.6148 - val_mse: 442.6148 - val_mae: 16.0027\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 494.7172 - mse: 494.7172 - mae: 17.1887 - val_loss: 439.7391 - val_mse: 439.7391 - val_mae: 15.9551\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 492.1702 - mse: 492.1702 - mae: 17.1453 - val_loss: 436.9219 - val_mse: 436.9219 - val_mae: 15.9079\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 489.6830 - mse: 489.6830 - mae: 17.1026 - val_loss: 434.1531 - val_mse: 434.1531 - val_mae: 15.8616\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 487.2000 - mse: 487.2000 - mae: 17.0607 - val_loss: 431.4521 - val_mse: 431.4521 - val_mae: 15.8167\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 484.7604 - mse: 484.7604 - mae: 17.0200 - val_loss: 428.8280 - val_mse: 428.8280 - val_mae: 15.7736\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 482.4276 - mse: 482.4276 - mae: 16.9801 - val_loss: 426.0812 - val_mse: 426.0812 - val_mae: 15.7280\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 480.0219 - mse: 480.0219 - mae: 16.9382 - val_loss: 423.3767 - val_mse: 423.3767 - val_mae: 15.6827\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 477.5794 - mse: 477.5794 - mae: 16.8972 - val_loss: 420.8320 - val_mse: 420.8320 - val_mae: 15.6400\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 475.3057 - mse: 475.3057 - mae: 16.8580 - val_loss: 418.2458 - val_mse: 418.2458 - val_mae: 15.5963\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 472.9717 - mse: 472.9717 - mae: 16.8182 - val_loss: 415.7372 - val_mse: 415.7372 - val_mae: 15.5539\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 470.7394 - mse: 470.7394 - mae: 16.7801 - val_loss: 413.2047 - val_mse: 413.2047 - val_mae: 15.5108\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 468.5373 - mse: 468.5373 - mae: 16.7420 - val_loss: 410.7156 - val_mse: 410.7156 - val_mae: 15.4685\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 466.3274 - mse: 466.3274 - mae: 16.7046 - val_loss: 408.3300 - val_mse: 408.3300 - val_mae: 15.4280\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 464.1995 - mse: 464.1995 - mae: 16.6683 - val_loss: 405.9280 - val_mse: 405.9280 - val_mae: 15.3871\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 462.1197 - mse: 462.1197 - mae: 16.6329 - val_loss: 403.5135 - val_mse: 403.5135 - val_mae: 15.3460\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 459.9773 - mse: 459.9773 - mae: 16.5974 - val_loss: 401.2679 - val_mse: 401.2679 - val_mae: 15.3081\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 457.9636 - mse: 457.9636 - mae: 16.5636 - val_loss: 398.9375 - val_mse: 398.9375 - val_mae: 15.2686\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 455.9653 - mse: 455.9653 - mae: 16.5299 - val_loss: 396.6388 - val_mse: 396.6388 - val_mae: 15.2293\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 905.4229 - mse: 905.4229 - mae: 23.8309 - val_loss: 854.0578 - val_mse: 854.0578 - val_mae: 22.7193\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 897.9210 - mse: 897.9210 - mae: 23.7320 - val_loss: 845.8404 - val_mse: 845.8404 - val_mae: 22.6130\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 890.5864 - mse: 890.5864 - mae: 23.6345 - val_loss: 837.6734 - val_mse: 837.6734 - val_mae: 22.5075\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 883.2707 - mse: 883.2707 - mae: 23.5368 - val_loss: 829.6728 - val_mse: 829.6728 - val_mae: 22.4032\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 876.0587 - mse: 876.0587 - mae: 23.4401 - val_loss: 821.8439 - val_mse: 821.8439 - val_mae: 22.3002\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 868.9360 - mse: 868.9360 - mae: 23.3441 - val_loss: 814.1732 - val_mse: 814.1732 - val_mae: 22.1986\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 862.0043 - mse: 862.0043 - mae: 23.2503 - val_loss: 806.5371 - val_mse: 806.5371 - val_mae: 22.0975\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 855.2100 - mse: 855.2100 - mae: 23.1580 - val_loss: 798.9034 - val_mse: 798.9034 - val_mae: 21.9965\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 848.2435 - mse: 848.2435 - mae: 23.0643 - val_loss: 791.5630 - val_mse: 791.5630 - val_mae: 21.8981\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 841.6044 - mse: 841.6044 - mae: 22.9731 - val_loss: 784.3130 - val_mse: 784.3130 - val_mae: 21.8008\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 835.0269 - mse: 835.0269 - mae: 22.8829 - val_loss: 777.1323 - val_mse: 777.1323 - val_mae: 21.7038\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 828.4808 - mse: 828.4808 - mae: 22.7936 - val_loss: 770.1039 - val_mse: 770.1039 - val_mae: 21.6085\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 822.0626 - mse: 822.0626 - mae: 22.7051 - val_loss: 763.1638 - val_mse: 763.1638 - val_mae: 21.5147\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 815.8398 - mse: 815.8398 - mae: 22.6170 - val_loss: 756.2036 - val_mse: 756.2036 - val_mae: 21.4203\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 809.4905 - mse: 809.4905 - mae: 22.5296 - val_loss: 749.3954 - val_mse: 749.3954 - val_mae: 21.3270\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 803.2218 - mse: 803.2218 - mae: 22.4423 - val_loss: 742.7930 - val_mse: 742.7930 - val_mae: 21.2354\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 797.2057 - mse: 797.2057 - mae: 22.3574 - val_loss: 736.1495 - val_mse: 736.1495 - val_mae: 21.1434\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 791.1413 - mse: 791.1413 - mae: 22.2722 - val_loss: 729.6275 - val_mse: 729.6275 - val_mae: 21.0526\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 785.1774 - mse: 785.1774 - mae: 22.1879 - val_loss: 723.1677 - val_mse: 723.1677 - val_mae: 20.9628\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 779.2224 - mse: 779.2224 - mae: 22.1036 - val_loss: 716.8326 - val_mse: 716.8326 - val_mae: 20.8738\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 773.4003 - mse: 773.4003 - mae: 22.0204 - val_loss: 710.5353 - val_mse: 710.5353 - val_mae: 20.7851\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 767.6155 - mse: 767.6155 - mae: 21.9379 - val_loss: 704.2816 - val_mse: 704.2816 - val_mae: 20.6969\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 761.8616 - mse: 761.8616 - mae: 21.8555 - val_loss: 698.1302 - val_mse: 698.1302 - val_mae: 20.6098\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 756.2468 - mse: 756.2468 - mae: 21.7736 - val_loss: 692.0620 - val_mse: 692.0620 - val_mae: 20.5237\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 750.6841 - mse: 750.6841 - mae: 21.6934 - val_loss: 686.1555 - val_mse: 686.1555 - val_mae: 20.4392\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 745.2224 - mse: 745.2224 - mae: 21.6132 - val_loss: 680.2610 - val_mse: 680.2610 - val_mae: 20.3547\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 739.7255 - mse: 739.7255 - mae: 21.5339 - val_loss: 674.5125 - val_mse: 674.5125 - val_mae: 20.2718\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 734.4795 - mse: 734.4795 - mae: 21.4557 - val_loss: 668.6517 - val_mse: 668.6517 - val_mae: 20.1874\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 729.0612 - mse: 729.0612 - mae: 21.3773 - val_loss: 662.9858 - val_mse: 662.9858 - val_mae: 20.1046\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 723.8364 - mse: 723.8364 - mae: 21.3003 - val_loss: 657.3507 - val_mse: 657.3507 - val_mae: 20.0220\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 718.6154 - mse: 718.6154 - mae: 21.2228 - val_loss: 651.8738 - val_mse: 651.8738 - val_mae: 19.9409\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 713.5214 - mse: 713.5214 - mae: 21.1470 - val_loss: 646.4227 - val_mse: 646.4227 - val_mae: 19.8598\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 708.4630 - mse: 708.4630 - mae: 21.0716 - val_loss: 641.0806 - val_mse: 641.0806 - val_mae: 19.7794\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 703.4583 - mse: 703.4583 - mae: 20.9967 - val_loss: 635.7758 - val_mse: 635.7758 - val_mae: 19.6994\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 698.6174 - mse: 698.6174 - mae: 20.9224 - val_loss: 630.3720 - val_mse: 630.3720 - val_mae: 19.6182\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 693.5712 - mse: 693.5712 - mae: 20.8470 - val_loss: 625.2380 - val_mse: 625.2380 - val_mae: 19.5393\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 688.7830 - mse: 688.7830 - mae: 20.7730 - val_loss: 620.0167 - val_mse: 620.0167 - val_mae: 19.4587\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 683.9407 - mse: 683.9407 - mae: 20.6986 - val_loss: 614.8558 - val_mse: 614.8558 - val_mae: 19.3785\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 679.1860 - mse: 679.1860 - mae: 20.6247 - val_loss: 609.7399 - val_mse: 609.7399 - val_mae: 19.2985\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 674.4452 - mse: 674.4452 - mae: 20.5513 - val_loss: 604.6506 - val_mse: 604.6506 - val_mae: 19.2181\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 669.7059 - mse: 669.7059 - mae: 20.4781 - val_loss: 599.6811 - val_mse: 599.6811 - val_mae: 19.1387\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 665.0733 - mse: 665.0733 - mae: 20.4044 - val_loss: 594.7338 - val_mse: 594.7338 - val_mae: 19.0592\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 660.4890 - mse: 660.4890 - mae: 20.3320 - val_loss: 589.7954 - val_mse: 589.7954 - val_mae: 18.9793\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 655.9888 - mse: 655.9888 - mae: 20.2593 - val_loss: 584.8816 - val_mse: 584.8816 - val_mae: 18.8999\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 651.3989 - mse: 651.3989 - mae: 20.1871 - val_loss: 580.1854 - val_mse: 580.1854 - val_mae: 18.8225\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 646.9919 - mse: 646.9919 - mae: 20.1144 - val_loss: 575.3995 - val_mse: 575.3995 - val_mae: 18.7437\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 642.5305 - mse: 642.5305 - mae: 20.0426 - val_loss: 570.6681 - val_mse: 570.6681 - val_mae: 18.6651\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 638.1591 - mse: 638.1591 - mae: 19.9707 - val_loss: 565.9340 - val_mse: 565.9340 - val_mae: 18.5866\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 633.8271 - mse: 633.8271 - mae: 19.8991 - val_loss: 561.2536 - val_mse: 561.2536 - val_mae: 18.5084\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 629.4720 - mse: 629.4720 - mae: 19.8274 - val_loss: 556.6916 - val_mse: 556.6916 - val_mae: 18.4319\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 625.1831 - mse: 625.1831 - mae: 19.7562 - val_loss: 552.1251 - val_mse: 552.1251 - val_mae: 18.3547\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 620.9371 - mse: 620.9371 - mae: 19.6846 - val_loss: 547.5590 - val_mse: 547.5590 - val_mae: 18.2773\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 616.7149 - mse: 616.7150 - mae: 19.6136 - val_loss: 543.0470 - val_mse: 543.0470 - val_mae: 18.2009\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 612.4718 - mse: 612.4718 - mae: 19.5427 - val_loss: 538.6191 - val_mse: 538.6191 - val_mae: 18.1251\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 608.3324 - mse: 608.3324 - mae: 19.4716 - val_loss: 534.1599 - val_mse: 534.1599 - val_mae: 18.0483\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 604.2449 - mse: 604.2449 - mae: 19.4010 - val_loss: 529.7802 - val_mse: 529.7802 - val_mae: 17.9730\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 600.1600 - mse: 600.1600 - mae: 19.3306 - val_loss: 525.5105 - val_mse: 525.5105 - val_mae: 17.8989\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 596.1636 - mse: 596.1636 - mae: 19.2614 - val_loss: 521.2870 - val_mse: 521.2870 - val_mae: 17.8250\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 592.1526 - mse: 592.1526 - mae: 19.1924 - val_loss: 517.1021 - val_mse: 517.1021 - val_mae: 17.7516\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 588.2325 - mse: 588.2325 - mae: 19.1232 - val_loss: 512.9973 - val_mse: 512.9973 - val_mae: 17.6791\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 584.3035 - mse: 584.3035 - mae: 19.0543 - val_loss: 508.9523 - val_mse: 508.9523 - val_mae: 17.6075\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 580.4803 - mse: 580.4803 - mae: 18.9863 - val_loss: 504.9179 - val_mse: 504.9179 - val_mae: 17.5360\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 576.6820 - mse: 576.6820 - mae: 18.9191 - val_loss: 500.9352 - val_mse: 500.9352 - val_mae: 17.4650\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 572.9398 - mse: 572.9398 - mae: 18.8534 - val_loss: 497.0497 - val_mse: 497.0497 - val_mae: 17.3951\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 569.3375 - mse: 569.3375 - mae: 18.7885 - val_loss: 493.1395 - val_mse: 493.1395 - val_mae: 17.3249\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 565.7849 - mse: 565.7849 - mae: 18.7243 - val_loss: 489.2729 - val_mse: 489.2729 - val_mae: 17.2556\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 562.1924 - mse: 562.1924 - mae: 18.6608 - val_loss: 485.6207 - val_mse: 485.6207 - val_mae: 17.1891\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 558.7995 - mse: 558.7995 - mae: 18.5993 - val_loss: 481.9253 - val_mse: 481.9253 - val_mae: 17.1222\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 555.3897 - mse: 555.3897 - mae: 18.5384 - val_loss: 478.3490 - val_mse: 478.3490 - val_mae: 17.0566\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 552.0587 - mse: 552.0587 - mae: 18.4782 - val_loss: 474.8080 - val_mse: 474.8080 - val_mae: 16.9913\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 548.7978 - mse: 548.7978 - mae: 18.4184 - val_loss: 471.3344 - val_mse: 471.3344 - val_mae: 16.9272\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 545.5869 - mse: 545.5869 - mae: 18.3602 - val_loss: 467.8241 - val_mse: 467.8241 - val_mae: 16.8619\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 542.3586 - mse: 542.3586 - mae: 18.3021 - val_loss: 464.4676 - val_mse: 464.4676 - val_mae: 16.7988\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 539.2319 - mse: 539.2319 - mae: 18.2447 - val_loss: 461.1289 - val_mse: 461.1289 - val_mae: 16.7360\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 536.1662 - mse: 536.1662 - mae: 18.1887 - val_loss: 457.8084 - val_mse: 457.8084 - val_mae: 16.6741\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 533.1161 - mse: 533.1161 - mae: 18.1333 - val_loss: 454.4827 - val_mse: 454.4827 - val_mae: 16.6118\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 530.1063 - mse: 530.1063 - mae: 18.0775 - val_loss: 451.2238 - val_mse: 451.2238 - val_mae: 16.5508\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 527.0632 - mse: 527.0632 - mae: 18.0230 - val_loss: 448.1658 - val_mse: 448.1658 - val_mae: 16.4932\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 524.1799 - mse: 524.1799 - mae: 17.9691 - val_loss: 444.9568 - val_mse: 444.9568 - val_mae: 16.4332\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 521.2397 - mse: 521.2397 - mae: 17.9154 - val_loss: 441.8644 - val_mse: 441.8644 - val_mae: 16.3751\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 518.3723 - mse: 518.3723 - mae: 17.8638 - val_loss: 438.8238 - val_mse: 438.8238 - val_mae: 16.3179\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 515.5342 - mse: 515.5342 - mae: 17.8116 - val_loss: 435.7747 - val_mse: 435.7747 - val_mae: 16.2602\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 512.7141 - mse: 512.7141 - mae: 17.7609 - val_loss: 432.7600 - val_mse: 432.7600 - val_mae: 16.2031\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 509.9373 - mse: 509.9373 - mae: 17.7094 - val_loss: 429.8257 - val_mse: 429.8257 - val_mae: 16.1467\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 507.2172 - mse: 507.2172 - mae: 17.6592 - val_loss: 426.8707 - val_mse: 426.8707 - val_mae: 16.0903\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 504.5345 - mse: 504.5345 - mae: 17.6097 - val_loss: 423.9483 - val_mse: 423.9483 - val_mae: 16.0345\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 501.8553 - mse: 501.8553 - mae: 17.5602 - val_loss: 421.0538 - val_mse: 421.0538 - val_mae: 15.9787\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 499.2016 - mse: 499.2016 - mae: 17.5121 - val_loss: 418.2467 - val_mse: 418.2467 - val_mae: 15.9242\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 496.6422 - mse: 496.6422 - mae: 17.4640 - val_loss: 415.4734 - val_mse: 415.4734 - val_mae: 15.8708\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 494.0526 - mse: 494.0526 - mae: 17.4169 - val_loss: 412.7736 - val_mse: 412.7736 - val_mae: 15.8189\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 491.5644 - mse: 491.5644 - mae: 17.3708 - val_loss: 410.0117 - val_mse: 410.0117 - val_mae: 15.7669\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 489.0050 - mse: 489.0050 - mae: 17.3246 - val_loss: 407.3770 - val_mse: 407.3770 - val_mae: 15.7164\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 486.6089 - mse: 486.6089 - mae: 17.2797 - val_loss: 404.6875 - val_mse: 404.6875 - val_mae: 15.6650\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 484.1351 - mse: 484.1351 - mae: 17.2336 - val_loss: 402.0782 - val_mse: 402.0782 - val_mae: 15.6145\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 481.6741 - mse: 481.6741 - mae: 17.1891 - val_loss: 399.4920 - val_mse: 399.4920 - val_mae: 15.5652\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 479.2772 - mse: 479.2772 - mae: 17.1442 - val_loss: 396.7971 - val_mse: 396.7971 - val_mae: 15.5136\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 476.7999 - mse: 476.7999 - mae: 17.0989 - val_loss: 394.1909 - val_mse: 394.1909 - val_mae: 15.4628\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 474.3971 - mse: 474.3971 - mae: 17.0541 - val_loss: 391.6145 - val_mse: 391.6145 - val_mae: 15.4126\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 471.9839 - mse: 471.9839 - mae: 17.0094 - val_loss: 389.0241 - val_mse: 389.0241 - val_mae: 15.3617\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 469.6334 - mse: 469.6334 - mae: 16.9647 - val_loss: 386.4453 - val_mse: 386.4453 - val_mae: 15.3107\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 953.8212 - mse: 953.8212 - mae: 24.1832 - val_loss: 946.6819 - val_mse: 946.6819 - val_mae: 24.0743\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 944.5660 - mse: 944.5660 - mae: 24.0642 - val_loss: 937.5243 - val_mse: 937.5243 - val_mae: 23.9550\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 935.6124 - mse: 935.6124 - mae: 23.9477 - val_loss: 928.3817 - val_mse: 928.3817 - val_mae: 23.8347\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 926.7282 - mse: 926.7282 - mae: 23.8318 - val_loss: 919.3797 - val_mse: 919.3797 - val_mae: 23.7155\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 917.9279 - mse: 917.9279 - mae: 23.7152 - val_loss: 910.4837 - val_mse: 910.4837 - val_mae: 23.5978\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 909.0833 - mse: 909.0833 - mae: 23.6001 - val_loss: 901.7840 - val_mse: 901.7840 - val_mae: 23.4819\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 900.4881 - mse: 900.4881 - mae: 23.4869 - val_loss: 893.1547 - val_mse: 893.1547 - val_mae: 23.3664\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 892.0295 - mse: 892.0295 - mae: 23.3736 - val_loss: 884.6281 - val_mse: 884.6281 - val_mae: 23.2521\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 883.5852 - mse: 883.5852 - mae: 23.2619 - val_loss: 876.2151 - val_mse: 876.2151 - val_mae: 23.1384\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 875.3677 - mse: 875.3677 - mae: 23.1508 - val_loss: 867.8168 - val_mse: 867.8168 - val_mae: 23.0249\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 867.0563 - mse: 867.0563 - mae: 23.0390 - val_loss: 859.5709 - val_mse: 859.5709 - val_mae: 22.9124\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 858.9733 - mse: 858.9733 - mae: 22.9282 - val_loss: 851.4293 - val_mse: 851.4293 - val_mae: 22.8005\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 851.0722 - mse: 851.0722 - mae: 22.8178 - val_loss: 843.2445 - val_mse: 843.2445 - val_mae: 22.6872\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 843.0154 - mse: 843.0154 - mae: 22.7065 - val_loss: 835.2403 - val_mse: 835.2403 - val_mae: 22.5748\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 835.0890 - mse: 835.0890 - mae: 22.5965 - val_loss: 827.3616 - val_mse: 827.3616 - val_mae: 22.4627\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 827.2773 - mse: 827.2773 - mae: 22.4873 - val_loss: 819.5922 - val_mse: 819.5922 - val_mae: 22.3516\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 819.5876 - mse: 819.5876 - mae: 22.3775 - val_loss: 811.8937 - val_mse: 811.8937 - val_mae: 22.2412\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 812.0005 - mse: 812.0005 - mae: 22.2679 - val_loss: 804.1428 - val_mse: 804.1428 - val_mae: 22.1284\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 804.3969 - mse: 804.3969 - mae: 22.1585 - val_loss: 796.4462 - val_mse: 796.4462 - val_mae: 22.0156\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 796.8550 - mse: 796.8550 - mae: 22.0485 - val_loss: 788.8808 - val_mse: 788.8808 - val_mae: 21.9053\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 789.2903 - mse: 789.2903 - mae: 21.9393 - val_loss: 781.4052 - val_mse: 781.4052 - val_mae: 21.7936\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 782.0503 - mse: 782.0503 - mae: 21.8310 - val_loss: 773.9907 - val_mse: 773.9907 - val_mae: 21.6832\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 774.6933 - mse: 774.6933 - mae: 21.7229 - val_loss: 766.7068 - val_mse: 766.7068 - val_mae: 21.5734\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 767.5226 - mse: 767.5226 - mae: 21.6156 - val_loss: 759.4715 - val_mse: 759.4715 - val_mae: 21.4635\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 760.3766 - mse: 760.3766 - mae: 21.5081 - val_loss: 752.3726 - val_mse: 752.3726 - val_mae: 21.3552\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 753.2993 - mse: 753.2993 - mae: 21.4003 - val_loss: 745.3154 - val_mse: 745.3154 - val_mae: 21.2465\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 746.3639 - mse: 746.3639 - mae: 21.2927 - val_loss: 738.1747 - val_mse: 738.1747 - val_mae: 21.1358\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 739.3851 - mse: 739.3851 - mae: 21.1837 - val_loss: 731.0312 - val_mse: 731.0312 - val_mae: 21.0238\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 732.2979 - mse: 732.2979 - mae: 21.0730 - val_loss: 724.0809 - val_mse: 724.0809 - val_mae: 20.9134\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 725.3900 - mse: 725.3900 - mae: 20.9626 - val_loss: 717.1265 - val_mse: 717.1265 - val_mae: 20.8020\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 718.5209 - mse: 718.5209 - mae: 20.8515 - val_loss: 710.0723 - val_mse: 710.0723 - val_mae: 20.6874\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 711.7275 - mse: 711.7275 - mae: 20.7393 - val_loss: 703.0525 - val_mse: 703.0525 - val_mae: 20.5726\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 704.8149 - mse: 704.8149 - mae: 20.6264 - val_loss: 696.1753 - val_mse: 696.1753 - val_mae: 20.4588\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 698.0770 - mse: 698.0770 - mae: 20.5132 - val_loss: 689.3286 - val_mse: 689.3286 - val_mae: 20.3447\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 691.3828 - mse: 691.3828 - mae: 20.4018 - val_loss: 682.5511 - val_mse: 682.5511 - val_mae: 20.2303\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 684.6678 - mse: 684.6678 - mae: 20.2904 - val_loss: 675.8935 - val_mse: 675.8935 - val_mae: 20.1162\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 678.1267 - mse: 678.1267 - mae: 20.1781 - val_loss: 669.3542 - val_mse: 669.3542 - val_mae: 20.0039\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 671.7103 - mse: 671.7103 - mae: 20.0699 - val_loss: 662.8239 - val_mse: 662.8239 - val_mae: 19.8912\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 665.4238 - mse: 665.4238 - mae: 19.9624 - val_loss: 656.3372 - val_mse: 656.3372 - val_mae: 19.7796\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 659.1420 - mse: 659.1420 - mae: 19.8541 - val_loss: 650.0496 - val_mse: 650.0496 - val_mae: 19.6724\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 652.9409 - mse: 652.9409 - mae: 19.7496 - val_loss: 643.9609 - val_mse: 643.9609 - val_mae: 19.5675\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 646.9174 - mse: 646.9174 - mae: 19.6468 - val_loss: 637.9543 - val_mse: 637.9543 - val_mae: 19.4639\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 641.0757 - mse: 641.0757 - mae: 19.5452 - val_loss: 631.9797 - val_mse: 631.9797 - val_mae: 19.3607\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 635.2234 - mse: 635.2234 - mae: 19.4449 - val_loss: 626.1064 - val_mse: 626.1064 - val_mae: 19.2589\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 629.5245 - mse: 629.5245 - mae: 19.3456 - val_loss: 620.3078 - val_mse: 620.3078 - val_mae: 19.1584\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 623.8038 - mse: 623.8038 - mae: 19.2476 - val_loss: 614.6711 - val_mse: 614.6711 - val_mae: 19.0607\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 618.3243 - mse: 618.3243 - mae: 19.1526 - val_loss: 609.0649 - val_mse: 609.0649 - val_mae: 18.9640\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 612.8632 - mse: 612.8632 - mae: 19.0576 - val_loss: 603.5109 - val_mse: 603.5109 - val_mae: 18.8689\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 607.4666 - mse: 607.4666 - mae: 18.9631 - val_loss: 597.9987 - val_mse: 597.9987 - val_mae: 18.7739\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 602.0056 - mse: 602.0056 - mae: 18.8697 - val_loss: 592.6472 - val_mse: 592.6472 - val_mae: 18.6816\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 596.7427 - mse: 596.7427 - mae: 18.7778 - val_loss: 587.2890 - val_mse: 587.2890 - val_mae: 18.5887\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 591.4880 - mse: 591.4880 - mae: 18.6863 - val_loss: 581.9792 - val_mse: 581.9792 - val_mae: 18.4961\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 586.3613 - mse: 586.3613 - mae: 18.5958 - val_loss: 576.6732 - val_mse: 576.6732 - val_mae: 18.4036\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 581.1933 - mse: 581.1933 - mae: 18.5042 - val_loss: 571.5075 - val_mse: 571.5075 - val_mae: 18.3135\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 576.0876 - mse: 576.0876 - mae: 18.4140 - val_loss: 566.4651 - val_mse: 566.4651 - val_mae: 18.2254\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 571.1442 - mse: 571.1442 - mae: 18.3263 - val_loss: 561.4454 - val_mse: 561.4454 - val_mae: 18.1372\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 566.1729 - mse: 566.1729 - mae: 18.2381 - val_loss: 556.5024 - val_mse: 556.5024 - val_mae: 18.0499\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 561.3389 - mse: 561.3389 - mae: 18.1523 - val_loss: 551.6619 - val_mse: 551.6619 - val_mae: 17.9642\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 556.6044 - mse: 556.6044 - mae: 18.0678 - val_loss: 546.9590 - val_mse: 546.9590 - val_mae: 17.8812\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 551.9470 - mse: 551.9470 - mae: 17.9850 - val_loss: 542.3303 - val_mse: 542.3303 - val_mae: 17.7986\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 547.4559 - mse: 547.4559 - mae: 17.9033 - val_loss: 537.7357 - val_mse: 537.7357 - val_mae: 17.7173\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 542.8245 - mse: 542.8245 - mae: 17.8224 - val_loss: 533.3036 - val_mse: 533.3036 - val_mae: 17.6372\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 538.4078 - mse: 538.4078 - mae: 17.7433 - val_loss: 528.9459 - val_mse: 528.9459 - val_mae: 17.5591\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 534.1252 - mse: 534.1252 - mae: 17.6658 - val_loss: 524.5524 - val_mse: 524.5524 - val_mae: 17.4815\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 529.7042 - mse: 529.7042 - mae: 17.5874 - val_loss: 520.3392 - val_mse: 520.3392 - val_mae: 17.4059\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 525.5059 - mse: 525.5059 - mae: 17.5111 - val_loss: 516.1313 - val_mse: 516.1313 - val_mae: 17.3307\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 521.3124 - mse: 521.3124 - mae: 17.4367 - val_loss: 511.9957 - val_mse: 511.9957 - val_mae: 17.2564\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 517.2177 - mse: 517.2177 - mae: 17.3623 - val_loss: 507.9416 - val_mse: 507.9416 - val_mae: 17.1834\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 513.1600 - mse: 513.1600 - mae: 17.2900 - val_loss: 503.9321 - val_mse: 503.9321 - val_mae: 17.1115\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 509.2163 - mse: 509.2163 - mae: 17.2178 - val_loss: 500.0057 - val_mse: 500.0057 - val_mae: 17.0405\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 505.3423 - mse: 505.3423 - mae: 17.1471 - val_loss: 496.1122 - val_mse: 496.1122 - val_mae: 16.9703\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 501.4583 - mse: 501.4583 - mae: 17.0776 - val_loss: 492.3473 - val_mse: 492.3473 - val_mae: 16.9024\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 497.7287 - mse: 497.7287 - mae: 17.0091 - val_loss: 488.6002 - val_mse: 488.6002 - val_mae: 16.8356\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 493.9748 - mse: 493.9748 - mae: 16.9415 - val_loss: 484.9344 - val_mse: 484.9344 - val_mae: 16.7702\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 490.3472 - mse: 490.3472 - mae: 16.8754 - val_loss: 481.2827 - val_mse: 481.2827 - val_mae: 16.7057\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 486.6689 - mse: 486.6689 - mae: 16.8098 - val_loss: 477.7064 - val_mse: 477.7064 - val_mae: 16.6418\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 483.1181 - mse: 483.1181 - mae: 16.7444 - val_loss: 474.2079 - val_mse: 474.2079 - val_mae: 16.5796\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 479.6237 - mse: 479.6237 - mae: 16.6814 - val_loss: 470.7507 - val_mse: 470.7507 - val_mae: 16.5182\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 476.1598 - mse: 476.1598 - mae: 16.6183 - val_loss: 467.3608 - val_mse: 467.3608 - val_mae: 16.4578\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 472.7854 - mse: 472.7854 - mae: 16.5566 - val_loss: 463.9561 - val_mse: 463.9561 - val_mae: 16.3981\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 469.4505 - mse: 469.4505 - mae: 16.4956 - val_loss: 460.5837 - val_mse: 460.5837 - val_mae: 16.3387\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 466.0432 - mse: 466.0432 - mae: 16.4347 - val_loss: 457.3549 - val_mse: 457.3549 - val_mae: 16.2806\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 462.8258 - mse: 462.8258 - mae: 16.3754 - val_loss: 454.1747 - val_mse: 454.1747 - val_mae: 16.2234\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 459.6538 - mse: 459.6538 - mae: 16.3169 - val_loss: 451.0411 - val_mse: 451.0411 - val_mae: 16.1668\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 456.4377 - mse: 456.4377 - mae: 16.2592 - val_loss: 447.9816 - val_mse: 447.9816 - val_mae: 16.1112\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 453.3627 - mse: 453.3627 - mae: 16.2023 - val_loss: 444.9074 - val_mse: 444.9074 - val_mae: 16.0559\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 450.3039 - mse: 450.3039 - mae: 16.1453 - val_loss: 441.8080 - val_mse: 441.8080 - val_mae: 16.0004\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 447.2185 - mse: 447.2185 - mae: 16.0891 - val_loss: 438.8035 - val_mse: 438.8035 - val_mae: 15.9469\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 444.2221 - mse: 444.2221 - mae: 16.0340 - val_loss: 435.8390 - val_mse: 435.8390 - val_mae: 15.8942\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 441.2322 - mse: 441.2322 - mae: 15.9791 - val_loss: 432.9051 - val_mse: 432.9051 - val_mae: 15.8417\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 438.3374 - mse: 438.3374 - mae: 15.9255 - val_loss: 430.0009 - val_mse: 430.0009 - val_mae: 15.7900\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 435.4303 - mse: 435.4303 - mae: 15.8720 - val_loss: 427.1631 - val_mse: 427.1631 - val_mae: 15.7392\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 432.5926 - mse: 432.5926 - mae: 15.8192 - val_loss: 424.3799 - val_mse: 424.3799 - val_mae: 15.6889\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 429.7599 - mse: 429.7599 - mae: 15.7673 - val_loss: 421.6699 - val_mse: 421.6699 - val_mae: 15.6400\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 427.0615 - mse: 427.0615 - mae: 15.7169 - val_loss: 418.8749 - val_mse: 418.8749 - val_mae: 15.5902\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 424.2794 - mse: 424.2794 - mae: 15.6655 - val_loss: 416.1616 - val_mse: 416.1616 - val_mae: 15.5413\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 421.6494 - mse: 421.6494 - mae: 15.6165 - val_loss: 413.4754 - val_mse: 413.4754 - val_mae: 15.4937\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 418.9134 - mse: 418.9134 - mae: 15.5658 - val_loss: 410.8819 - val_mse: 410.8819 - val_mae: 15.4470\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 416.3617 - mse: 416.3617 - mae: 15.5171 - val_loss: 408.2791 - val_mse: 408.2791 - val_mae: 15.3998\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 413.7509 - mse: 413.7509 - mae: 15.4693 - val_loss: 405.7799 - val_mse: 405.7799 - val_mae: 15.3542\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: 1326.0114 - mse: 1326.0114 - mae: 23.1440 - val_loss: 323.1628 - val_mse: 323.1628 - val_mae: 16.2522\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 391.8821 - mse: 391.8821 - mae: 16.6290 - val_loss: 278.8870 - val_mse: 278.8870 - val_mae: 14.5938\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 375.3069 - mse: 375.3069 - mae: 15.9315 - val_loss: 235.0392 - val_mse: 235.0392 - val_mae: 12.3386\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 352.5410 - mse: 352.5410 - mae: 15.2989 - val_loss: 279.4518 - val_mse: 279.4518 - val_mae: 14.3923\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 341.9779 - mse: 341.9779 - mae: 14.7552 - val_loss: 235.2323 - val_mse: 235.2323 - val_mae: 12.4628\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 331.0079 - mse: 331.0079 - mae: 14.3234 - val_loss: 227.6568 - val_mse: 227.6568 - val_mae: 11.7996\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 329.4137 - mse: 329.4137 - mae: 14.2070 - val_loss: 226.3727 - val_mse: 226.3727 - val_mae: 11.6898\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 328.5513 - mse: 328.5513 - mae: 14.1022 - val_loss: 234.4954 - val_mse: 234.4954 - val_mae: 12.3795\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 328.8527 - mse: 328.8527 - mae: 14.1505 - val_loss: 260.2201 - val_mse: 260.2201 - val_mae: 13.8263\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 332.8490 - mse: 332.8490 - mae: 14.4236 - val_loss: 235.8902 - val_mse: 235.8902 - val_mae: 12.4179\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 333.1465 - mse: 333.1465 - mae: 14.4854 - val_loss: 226.7397 - val_mse: 226.7397 - val_mae: 11.6600\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 330.9715 - mse: 330.9715 - mae: 14.3180 - val_loss: 231.3266 - val_mse: 231.3266 - val_mae: 11.9742\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 336.5306 - mse: 336.5306 - mae: 14.7285 - val_loss: 230.0949 - val_mse: 230.0949 - val_mae: 11.8796\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 329.9327 - mse: 329.9327 - mae: 14.2889 - val_loss: 226.9466 - val_mse: 226.9466 - val_mae: 11.6479\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 324.3355 - mse: 324.3355 - mae: 14.0901 - val_loss: 210.5950 - val_mse: 210.5950 - val_mae: 11.0059\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 309.3818 - mse: 309.3818 - mae: 13.6033 - val_loss: 218.2190 - val_mse: 218.2190 - val_mae: 11.4297\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 310.7510 - mse: 310.7510 - mae: 13.6919 - val_loss: 209.9304 - val_mse: 209.9304 - val_mae: 11.0047\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 306.7240 - mse: 306.7240 - mae: 13.5337 - val_loss: 212.3789 - val_mse: 212.3789 - val_mae: 11.3273\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 311.2802 - mse: 311.2802 - mae: 13.9153 - val_loss: 209.6039 - val_mse: 209.6039 - val_mae: 11.0148\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 304.7883 - mse: 304.7883 - mae: 13.3546 - val_loss: 210.8823 - val_mse: 210.8823 - val_mae: 11.1784\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 304.8292 - mse: 304.8292 - mae: 13.4450 - val_loss: 227.5358 - val_mse: 227.5358 - val_mae: 12.4349\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 309.6234 - mse: 309.6234 - mae: 13.7236 - val_loss: 214.2618 - val_mse: 214.2618 - val_mae: 11.4785\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 319.3835 - mse: 319.3835 - mae: 14.1882 - val_loss: 210.3636 - val_mse: 210.3636 - val_mae: 11.1110\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 309.7895 - mse: 309.7895 - mae: 13.5336 - val_loss: 208.3340 - val_mse: 208.3340 - val_mae: 10.9038\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 307.5984 - mse: 307.5984 - mae: 13.4987 - val_loss: 208.7740 - val_mse: 208.7740 - val_mae: 10.8881\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 306.6226 - mse: 306.6226 - mae: 13.4840 - val_loss: 213.3176 - val_mse: 213.3176 - val_mae: 11.3894\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 307.9305 - mse: 307.9305 - mae: 13.6752 - val_loss: 215.0548 - val_mse: 215.0548 - val_mae: 11.5127\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 305.3875 - mse: 305.3875 - mae: 13.4580 - val_loss: 207.6782 - val_mse: 207.6782 - val_mae: 10.8622\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 310.7874 - mse: 310.7874 - mae: 13.8123 - val_loss: 209.8577 - val_mse: 209.8577 - val_mae: 11.0080\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 306.5674 - mse: 306.5674 - mae: 13.4560 - val_loss: 207.6162 - val_mse: 207.6162 - val_mae: 10.8146\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 301.7761 - mse: 301.7761 - mae: 13.3112 - val_loss: 208.5510 - val_mse: 208.5510 - val_mae: 10.8180\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 304.5312 - mse: 304.5312 - mae: 13.2339 - val_loss: 218.6542 - val_mse: 218.6542 - val_mae: 11.7723\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 304.0959 - mse: 304.0959 - mae: 13.5105 - val_loss: 214.8951 - val_mse: 214.8951 - val_mae: 11.4841\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 304.1436 - mse: 304.1436 - mae: 13.4497 - val_loss: 231.4901 - val_mse: 231.4901 - val_mae: 12.6951\n",
            "Epoch 34: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 934.3169 - mse: 934.3169 - mae: 23.3623 - val_loss: 355.6827 - val_mse: 355.6827 - val_mae: 17.1035\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 476.9916 - mse: 476.9916 - mae: 18.6569 - val_loss: 316.6860 - val_mse: 316.6860 - val_mae: 15.6610\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 466.1818 - mse: 466.1818 - mae: 18.2927 - val_loss: 338.8195 - val_mse: 338.8195 - val_mae: 16.5843\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 454.6018 - mse: 454.6018 - mae: 17.9833 - val_loss: 307.1968 - val_mse: 307.1968 - val_mae: 15.2086\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 449.3331 - mse: 449.3331 - mae: 17.8365 - val_loss: 297.5225 - val_mse: 297.5225 - val_mae: 14.4943\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 444.1919 - mse: 444.1919 - mae: 17.4175 - val_loss: 296.6091 - val_mse: 296.6091 - val_mae: 14.3699\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 442.3335 - mse: 442.3335 - mae: 17.3846 - val_loss: 298.3828 - val_mse: 298.3828 - val_mae: 14.6491\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 439.6610 - mse: 439.6610 - mae: 17.3148 - val_loss: 298.0495 - val_mse: 298.0495 - val_mae: 14.6075\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 436.6014 - mse: 436.6014 - mae: 17.1022 - val_loss: 304.9303 - val_mse: 304.9303 - val_mae: 15.1225\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 436.4517 - mse: 436.4517 - mae: 17.1384 - val_loss: 300.6486 - val_mse: 300.6486 - val_mae: 14.8284\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 436.9388 - mse: 436.9388 - mae: 17.1862 - val_loss: 298.3690 - val_mse: 298.3690 - val_mae: 14.6642\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 436.7720 - mse: 436.7720 - mae: 17.1471 - val_loss: 295.3752 - val_mse: 295.3752 - val_mae: 14.3758\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 438.1329 - mse: 438.1329 - mae: 17.2300 - val_loss: 292.4313 - val_mse: 292.4313 - val_mae: 14.0533\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 435.0351 - mse: 435.0351 - mae: 16.9911 - val_loss: 293.5416 - val_mse: 293.5416 - val_mae: 14.1926\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 435.1618 - mse: 435.1618 - mae: 17.0383 - val_loss: 293.4140 - val_mse: 293.4140 - val_mae: 14.1643\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 433.9961 - mse: 433.9961 - mae: 16.9871 - val_loss: 292.5210 - val_mse: 292.5210 - val_mae: 14.0302\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 434.7225 - mse: 434.7225 - mae: 16.9618 - val_loss: 293.9762 - val_mse: 293.9762 - val_mae: 14.2455\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 434.2450 - mse: 434.2450 - mae: 16.9592 - val_loss: 295.1315 - val_mse: 295.1315 - val_mae: 14.3252\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 437.5053 - mse: 437.5053 - mae: 17.2040 - val_loss: 292.6240 - val_mse: 292.6240 - val_mae: 14.0313\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 436.6630 - mse: 436.6630 - mae: 17.0444 - val_loss: 292.6501 - val_mse: 292.6501 - val_mae: 14.0787\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 436.6819 - mse: 436.6819 - mae: 17.0759 - val_loss: 293.3321 - val_mse: 293.3321 - val_mae: 14.1800\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.5786 - mse: 406.5786 - mae: 16.4045 - val_loss: 207.5073 - val_mse: 207.5073 - val_mae: 11.4393\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 341.0473 - mse: 341.0473 - mae: 14.5987 - val_loss: 214.8985 - val_mse: 214.8985 - val_mae: 11.4827\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 333.3494 - mse: 333.3494 - mae: 14.2671 - val_loss: 267.1720 - val_mse: 267.1720 - val_mae: 14.2460\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 341.8696 - mse: 341.8696 - mae: 14.7974 - val_loss: 212.1445 - val_mse: 212.1445 - val_mae: 11.2234\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 337.1717 - mse: 337.1717 - mae: 14.5423 - val_loss: 214.0854 - val_mse: 214.0854 - val_mae: 11.3984\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 340.3705 - mse: 340.3705 - mae: 14.5422 - val_loss: 210.4999 - val_mse: 210.4999 - val_mae: 11.2486\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 349.4305 - mse: 349.4305 - mae: 15.0773 - val_loss: 237.9074 - val_mse: 237.9074 - val_mae: 13.1199\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 351.1954 - mse: 351.1954 - mae: 14.9681 - val_loss: 218.9024 - val_mse: 218.9024 - val_mae: 11.8386\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 331.9688 - mse: 331.9688 - mae: 14.3167 - val_loss: 210.4982 - val_mse: 210.4982 - val_mae: 11.2523\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 332.4226 - mse: 332.4226 - mae: 14.3150 - val_loss: 210.1783 - val_mse: 210.1783 - val_mae: 11.1812\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 328.4289 - mse: 328.4289 - mae: 13.9473 - val_loss: 229.3875 - val_mse: 229.3875 - val_mae: 12.1978\n",
            "Epoch 32: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: 1502.1962 - mse: 1502.1962 - mae: 26.6881 - val_loss: 537.8090 - val_mse: 537.8090 - val_mae: 21.4691\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 519.1551 - mse: 519.1551 - mae: 20.7476 - val_loss: 432.8337 - val_mse: 432.8337 - val_mae: 18.2691\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 432.7975 - mse: 432.7975 - mae: 18.0805 - val_loss: 405.8330 - val_mse: 405.8330 - val_mae: 17.4891\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 412.5018 - mse: 412.5018 - mae: 17.4380 - val_loss: 386.7540 - val_mse: 386.7540 - val_mae: 16.7901\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 395.5970 - mse: 395.5970 - mae: 16.8999 - val_loss: 379.0170 - val_mse: 379.0170 - val_mae: 16.6058\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 389.9740 - mse: 389.9740 - mae: 16.6436 - val_loss: 368.8644 - val_mse: 368.8644 - val_mae: 16.0394\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 387.4950 - mse: 387.4950 - mae: 16.4798 - val_loss: 366.8362 - val_mse: 366.8362 - val_mae: 15.8625\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 378.7151 - mse: 378.7151 - mae: 16.1062 - val_loss: 362.7165 - val_mse: 362.7165 - val_mae: 15.7209\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 369.3478 - mse: 369.3478 - mae: 15.8534 - val_loss: 346.5446 - val_mse: 346.5446 - val_mae: 15.2860\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 335.8845 - mse: 335.8845 - mae: 15.0323 - val_loss: 319.7228 - val_mse: 319.7228 - val_mae: 14.5710\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 316.4634 - mse: 316.4634 - mae: 14.4242 - val_loss: 305.5207 - val_mse: 305.5207 - val_mae: 14.1104\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 313.1273 - mse: 313.1273 - mae: 14.2266 - val_loss: 294.4036 - val_mse: 294.4036 - val_mae: 13.7369\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 309.9462 - mse: 309.9462 - mae: 14.0086 - val_loss: 292.2461 - val_mse: 292.2461 - val_mae: 13.5138\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 314.4787 - mse: 314.4787 - mae: 14.2375 - val_loss: 293.7758 - val_mse: 293.7758 - val_mae: 13.4243\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 317.6605 - mse: 317.6605 - mae: 14.3413 - val_loss: 316.2982 - val_mse: 316.2982 - val_mae: 14.6575\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 324.3157 - mse: 324.3157 - mae: 14.8185 - val_loss: 315.2469 - val_mse: 315.2469 - val_mae: 14.6161\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 317.7145 - mse: 317.7145 - mae: 14.4800 - val_loss: 333.0068 - val_mse: 333.0068 - val_mae: 15.5137\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 324.5584 - mse: 324.5584 - mae: 14.8755 - val_loss: 335.7715 - val_mse: 335.7715 - val_mae: 15.6591\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 316.3420 - mse: 316.3420 - mae: 14.5009 - val_loss: 319.5132 - val_mse: 319.5132 - val_mae: 14.8611\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 318.9722 - mse: 318.9722 - mae: 14.3759 - val_loss: 290.9671 - val_mse: 290.9671 - val_mae: 13.2699\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 310.3380 - mse: 310.3380 - mae: 14.1057 - val_loss: 286.9426 - val_mse: 286.9426 - val_mae: 13.2690\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 306.5466 - mse: 306.5466 - mae: 13.8161 - val_loss: 293.3274 - val_mse: 293.3274 - val_mae: 13.6831\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 317.4718 - mse: 317.4718 - mae: 14.4838 - val_loss: 287.5986 - val_mse: 287.5986 - val_mae: 13.3426\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 316.5735 - mse: 316.5735 - mae: 14.4329 - val_loss: 291.1776 - val_mse: 291.1776 - val_mae: 13.4312\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 309.3193 - mse: 309.3193 - mae: 14.1101 - val_loss: 288.0377 - val_mse: 288.0377 - val_mae: 13.0586\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 305.8354 - mse: 305.8354 - mae: 13.8067 - val_loss: 286.3789 - val_mse: 286.3789 - val_mae: 13.1312\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 304.8425 - mse: 304.8425 - mae: 13.8442 - val_loss: 303.7899 - val_mse: 303.7899 - val_mae: 14.0915\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 312.1317 - mse: 312.1317 - mae: 14.1580 - val_loss: 303.1456 - val_mse: 303.1456 - val_mae: 14.0421\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 305.7789 - mse: 305.7789 - mae: 13.8474 - val_loss: 294.4519 - val_mse: 294.4519 - val_mae: 13.5312\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 306.1341 - mse: 306.1341 - mae: 13.7861 - val_loss: 293.4683 - val_mse: 293.4683 - val_mae: 13.5246\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 306.3579 - mse: 306.3579 - mae: 13.8828 - val_loss: 295.4247 - val_mse: 295.4247 - val_mae: 13.6611\n",
            "Epoch 31: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 25ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 23ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 25ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: 1857.6857 - mse: 1857.6857 - mae: 31.4157 - val_loss: 1777.2075 - val_mse: 1777.2075 - val_mae: 30.3350\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1657.5756 - mse: 1657.5756 - mae: 30.1549 - val_loss: 1576.1233 - val_mse: 1576.1233 - val_mae: 29.0472\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1478.9447 - mse: 1478.9447 - mae: 28.9612 - val_loss: 1406.4287 - val_mse: 1406.4287 - val_mae: 27.8775\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1327.6486 - mse: 1327.6486 - mae: 27.8630 - val_loss: 1260.2537 - val_mse: 1260.2537 - val_mae: 26.7927\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1199.9857 - mse: 1199.9857 - mae: 26.8654 - val_loss: 1136.8584 - val_mse: 1136.8584 - val_mae: 25.8102\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1090.2030 - mse: 1090.2030 - mae: 25.9621 - val_loss: 1031.1244 - val_mse: 1031.1244 - val_mae: 24.9094\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 996.7958 - mse: 996.7958 - mae: 25.1240 - val_loss: 939.1176 - val_mse: 939.1176 - val_mae: 24.0682\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 916.8848 - mse: 916.8848 - mae: 24.3615 - val_loss: 860.8745 - val_mse: 860.8745 - val_mae: 23.3051\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 848.7457 - mse: 848.7457 - mae: 23.6711 - val_loss: 793.5659 - val_mse: 793.5659 - val_mae: 22.6056\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 791.0760 - mse: 791.0760 - mae: 23.0517 - val_loss: 737.0151 - val_mse: 737.0151 - val_mae: 21.9848\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 742.0755 - mse: 742.0755 - mae: 22.4842 - val_loss: 687.0078 - val_mse: 687.0078 - val_mae: 21.4083\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 699.9651 - mse: 699.9651 - mae: 21.9769 - val_loss: 645.4847 - val_mse: 645.4847 - val_mae: 20.9011\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 663.9073 - mse: 663.9073 - mae: 21.5186 - val_loss: 608.8660 - val_mse: 608.8660 - val_mae: 20.4293\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 633.2345 - mse: 633.2345 - mae: 21.1016 - val_loss: 576.9589 - val_mse: 576.9589 - val_mae: 19.9983\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 606.4114 - mse: 606.4114 - mae: 20.7280 - val_loss: 549.8802 - val_mse: 549.8802 - val_mae: 19.6114\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 583.4005 - mse: 583.4005 - mae: 20.3866 - val_loss: 525.8500 - val_mse: 525.8500 - val_mae: 19.2531\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 563.6097 - mse: 563.6097 - mae: 20.0799 - val_loss: 504.4486 - val_mse: 504.4486 - val_mae: 18.9232\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 546.6648 - mse: 546.6648 - mae: 19.8040 - val_loss: 486.3763 - val_mse: 486.3763 - val_mae: 18.6364\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 532.2242 - mse: 532.2242 - mae: 19.5606 - val_loss: 470.9516 - val_mse: 470.9516 - val_mae: 18.3822\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 519.6989 - mse: 519.6989 - mae: 19.3444 - val_loss: 457.1311 - val_mse: 457.1311 - val_mae: 18.1462\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 508.7546 - mse: 508.7546 - mae: 19.1482 - val_loss: 445.1171 - val_mse: 445.1171 - val_mae: 17.9346\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 499.2405 - mse: 499.2405 - mae: 18.9728 - val_loss: 434.4486 - val_mse: 434.4486 - val_mae: 17.7420\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 490.9339 - mse: 490.9339 - mae: 18.8144 - val_loss: 424.8504 - val_mse: 424.8504 - val_mae: 17.5681\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 483.6878 - mse: 483.6878 - mae: 18.6739 - val_loss: 416.5374 - val_mse: 416.5374 - val_mae: 17.4160\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 477.3521 - mse: 477.3521 - mae: 18.5496 - val_loss: 409.2521 - val_mse: 409.2521 - val_mae: 17.2799\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 471.8186 - mse: 471.8186 - mae: 18.4375 - val_loss: 402.6423 - val_mse: 402.6423 - val_mae: 17.1540\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 466.9159 - mse: 466.9159 - mae: 18.3363 - val_loss: 396.6858 - val_mse: 396.6858 - val_mae: 17.0390\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 462.6004 - mse: 462.6004 - mae: 18.2469 - val_loss: 391.4613 - val_mse: 391.4613 - val_mae: 16.9347\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 458.7569 - mse: 458.7569 - mae: 18.1660 - val_loss: 386.6746 - val_mse: 386.6746 - val_mae: 16.8386\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 455.2414 - mse: 455.2414 - mae: 18.0927 - val_loss: 382.3931 - val_mse: 382.3931 - val_mae: 16.7529\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 452.1569 - mse: 452.1569 - mae: 18.0259 - val_loss: 378.3723 - val_mse: 378.3723 - val_mae: 16.6722\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 449.3440 - mse: 449.3440 - mae: 17.9639 - val_loss: 374.7584 - val_mse: 374.7584 - val_mae: 16.5987\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 446.8825 - mse: 446.8825 - mae: 17.9091 - val_loss: 371.5724 - val_mse: 371.5724 - val_mae: 16.5355\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 444.6666 - mse: 444.6666 - mae: 17.8595 - val_loss: 368.7060 - val_mse: 368.7060 - val_mae: 16.4773\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 442.7006 - mse: 442.7006 - mae: 17.8143 - val_loss: 365.9839 - val_mse: 365.9839 - val_mae: 16.4216\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 440.7963 - mse: 440.7963 - mae: 17.7703 - val_loss: 363.5760 - val_mse: 363.5760 - val_mae: 16.3706\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 439.0395 - mse: 439.0395 - mae: 17.7325 - val_loss: 361.0414 - val_mse: 361.0414 - val_mae: 16.3200\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 437.4366 - mse: 437.4366 - mae: 17.6952 - val_loss: 358.7847 - val_mse: 358.7847 - val_mae: 16.2738\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 435.9799 - mse: 435.9799 - mae: 17.6617 - val_loss: 356.7643 - val_mse: 356.7643 - val_mae: 16.2317\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 434.7044 - mse: 434.7044 - mae: 17.6320 - val_loss: 354.8628 - val_mse: 354.8628 - val_mae: 16.1934\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 433.4071 - mse: 433.4071 - mae: 17.6013 - val_loss: 353.1805 - val_mse: 353.1805 - val_mae: 16.1590\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 432.2632 - mse: 432.2632 - mae: 17.5745 - val_loss: 351.5471 - val_mse: 351.5471 - val_mae: 16.1251\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 431.1725 - mse: 431.1725 - mae: 17.5487 - val_loss: 349.9624 - val_mse: 349.9624 - val_mae: 16.0925\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 430.1710 - mse: 430.1710 - mae: 17.5268 - val_loss: 348.3862 - val_mse: 348.3862 - val_mae: 16.0612\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 429.1909 - mse: 429.1909 - mae: 17.5025 - val_loss: 347.0371 - val_mse: 347.0371 - val_mae: 16.0329\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 428.2738 - mse: 428.2738 - mae: 17.4806 - val_loss: 345.7232 - val_mse: 345.7232 - val_mae: 16.0049\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 427.4351 - mse: 427.4351 - mae: 17.4598 - val_loss: 344.4229 - val_mse: 344.4229 - val_mae: 15.9777\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 426.5693 - mse: 426.5693 - mae: 17.4393 - val_loss: 343.1611 - val_mse: 343.1611 - val_mae: 15.9525\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 425.7759 - mse: 425.7759 - mae: 17.4200 - val_loss: 342.0460 - val_mse: 342.0460 - val_mae: 15.9293\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 425.0204 - mse: 425.0204 - mae: 17.4017 - val_loss: 340.9555 - val_mse: 340.9555 - val_mae: 15.9072\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 424.2936 - mse: 424.2936 - mae: 17.3839 - val_loss: 339.9020 - val_mse: 339.9020 - val_mae: 15.8853\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 423.5800 - mse: 423.5800 - mae: 17.3668 - val_loss: 338.8360 - val_mse: 338.8360 - val_mae: 15.8636\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 422.9005 - mse: 422.9005 - mae: 17.3502 - val_loss: 337.8443 - val_mse: 337.8443 - val_mae: 15.8424\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 422.2421 - mse: 422.2421 - mae: 17.3349 - val_loss: 336.8984 - val_mse: 336.8984 - val_mae: 15.8230\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 421.5666 - mse: 421.5666 - mae: 17.3181 - val_loss: 335.8419 - val_mse: 335.8419 - val_mae: 15.8003\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.9244 - mse: 420.9244 - mae: 17.3018 - val_loss: 334.9283 - val_mse: 334.9283 - val_mae: 15.7812\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.2953 - mse: 420.2953 - mae: 17.2861 - val_loss: 334.0226 - val_mse: 334.0226 - val_mae: 15.7612\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 419.6781 - mse: 419.6781 - mae: 17.2712 - val_loss: 333.0347 - val_mse: 333.0347 - val_mae: 15.7407\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 419.0739 - mse: 419.0739 - mae: 17.2558 - val_loss: 332.3059 - val_mse: 332.3059 - val_mae: 15.7237\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 418.4744 - mse: 418.4744 - mae: 17.2402 - val_loss: 331.4719 - val_mse: 331.4719 - val_mae: 15.7040\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 417.8978 - mse: 417.8978 - mae: 17.2252 - val_loss: 330.6421 - val_mse: 330.6421 - val_mae: 15.6851\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 417.3524 - mse: 417.3524 - mae: 17.2109 - val_loss: 329.9180 - val_mse: 329.9180 - val_mae: 15.6677\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 416.7857 - mse: 416.7857 - mae: 17.1958 - val_loss: 329.1365 - val_mse: 329.1365 - val_mae: 15.6486\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 416.2327 - mse: 416.2327 - mae: 17.1819 - val_loss: 328.3742 - val_mse: 328.3742 - val_mae: 15.6308\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 415.7041 - mse: 415.7041 - mae: 17.1678 - val_loss: 327.6798 - val_mse: 327.6798 - val_mae: 15.6141\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 415.1609 - mse: 415.1609 - mae: 17.1546 - val_loss: 326.8922 - val_mse: 326.8922 - val_mae: 15.5959\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 414.6263 - mse: 414.6263 - mae: 17.1402 - val_loss: 326.2029 - val_mse: 326.2029 - val_mae: 15.5787\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 414.1267 - mse: 414.1267 - mae: 17.1274 - val_loss: 325.5140 - val_mse: 325.5140 - val_mae: 15.5623\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 413.6317 - mse: 413.6317 - mae: 17.1134 - val_loss: 324.9736 - val_mse: 324.9736 - val_mae: 15.5479\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 413.1017 - mse: 413.1017 - mae: 17.0996 - val_loss: 324.2912 - val_mse: 324.2912 - val_mae: 15.5312\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 412.6132 - mse: 412.6132 - mae: 17.0865 - val_loss: 323.6701 - val_mse: 323.6701 - val_mae: 15.5148\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 412.1237 - mse: 412.1237 - mae: 17.0729 - val_loss: 322.9746 - val_mse: 322.9746 - val_mae: 15.4970\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 411.6480 - mse: 411.6480 - mae: 17.0595 - val_loss: 322.4648 - val_mse: 322.4648 - val_mae: 15.4830\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 411.1842 - mse: 411.1842 - mae: 17.0465 - val_loss: 321.8207 - val_mse: 321.8207 - val_mae: 15.4669\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 410.7114 - mse: 410.7114 - mae: 17.0339 - val_loss: 321.2828 - val_mse: 321.2828 - val_mae: 15.4531\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 410.2747 - mse: 410.2747 - mae: 17.0215 - val_loss: 320.6978 - val_mse: 320.6978 - val_mae: 15.4379\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 409.8203 - mse: 409.8203 - mae: 17.0092 - val_loss: 320.0940 - val_mse: 320.0940 - val_mae: 15.4229\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 409.3806 - mse: 409.3806 - mae: 16.9963 - val_loss: 319.6104 - val_mse: 319.6104 - val_mae: 15.4095\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 408.9392 - mse: 408.9392 - mae: 16.9839 - val_loss: 319.0560 - val_mse: 319.0560 - val_mae: 15.3951\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 408.5206 - mse: 408.5206 - mae: 16.9720 - val_loss: 318.5354 - val_mse: 318.5354 - val_mae: 15.3810\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 408.1038 - mse: 408.1038 - mae: 16.9607 - val_loss: 318.0068 - val_mse: 318.0068 - val_mae: 15.3681\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 407.6494 - mse: 407.6494 - mae: 16.9485 - val_loss: 317.3695 - val_mse: 317.3695 - val_mae: 15.3526\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 407.2317 - mse: 407.2317 - mae: 16.9368 - val_loss: 316.8911 - val_mse: 316.8911 - val_mae: 15.3399\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.8249 - mse: 406.8249 - mae: 16.9252 - val_loss: 316.3793 - val_mse: 316.3793 - val_mae: 15.3267\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.4485 - mse: 406.4485 - mae: 16.9140 - val_loss: 315.8888 - val_mse: 315.8888 - val_mae: 15.3133\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 406.0414 - mse: 406.0414 - mae: 16.9023 - val_loss: 315.4299 - val_mse: 315.4299 - val_mae: 15.3010\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 405.6599 - mse: 405.6599 - mae: 16.8921 - val_loss: 314.9912 - val_mse: 314.9912 - val_mae: 15.2891\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 405.2772 - mse: 405.2772 - mae: 16.8809 - val_loss: 314.4805 - val_mse: 314.4805 - val_mae: 15.2754\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.9121 - mse: 404.9121 - mae: 16.8705 - val_loss: 314.0033 - val_mse: 314.0033 - val_mae: 15.2628\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.5150 - mse: 404.5150 - mae: 16.8595 - val_loss: 313.5728 - val_mse: 313.5728 - val_mae: 15.2509\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.1499 - mse: 404.1499 - mae: 16.8484 - val_loss: 313.1037 - val_mse: 313.1037 - val_mae: 15.2375\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 403.7878 - mse: 403.7878 - mae: 16.8384 - val_loss: 312.6307 - val_mse: 312.6307 - val_mae: 15.2254\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 403.4169 - mse: 403.4169 - mae: 16.8270 - val_loss: 312.1607 - val_mse: 312.1607 - val_mae: 15.2122\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 403.0638 - mse: 403.0638 - mae: 16.8165 - val_loss: 311.7201 - val_mse: 311.7201 - val_mae: 15.1998\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 402.6987 - mse: 402.6987 - mae: 16.8057 - val_loss: 311.3127 - val_mse: 311.3127 - val_mae: 15.1884\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 402.3472 - mse: 402.3472 - mae: 16.7956 - val_loss: 310.8541 - val_mse: 310.8541 - val_mae: 15.1761\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 402.0002 - mse: 402.0002 - mae: 16.7854 - val_loss: 310.4221 - val_mse: 310.4221 - val_mae: 15.1640\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 401.6778 - mse: 401.6778 - mae: 16.7756 - val_loss: 310.0755 - val_mse: 310.0755 - val_mae: 15.1537\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 401.3217 - mse: 401.3217 - mae: 16.7659 - val_loss: 309.6180 - val_mse: 309.6180 - val_mae: 15.1413\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 400.9821 - mse: 400.9821 - mae: 16.7553 - val_loss: 309.2175 - val_mse: 309.2175 - val_mae: 15.1295\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 1868.4529 - mse: 1868.4529 - mae: 32.3610 - val_loss: 1779.2217 - val_mse: 1779.2217 - val_mae: 31.0048\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1672.3833 - mse: 1672.3833 - mae: 31.0784 - val_loss: 1580.2275 - val_mse: 1580.2275 - val_mae: 29.6905\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1498.2101 - mse: 1498.2101 - mae: 29.8423 - val_loss: 1407.7876 - val_mse: 1407.7876 - val_mae: 28.4634\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1349.6174 - mse: 1349.6174 - mae: 28.7089 - val_loss: 1261.3274 - val_mse: 1261.3274 - val_mae: 27.3404\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1222.5986 - mse: 1222.5986 - mae: 27.6714 - val_loss: 1136.2638 - val_mse: 1136.2638 - val_mae: 26.3121\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1113.5631 - mse: 1113.5631 - mae: 26.7198 - val_loss: 1027.6743 - val_mse: 1027.6743 - val_mae: 25.3589\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1019.6685 - mse: 1019.6685 - mae: 25.8300 - val_loss: 933.2954 - val_mse: 933.2954 - val_mae: 24.4719\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 938.9827 - mse: 938.9827 - mae: 25.0072 - val_loss: 851.8591 - val_mse: 851.8591 - val_mae: 23.6527\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 869.7995 - mse: 869.7995 - mae: 24.2696 - val_loss: 783.0013 - val_mse: 783.0013 - val_mae: 22.9078\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 811.0975 - mse: 811.0975 - mae: 23.5928 - val_loss: 723.4102 - val_mse: 723.4102 - val_mae: 22.2203\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 760.5543 - mse: 760.5543 - mae: 22.9777 - val_loss: 671.7100 - val_mse: 671.7100 - val_mae: 21.5871\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 717.1328 - mse: 717.1328 - mae: 22.4188 - val_loss: 627.3850 - val_mse: 627.3850 - val_mae: 21.0132\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 679.7323 - mse: 679.7323 - mae: 21.9093 - val_loss: 588.7941 - val_mse: 588.7941 - val_mae: 20.4851\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 647.6437 - mse: 647.6437 - mae: 21.4425 - val_loss: 555.0481 - val_mse: 555.0481 - val_mae: 19.9976\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 619.7521 - mse: 619.7521 - mae: 21.0250 - val_loss: 526.1083 - val_mse: 526.1083 - val_mae: 19.5538\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 595.8094 - mse: 595.8094 - mae: 20.6511 - val_loss: 501.2180 - val_mse: 501.2180 - val_mae: 19.1480\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 575.2375 - mse: 575.2375 - mae: 20.3082 - val_loss: 479.1127 - val_mse: 479.1127 - val_mae: 18.7699\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 557.2955 - mse: 557.2955 - mae: 19.9984 - val_loss: 459.9254 - val_mse: 459.9254 - val_mae: 18.4261\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 541.7914 - mse: 541.7914 - mae: 19.7185 - val_loss: 443.1465 - val_mse: 443.1465 - val_mae: 18.1123\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 528.2374 - mse: 528.2374 - mae: 19.4661 - val_loss: 428.3773 - val_mse: 428.3773 - val_mae: 17.8243\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 516.3876 - mse: 516.3876 - mae: 19.2367 - val_loss: 415.3200 - val_mse: 415.3200 - val_mae: 17.5609\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 506.0195 - mse: 506.0195 - mae: 19.0288 - val_loss: 403.7399 - val_mse: 403.7399 - val_mae: 17.3187\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 496.9165 - mse: 496.9165 - mae: 18.8404 - val_loss: 393.5877 - val_mse: 393.5877 - val_mae: 17.0983\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 489.0568 - mse: 489.0568 - mae: 18.6706 - val_loss: 384.6484 - val_mse: 384.6484 - val_mae: 16.8979\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 482.1670 - mse: 482.1670 - mae: 18.5211 - val_loss: 376.8375 - val_mse: 376.8375 - val_mae: 16.7182\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 476.0132 - mse: 476.0132 - mae: 18.3811 - val_loss: 369.6389 - val_mse: 369.6389 - val_mae: 16.5503\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 470.4993 - mse: 470.4993 - mae: 18.2559 - val_loss: 363.3494 - val_mse: 363.3494 - val_mae: 16.3981\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 465.6354 - mse: 465.6354 - mae: 18.1414 - val_loss: 357.4070 - val_mse: 357.4070 - val_mae: 16.2535\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 461.1848 - mse: 461.1848 - mae: 18.0358 - val_loss: 352.3289 - val_mse: 352.3289 - val_mae: 16.1260\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 457.2891 - mse: 457.2891 - mae: 17.9412 - val_loss: 347.6939 - val_mse: 347.6939 - val_mae: 16.0098\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 453.8192 - mse: 453.8192 - mae: 17.8548 - val_loss: 343.5789 - val_mse: 343.5789 - val_mae: 15.9041\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 450.6931 - mse: 450.6931 - mae: 17.7764 - val_loss: 339.7583 - val_mse: 339.7583 - val_mae: 15.8051\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 447.8744 - mse: 447.8744 - mae: 17.7040 - val_loss: 336.4437 - val_mse: 336.4437 - val_mae: 15.7187\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 445.2789 - mse: 445.2789 - mae: 17.6386 - val_loss: 333.2458 - val_mse: 333.2458 - val_mae: 15.6363\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 442.9168 - mse: 442.9168 - mae: 17.5780 - val_loss: 330.2312 - val_mse: 330.2312 - val_mae: 15.5601\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 440.7248 - mse: 440.7248 - mae: 17.5205 - val_loss: 327.7193 - val_mse: 327.7193 - val_mae: 15.4932\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 438.7231 - mse: 438.7231 - mae: 17.4665 - val_loss: 325.1988 - val_mse: 325.1988 - val_mae: 15.4276\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 436.8459 - mse: 436.8459 - mae: 17.4167 - val_loss: 322.8313 - val_mse: 322.8313 - val_mae: 15.3670\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 435.0993 - mse: 435.0993 - mae: 17.3701 - val_loss: 320.6487 - val_mse: 320.6487 - val_mae: 15.3112\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 433.4668 - mse: 433.4668 - mae: 17.3250 - val_loss: 318.4997 - val_mse: 318.4997 - val_mae: 15.2569\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 431.8751 - mse: 431.8751 - mae: 17.2829 - val_loss: 316.5921 - val_mse: 316.5921 - val_mae: 15.2085\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 430.3589 - mse: 430.3589 - mae: 17.2420 - val_loss: 314.7621 - val_mse: 314.7621 - val_mae: 15.1602\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 428.9628 - mse: 428.9628 - mae: 17.2022 - val_loss: 313.0102 - val_mse: 313.0102 - val_mae: 15.1136\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 427.6493 - mse: 427.6493 - mae: 17.1664 - val_loss: 311.3831 - val_mse: 311.3831 - val_mae: 15.0707\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 426.3957 - mse: 426.3957 - mae: 17.1325 - val_loss: 309.8963 - val_mse: 309.8963 - val_mae: 15.0305\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 425.1201 - mse: 425.1201 - mae: 17.0954 - val_loss: 308.3497 - val_mse: 308.3497 - val_mae: 14.9883\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 423.9170 - mse: 423.9170 - mae: 17.0613 - val_loss: 306.9007 - val_mse: 306.9007 - val_mae: 14.9490\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 422.7819 - mse: 422.7819 - mae: 17.0304 - val_loss: 305.4854 - val_mse: 305.4854 - val_mae: 14.9121\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 421.7123 - mse: 421.7123 - mae: 16.9996 - val_loss: 304.2333 - val_mse: 304.2333 - val_mae: 14.8781\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.6587 - mse: 420.6587 - mae: 16.9718 - val_loss: 303.0826 - val_mse: 303.0826 - val_mae: 14.8471\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 419.6223 - mse: 419.6223 - mae: 16.9434 - val_loss: 301.8085 - val_mse: 301.8085 - val_mae: 14.8125\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 418.5933 - mse: 418.5933 - mae: 16.9144 - val_loss: 300.5757 - val_mse: 300.5757 - val_mae: 14.7788\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 417.5503 - mse: 417.5503 - mae: 16.8858 - val_loss: 299.4066 - val_mse: 299.4066 - val_mae: 14.7463\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 416.5671 - mse: 416.5671 - mae: 16.8582 - val_loss: 298.3408 - val_mse: 298.3408 - val_mae: 14.7157\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 415.5941 - mse: 415.5941 - mae: 16.8296 - val_loss: 297.2262 - val_mse: 297.2262 - val_mae: 14.6838\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 414.6840 - mse: 414.6840 - mae: 16.8035 - val_loss: 296.2086 - val_mse: 296.2086 - val_mae: 14.6551\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 413.7918 - mse: 413.7918 - mae: 16.7769 - val_loss: 295.2532 - val_mse: 295.2532 - val_mae: 14.6269\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 412.9260 - mse: 412.9260 - mae: 16.7520 - val_loss: 294.2995 - val_mse: 294.2995 - val_mae: 14.5992\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 412.0987 - mse: 412.0987 - mae: 16.7271 - val_loss: 293.3910 - val_mse: 293.3910 - val_mae: 14.5722\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 411.3000 - mse: 411.3000 - mae: 16.7037 - val_loss: 292.5854 - val_mse: 292.5854 - val_mae: 14.5479\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 410.5490 - mse: 410.5490 - mae: 16.6821 - val_loss: 291.7604 - val_mse: 291.7604 - val_mae: 14.5234\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 409.7601 - mse: 409.7601 - mae: 16.6594 - val_loss: 290.9443 - val_mse: 290.9443 - val_mae: 14.4992\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 409.0168 - mse: 409.0168 - mae: 16.6381 - val_loss: 290.1125 - val_mse: 290.1125 - val_mae: 14.4749\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 408.2941 - mse: 408.2941 - mae: 16.6173 - val_loss: 289.3706 - val_mse: 289.3706 - val_mae: 14.4527\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 407.6129 - mse: 407.6129 - mae: 16.5964 - val_loss: 288.5652 - val_mse: 288.5652 - val_mae: 14.4297\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.8894 - mse: 406.8894 - mae: 16.5770 - val_loss: 287.7796 - val_mse: 287.7796 - val_mae: 14.4075\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.2119 - mse: 406.2119 - mae: 16.5586 - val_loss: 287.1296 - val_mse: 287.1296 - val_mae: 14.3880\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 405.5546 - mse: 405.5546 - mae: 16.5407 - val_loss: 286.4221 - val_mse: 286.4221 - val_mae: 14.3673\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.9257 - mse: 404.9257 - mae: 16.5234 - val_loss: 285.8042 - val_mse: 285.8042 - val_mae: 14.3492\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 404.2926 - mse: 404.2926 - mae: 16.5056 - val_loss: 285.1414 - val_mse: 285.1414 - val_mae: 14.3297\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 403.6663 - mse: 403.6663 - mae: 16.4901 - val_loss: 284.5147 - val_mse: 284.5147 - val_mae: 14.3113\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 403.0606 - mse: 403.0606 - mae: 16.4725 - val_loss: 283.8272 - val_mse: 283.8272 - val_mae: 14.2912\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 402.4469 - mse: 402.4469 - mae: 16.4563 - val_loss: 283.2438 - val_mse: 283.2438 - val_mae: 14.2735\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 401.8424 - mse: 401.8424 - mae: 16.4401 - val_loss: 282.6414 - val_mse: 282.6414 - val_mae: 14.2557\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 401.2552 - mse: 401.2552 - mae: 16.4258 - val_loss: 282.0800 - val_mse: 282.0800 - val_mae: 14.2397\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 400.7010 - mse: 400.7010 - mae: 16.4106 - val_loss: 281.4414 - val_mse: 281.4414 - val_mae: 14.2210\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 400.0960 - mse: 400.0960 - mae: 16.3961 - val_loss: 280.8239 - val_mse: 280.8239 - val_mae: 14.2039\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 399.5337 - mse: 399.5337 - mae: 16.3830 - val_loss: 280.3180 - val_mse: 280.3180 - val_mae: 14.1896\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 398.9366 - mse: 398.9366 - mae: 16.3668 - val_loss: 279.6574 - val_mse: 279.6574 - val_mae: 14.1707\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 398.3957 - mse: 398.3957 - mae: 16.3523 - val_loss: 279.1153 - val_mse: 279.1153 - val_mae: 14.1546\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 397.8384 - mse: 397.8384 - mae: 16.3386 - val_loss: 278.5663 - val_mse: 278.5663 - val_mae: 14.1390\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 397.2787 - mse: 397.2787 - mae: 16.3229 - val_loss: 277.9352 - val_mse: 277.9352 - val_mae: 14.1208\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 396.7263 - mse: 396.7263 - mae: 16.3091 - val_loss: 277.3739 - val_mse: 277.3739 - val_mae: 14.1051\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 396.1915 - mse: 396.1915 - mae: 16.2955 - val_loss: 276.8345 - val_mse: 276.8345 - val_mae: 14.0900\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 395.6650 - mse: 395.6650 - mae: 16.2813 - val_loss: 276.2597 - val_mse: 276.2597 - val_mae: 14.0732\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 395.1413 - mse: 395.1413 - mae: 16.2679 - val_loss: 275.7158 - val_mse: 275.7158 - val_mae: 14.0580\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 394.6426 - mse: 394.6426 - mae: 16.2551 - val_loss: 275.1634 - val_mse: 275.1634 - val_mae: 14.0420\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 394.1436 - mse: 394.1436 - mae: 16.2421 - val_loss: 274.6788 - val_mse: 274.6788 - val_mae: 14.0281\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 393.6729 - mse: 393.6729 - mae: 16.2309 - val_loss: 274.1691 - val_mse: 274.1691 - val_mae: 14.0137\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 393.1722 - mse: 393.1722 - mae: 16.2177 - val_loss: 273.6800 - val_mse: 273.6800 - val_mae: 13.9993\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 392.7021 - mse: 392.7021 - mae: 16.2040 - val_loss: 273.1212 - val_mse: 273.1212 - val_mae: 13.9830\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 392.2144 - mse: 392.2144 - mae: 16.1908 - val_loss: 272.6134 - val_mse: 272.6134 - val_mae: 13.9683\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 391.7578 - mse: 391.7578 - mae: 16.1776 - val_loss: 272.0945 - val_mse: 272.0945 - val_mae: 13.9530\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 391.2881 - mse: 391.2881 - mae: 16.1645 - val_loss: 271.6378 - val_mse: 271.6378 - val_mae: 13.9391\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 390.8118 - mse: 390.8118 - mae: 16.1535 - val_loss: 271.1331 - val_mse: 271.1331 - val_mae: 13.9244\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 390.3258 - mse: 390.3258 - mae: 16.1396 - val_loss: 270.5652 - val_mse: 270.5652 - val_mae: 13.9077\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 389.8362 - mse: 389.8362 - mae: 16.1257 - val_loss: 270.0714 - val_mse: 270.0714 - val_mae: 13.8922\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 389.3585 - mse: 389.3585 - mae: 16.1138 - val_loss: 269.5731 - val_mse: 269.5731 - val_mae: 13.8772\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 388.8510 - mse: 388.8510 - mae: 16.1000 - val_loss: 269.0158 - val_mse: 269.0158 - val_mae: 13.8603\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 388.3363 - mse: 388.3363 - mae: 16.0850 - val_loss: 268.4576 - val_mse: 268.4576 - val_mae: 13.8427\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 2156.0449 - mse: 2156.0449 - mae: 34.5310 - val_loss: 2094.2705 - val_mse: 2094.2705 - val_mae: 33.9877\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1918.6101 - mse: 1918.6101 - mae: 33.0158 - val_loss: 1854.5861 - val_mse: 1854.5861 - val_mae: 32.4351\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1707.7184 - mse: 1707.7184 - mae: 31.5511 - val_loss: 1644.1073 - val_mse: 1644.1073 - val_mae: 30.9661\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1525.4729 - mse: 1525.4729 - mae: 30.2005 - val_loss: 1466.0265 - val_mse: 1466.0265 - val_mae: 29.6274\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1369.6602 - mse: 1369.6602 - mae: 28.9629 - val_loss: 1312.2823 - val_mse: 1312.2823 - val_mae: 28.3887\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1235.6172 - mse: 1235.6172 - mae: 27.8339 - val_loss: 1181.2778 - val_mse: 1181.2778 - val_mae: 27.2553\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1120.9979 - mse: 1120.9979 - mae: 26.7865 - val_loss: 1068.4943 - val_mse: 1068.4943 - val_mae: 26.2104\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1022.3633 - mse: 1022.3633 - mae: 25.8309 - val_loss: 972.1060 - val_mse: 972.1060 - val_mae: 25.2595\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 937.6365 - mse: 937.6365 - mae: 24.9492 - val_loss: 888.2367 - val_mse: 888.2367 - val_mae: 24.3782\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 864.1030 - mse: 864.1030 - mae: 24.1336 - val_loss: 815.7237 - val_mse: 815.7237 - val_mae: 23.5685\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 801.1384 - mse: 801.1384 - mae: 23.3938 - val_loss: 754.0170 - val_mse: 754.0170 - val_mae: 22.8347\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 746.9956 - mse: 746.9956 - mae: 22.7185 - val_loss: 701.0679 - val_mse: 701.0679 - val_mae: 22.1653\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 700.2183 - mse: 700.2183 - mae: 22.0913 - val_loss: 654.3595 - val_mse: 654.3595 - val_mae: 21.5387\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 659.5406 - mse: 659.5406 - mae: 21.5273 - val_loss: 614.9064 - val_mse: 614.9064 - val_mae: 20.9804\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 624.7964 - mse: 624.7964 - mae: 21.0211 - val_loss: 580.9218 - val_mse: 580.9218 - val_mae: 20.4791\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 594.8276 - mse: 594.8276 - mae: 20.5626 - val_loss: 551.7548 - val_mse: 551.7548 - val_mae: 20.0276\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 568.7474 - mse: 568.7474 - mae: 20.1393 - val_loss: 526.2772 - val_mse: 526.2772 - val_mae: 19.6155\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 545.9787 - mse: 545.9787 - mae: 19.7473 - val_loss: 503.7680 - val_mse: 503.7680 - val_mae: 19.2341\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 526.1378 - mse: 526.1378 - mae: 19.3960 - val_loss: 484.4302 - val_mse: 484.4302 - val_mae: 18.8946\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 508.8850 - mse: 508.8850 - mae: 19.0800 - val_loss: 467.6291 - val_mse: 467.6291 - val_mae: 18.5894\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 493.9443 - mse: 493.9443 - mae: 18.7970 - val_loss: 453.3359 - val_mse: 453.3359 - val_mae: 18.3151\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 480.9946 - mse: 480.9946 - mae: 18.5381 - val_loss: 440.6486 - val_mse: 440.6486 - val_mae: 18.0613\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 469.6107 - mse: 469.6107 - mae: 18.3070 - val_loss: 429.8430 - val_mse: 429.8430 - val_mae: 17.8340\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 459.7153 - mse: 459.7153 - mae: 18.0978 - val_loss: 420.4002 - val_mse: 420.4002 - val_mae: 17.6284\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 451.0582 - mse: 451.0582 - mae: 17.9083 - val_loss: 412.2418 - val_mse: 412.2418 - val_mae: 17.4453\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 443.4158 - mse: 443.4158 - mae: 17.7375 - val_loss: 405.0595 - val_mse: 405.0595 - val_mae: 17.2792\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 436.6153 - mse: 436.6153 - mae: 17.5826 - val_loss: 398.6516 - val_mse: 398.6516 - val_mae: 17.1261\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 430.5589 - mse: 430.5589 - mae: 17.4414 - val_loss: 393.0241 - val_mse: 393.0241 - val_mae: 16.9848\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 425.2234 - mse: 425.2234 - mae: 17.3134 - val_loss: 388.2441 - val_mse: 388.2441 - val_mae: 16.8613\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.5081 - mse: 420.5081 - mae: 17.1986 - val_loss: 383.8459 - val_mse: 383.8459 - val_mae: 16.7452\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 416.2234 - mse: 416.2234 - mae: 17.0919 - val_loss: 379.9496 - val_mse: 379.9496 - val_mae: 16.6391\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 412.3643 - mse: 412.3643 - mae: 16.9944 - val_loss: 376.5171 - val_mse: 376.5171 - val_mae: 16.5434\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 408.8755 - mse: 408.8755 - mae: 16.9061 - val_loss: 373.4953 - val_mse: 373.4953 - val_mae: 16.4568\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 405.7207 - mse: 405.7207 - mae: 16.8242 - val_loss: 370.6830 - val_mse: 370.6830 - val_mae: 16.3742\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 402.8639 - mse: 402.8639 - mae: 16.7496 - val_loss: 368.2145 - val_mse: 368.2145 - val_mae: 16.2992\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 400.2244 - mse: 400.2244 - mae: 16.6803 - val_loss: 366.0117 - val_mse: 366.0117 - val_mae: 16.2314\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 397.7698 - mse: 397.7698 - mae: 16.6158 - val_loss: 363.9272 - val_mse: 363.9272 - val_mae: 16.1669\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 395.4713 - mse: 395.4713 - mae: 16.5562 - val_loss: 362.0118 - val_mse: 362.0118 - val_mae: 16.1060\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 393.4037 - mse: 393.4037 - mae: 16.5009 - val_loss: 360.2614 - val_mse: 360.2614 - val_mae: 16.0496\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 391.4624 - mse: 391.4624 - mae: 16.4495 - val_loss: 358.7388 - val_mse: 358.7388 - val_mae: 16.0000\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 389.6884 - mse: 389.6884 - mae: 16.4028 - val_loss: 357.3723 - val_mse: 357.3723 - val_mae: 15.9549\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 388.0433 - mse: 388.0433 - mae: 16.3587 - val_loss: 356.0603 - val_mse: 356.0603 - val_mae: 15.9108\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 386.5090 - mse: 386.5090 - mae: 16.3200 - val_loss: 354.9048 - val_mse: 354.9048 - val_mae: 15.8732\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 385.0446 - mse: 385.0446 - mae: 16.2822 - val_loss: 353.7741 - val_mse: 353.7741 - val_mae: 15.8347\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 383.6268 - mse: 383.6268 - mae: 16.2443 - val_loss: 352.7157 - val_mse: 352.7157 - val_mae: 15.7980\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 382.2955 - mse: 382.2955 - mae: 16.2102 - val_loss: 351.7379 - val_mse: 351.7379 - val_mae: 15.7655\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 381.0187 - mse: 381.0187 - mae: 16.1791 - val_loss: 350.7833 - val_mse: 350.7833 - val_mae: 15.7336\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 379.7561 - mse: 379.7561 - mae: 16.1460 - val_loss: 349.8323 - val_mse: 349.8323 - val_mae: 15.7002\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 378.5654 - mse: 378.5654 - mae: 16.1168 - val_loss: 348.9594 - val_mse: 348.9594 - val_mae: 15.6698\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 377.4143 - mse: 377.4143 - mae: 16.0879 - val_loss: 348.1465 - val_mse: 348.1465 - val_mae: 15.6416\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.2942 - mse: 376.2942 - mae: 16.0613 - val_loss: 347.3729 - val_mse: 347.3729 - val_mae: 15.6141\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 375.2646 - mse: 375.2646 - mae: 16.0357 - val_loss: 346.5947 - val_mse: 346.5947 - val_mae: 15.5863\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.2179 - mse: 374.2179 - mae: 16.0106 - val_loss: 345.8831 - val_mse: 345.8831 - val_mae: 15.5611\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 373.2247 - mse: 373.2247 - mae: 15.9874 - val_loss: 345.2281 - val_mse: 345.2281 - val_mae: 15.5386\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 372.2790 - mse: 372.2790 - mae: 15.9647 - val_loss: 344.6084 - val_mse: 344.6084 - val_mae: 15.5170\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 371.3877 - mse: 371.3877 - mae: 15.9433 - val_loss: 344.0070 - val_mse: 344.0070 - val_mae: 15.4963\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 370.5228 - mse: 370.5228 - mae: 15.9217 - val_loss: 343.4238 - val_mse: 343.4238 - val_mae: 15.4755\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 369.6914 - mse: 369.6914 - mae: 15.9013 - val_loss: 342.8883 - val_mse: 342.8883 - val_mae: 15.4571\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 368.8991 - mse: 368.8991 - mae: 15.8809 - val_loss: 342.3897 - val_mse: 342.3897 - val_mae: 15.4398\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 368.0866 - mse: 368.0866 - mae: 15.8614 - val_loss: 341.8958 - val_mse: 341.8958 - val_mae: 15.4234\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 367.3339 - mse: 367.3339 - mae: 15.8433 - val_loss: 341.4187 - val_mse: 341.4187 - val_mae: 15.4065\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 366.5579 - mse: 366.5579 - mae: 15.8239 - val_loss: 341.0057 - val_mse: 341.0057 - val_mae: 15.3922\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 365.8106 - mse: 365.8106 - mae: 15.8054 - val_loss: 340.5580 - val_mse: 340.5580 - val_mae: 15.3768\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 365.1028 - mse: 365.1028 - mae: 15.7884 - val_loss: 340.1035 - val_mse: 340.1035 - val_mae: 15.3608\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 364.3964 - mse: 364.3964 - mae: 15.7712 - val_loss: 339.6963 - val_mse: 339.6963 - val_mae: 15.3475\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 363.6913 - mse: 363.6913 - mae: 15.7535 - val_loss: 339.2880 - val_mse: 339.2880 - val_mae: 15.3337\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 363.0349 - mse: 363.0349 - mae: 15.7370 - val_loss: 338.9045 - val_mse: 338.9045 - val_mae: 15.3213\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 362.3909 - mse: 362.3909 - mae: 15.7209 - val_loss: 338.5252 - val_mse: 338.5252 - val_mae: 15.3092\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 361.7610 - mse: 361.7610 - mae: 15.7046 - val_loss: 338.1454 - val_mse: 338.1454 - val_mae: 15.2958\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 361.1292 - mse: 361.1292 - mae: 15.6891 - val_loss: 337.7892 - val_mse: 337.7892 - val_mae: 15.2841\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 360.5170 - mse: 360.5170 - mae: 15.6736 - val_loss: 337.4356 - val_mse: 337.4356 - val_mae: 15.2718\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 359.9061 - mse: 359.9061 - mae: 15.6579 - val_loss: 337.1066 - val_mse: 337.1066 - val_mae: 15.2610\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 359.3372 - mse: 359.3372 - mae: 15.6432 - val_loss: 336.7589 - val_mse: 336.7589 - val_mae: 15.2488\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 358.7400 - mse: 358.7400 - mae: 15.6287 - val_loss: 336.4271 - val_mse: 336.4271 - val_mae: 15.2387\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 358.1787 - mse: 358.1787 - mae: 15.6146 - val_loss: 336.0986 - val_mse: 336.0986 - val_mae: 15.2274\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 357.6299 - mse: 357.6299 - mae: 15.6000 - val_loss: 335.7889 - val_mse: 335.7889 - val_mae: 15.2169\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 357.0685 - mse: 357.0685 - mae: 15.5857 - val_loss: 335.4785 - val_mse: 335.4785 - val_mae: 15.2070\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 356.5193 - mse: 356.5193 - mae: 15.5725 - val_loss: 335.1658 - val_mse: 335.1658 - val_mae: 15.1968\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 355.9909 - mse: 355.9909 - mae: 15.5592 - val_loss: 334.8641 - val_mse: 334.8641 - val_mae: 15.1872\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 355.4416 - mse: 355.4416 - mae: 15.5450 - val_loss: 334.5458 - val_mse: 334.5458 - val_mae: 15.1759\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 354.9309 - mse: 354.9309 - mae: 15.5319 - val_loss: 334.2419 - val_mse: 334.2419 - val_mae: 15.1653\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 354.4155 - mse: 354.4155 - mae: 15.5185 - val_loss: 333.9688 - val_mse: 333.9688 - val_mae: 15.1571\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 353.9018 - mse: 353.9018 - mae: 15.5048 - val_loss: 333.6938 - val_mse: 333.6938 - val_mae: 15.1488\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 353.4034 - mse: 353.4034 - mae: 15.4921 - val_loss: 333.4241 - val_mse: 333.4241 - val_mae: 15.1407\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 352.9421 - mse: 352.9421 - mae: 15.4802 - val_loss: 333.1471 - val_mse: 333.1471 - val_mae: 15.1320\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 352.4507 - mse: 352.4507 - mae: 15.4673 - val_loss: 332.8584 - val_mse: 332.8584 - val_mae: 15.1223\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 351.9784 - mse: 351.9784 - mae: 15.4540 - val_loss: 332.5582 - val_mse: 332.5582 - val_mae: 15.1117\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 351.4936 - mse: 351.4936 - mae: 15.4407 - val_loss: 332.2959 - val_mse: 332.2959 - val_mae: 15.1030\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 351.0337 - mse: 351.0337 - mae: 15.4286 - val_loss: 332.0228 - val_mse: 332.0228 - val_mae: 15.0941\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 350.5667 - mse: 350.5667 - mae: 15.4163 - val_loss: 331.7424 - val_mse: 331.7424 - val_mae: 15.0851\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 350.0962 - mse: 350.0962 - mae: 15.4040 - val_loss: 331.4803 - val_mse: 331.4803 - val_mae: 15.0772\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 349.6402 - mse: 349.6402 - mae: 15.3922 - val_loss: 331.2289 - val_mse: 331.2289 - val_mae: 15.0694\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 349.2038 - mse: 349.2038 - mae: 15.3808 - val_loss: 330.9727 - val_mse: 330.9727 - val_mae: 15.0620\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 348.7760 - mse: 348.7760 - mae: 15.3691 - val_loss: 330.7144 - val_mse: 330.7144 - val_mae: 15.0536\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 348.3353 - mse: 348.3353 - mae: 15.3570 - val_loss: 330.4242 - val_mse: 330.4242 - val_mae: 15.0435\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 347.8971 - mse: 347.8971 - mae: 15.3445 - val_loss: 330.1666 - val_mse: 330.1666 - val_mae: 15.0356\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 347.4633 - mse: 347.4633 - mae: 15.3338 - val_loss: 329.9093 - val_mse: 329.9093 - val_mae: 15.0275\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 347.0379 - mse: 347.0379 - mae: 15.3215 - val_loss: 329.6476 - val_mse: 329.6476 - val_mae: 15.0192\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 346.6108 - mse: 346.6108 - mae: 15.3091 - val_loss: 329.3539 - val_mse: 329.3539 - val_mae: 15.0097\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 346.1918 - mse: 346.1918 - mae: 15.2977 - val_loss: 329.0732 - val_mse: 329.0732 - val_mae: 15.0012\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 23ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 406.7050 - mse: 406.7050 - mae: 16.8252 - val_loss: 238.1042 - val_mse: 238.1042 - val_mae: 12.7419\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 312.6391 - mse: 312.6391 - mae: 13.9668 - val_loss: 204.9609 - val_mse: 204.9609 - val_mae: 11.3581\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 282.8331 - mse: 282.8331 - mae: 12.9765 - val_loss: 188.5252 - val_mse: 188.5252 - val_mae: 10.7078\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 271.5712 - mse: 271.5712 - mae: 12.5409 - val_loss: 185.0371 - val_mse: 185.0371 - val_mae: 10.6519\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 260.9515 - mse: 260.9515 - mae: 12.2291 - val_loss: 172.3315 - val_mse: 172.3315 - val_mae: 10.0010\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 255.7280 - mse: 255.7280 - mae: 12.0276 - val_loss: 169.2007 - val_mse: 169.2007 - val_mae: 9.7721\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 253.2890 - mse: 253.2890 - mae: 11.8803 - val_loss: 166.4690 - val_mse: 166.4690 - val_mae: 9.6772\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.5353 - mse: 251.5353 - mae: 11.8657 - val_loss: 169.9333 - val_mse: 169.9333 - val_mae: 10.0346\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 245.1366 - mse: 245.1366 - mae: 11.6104 - val_loss: 176.0501 - val_mse: 176.0501 - val_mae: 10.4353\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 245.0429 - mse: 245.0429 - mae: 11.6979 - val_loss: 163.5303 - val_mse: 163.5303 - val_mae: 9.5927\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 243.4912 - mse: 243.4912 - mae: 11.5990 - val_loss: 160.7202 - val_mse: 160.7202 - val_mae: 9.3643\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 243.6163 - mse: 243.6163 - mae: 11.5806 - val_loss: 159.6284 - val_mse: 159.6284 - val_mae: 9.2774\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 240.5741 - mse: 240.5741 - mae: 11.4378 - val_loss: 171.5364 - val_mse: 171.5364 - val_mae: 10.2513\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 242.5634 - mse: 242.5634 - mae: 11.6917 - val_loss: 155.0838 - val_mse: 155.0838 - val_mae: 9.0668\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 235.7857 - mse: 235.7857 - mae: 11.3277 - val_loss: 155.4175 - val_mse: 155.4175 - val_mae: 9.1192\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 236.9991 - mse: 236.9991 - mae: 11.3848 - val_loss: 153.7797 - val_mse: 153.7797 - val_mae: 8.9212\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 235.1015 - mse: 235.1015 - mae: 11.2164 - val_loss: 154.6663 - val_mse: 154.6663 - val_mae: 9.2350\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 231.5953 - mse: 231.5953 - mae: 11.1795 - val_loss: 149.1661 - val_mse: 149.1661 - val_mae: 8.8243\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 226.7880 - mse: 226.7880 - mae: 10.9528 - val_loss: 152.1269 - val_mse: 152.1269 - val_mae: 9.1806\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 227.2654 - mse: 227.2654 - mae: 11.0046 - val_loss: 158.1094 - val_mse: 158.1094 - val_mae: 9.6370\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 225.8274 - mse: 225.8274 - mae: 11.0583 - val_loss: 148.2556 - val_mse: 148.2556 - val_mae: 8.8945\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 227.1928 - mse: 227.1928 - mae: 10.9993 - val_loss: 146.2796 - val_mse: 146.2796 - val_mae: 8.6370\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.6050 - mse: 224.6050 - mae: 10.8441 - val_loss: 151.6942 - val_mse: 151.6942 - val_mae: 9.2267\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.1949 - mse: 224.1949 - mae: 10.8849 - val_loss: 145.4111 - val_mse: 145.4111 - val_mae: 8.8571\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 219.4098 - mse: 219.4098 - mae: 10.7821 - val_loss: 141.4490 - val_mse: 141.4490 - val_mae: 8.5782\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 217.6741 - mse: 217.6741 - mae: 10.6526 - val_loss: 147.1566 - val_mse: 147.1566 - val_mae: 9.0909\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 219.2449 - mse: 219.2449 - mae: 10.7961 - val_loss: 139.2242 - val_mse: 139.2242 - val_mae: 8.3630\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 220.9262 - mse: 220.9262 - mae: 10.8099 - val_loss: 138.7445 - val_mse: 138.7445 - val_mae: 8.3397\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 216.4713 - mse: 216.4713 - mae: 10.5875 - val_loss: 143.2050 - val_mse: 143.2050 - val_mae: 8.7722\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 216.2219 - mse: 216.2219 - mae: 10.6553 - val_loss: 138.1408 - val_mse: 138.1408 - val_mae: 8.2486\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 215.9162 - mse: 215.9162 - mae: 10.6103 - val_loss: 139.0841 - val_mse: 139.0841 - val_mae: 8.2602\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 216.8651 - mse: 216.8651 - mae: 10.4941 - val_loss: 145.8519 - val_mse: 145.8519 - val_mae: 9.0118\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 217.7536 - mse: 217.7536 - mae: 10.7176 - val_loss: 141.2806 - val_mse: 141.2806 - val_mae: 8.6352\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 214.2828 - mse: 214.2828 - mae: 10.5367 - val_loss: 142.6545 - val_mse: 142.6545 - val_mae: 8.7614\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 214.8106 - mse: 214.8106 - mae: 10.5886 - val_loss: 140.0463 - val_mse: 140.0463 - val_mae: 8.5216\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.3980 - mse: 215.3980 - mae: 10.5903 - val_loss: 138.1799 - val_mse: 138.1799 - val_mae: 8.3298\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 214.5230 - mse: 214.5230 - mae: 10.5621 - val_loss: 136.9948 - val_mse: 136.9948 - val_mae: 8.1757\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 215.7438 - mse: 215.7438 - mae: 10.5680 - val_loss: 139.0115 - val_mse: 139.0115 - val_mae: 8.2495\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.3933 - mse: 215.3933 - mae: 10.5261 - val_loss: 139.7549 - val_mse: 139.7549 - val_mae: 8.5156\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 215.0275 - mse: 215.0275 - mae: 10.6177 - val_loss: 141.6266 - val_mse: 141.6266 - val_mae: 8.6790\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 215.0668 - mse: 215.0668 - mae: 10.6231 - val_loss: 136.7854 - val_mse: 136.7854 - val_mae: 8.2050\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 214.0170 - mse: 214.0170 - mae: 10.5068 - val_loss: 136.8690 - val_mse: 136.8690 - val_mae: 8.2200\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 214.4792 - mse: 214.4792 - mae: 10.5168 - val_loss: 136.2852 - val_mse: 136.2852 - val_mae: 8.1730\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.9607 - mse: 212.9607 - mae: 10.4497 - val_loss: 136.4839 - val_mse: 136.4839 - val_mae: 8.2208\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 211.6685 - mse: 211.6685 - mae: 10.4945 - val_loss: 135.1504 - val_mse: 135.1504 - val_mae: 8.1985\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.8114 - mse: 209.8114 - mae: 10.3978 - val_loss: 133.8037 - val_mse: 133.8037 - val_mae: 8.0091\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 211.7737 - mse: 211.7737 - mae: 10.3925 - val_loss: 133.9981 - val_mse: 133.9981 - val_mae: 8.1056\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.2036 - mse: 209.2036 - mae: 10.3037 - val_loss: 141.6145 - val_mse: 141.6145 - val_mae: 8.8176\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 209.3189 - mse: 209.3189 - mae: 10.3597 - val_loss: 145.2208 - val_mse: 145.2208 - val_mae: 9.0799\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 210.1365 - mse: 210.1365 - mae: 10.4118 - val_loss: 132.9715 - val_mse: 132.9715 - val_mae: 8.0221\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 208.2799 - mse: 208.2799 - mae: 10.2415 - val_loss: 137.6365 - val_mse: 137.6365 - val_mae: 8.5183\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 211.1235 - mse: 211.1235 - mae: 10.4907 - val_loss: 132.7250 - val_mse: 132.7250 - val_mae: 8.0214\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 208.5276 - mse: 208.5276 - mae: 10.3180 - val_loss: 137.9472 - val_mse: 137.9472 - val_mae: 8.5516\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 207.3548 - mse: 207.3548 - mae: 10.4076 - val_loss: 134.7973 - val_mse: 134.7973 - val_mae: 8.0360\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 209.7563 - mse: 209.7563 - mae: 10.2990 - val_loss: 138.8217 - val_mse: 138.8217 - val_mae: 8.6259\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 206.4406 - mse: 206.4406 - mae: 10.2827 - val_loss: 130.8174 - val_mse: 130.8174 - val_mae: 8.0162\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 202.9301 - mse: 202.9301 - mae: 10.1181 - val_loss: 127.9089 - val_mse: 127.9089 - val_mae: 7.9180\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 200.2010 - mse: 200.2010 - mae: 10.0083 - val_loss: 133.2389 - val_mse: 133.2389 - val_mae: 8.5611\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 198.0122 - mse: 198.0122 - mae: 10.0384 - val_loss: 123.8465 - val_mse: 123.8465 - val_mae: 7.6298\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 195.2526 - mse: 195.2526 - mae: 9.9125 - val_loss: 121.3116 - val_mse: 121.3116 - val_mae: 7.6855\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 196.0622 - mse: 196.0622 - mae: 9.8984 - val_loss: 120.1286 - val_mse: 120.1286 - val_mae: 7.5124\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 191.0768 - mse: 191.0768 - mae: 9.7342 - val_loss: 118.9794 - val_mse: 118.9794 - val_mae: 7.4111\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 191.3361 - mse: 191.3361 - mae: 9.7394 - val_loss: 119.4093 - val_mse: 119.4093 - val_mae: 7.5547\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 191.7984 - mse: 191.7984 - mae: 9.6728 - val_loss: 130.3963 - val_mse: 130.3963 - val_mae: 8.5230\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 194.1079 - mse: 194.1079 - mae: 9.9877 - val_loss: 117.2991 - val_mse: 117.2991 - val_mae: 7.3714\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 189.4024 - mse: 189.4024 - mae: 9.6092 - val_loss: 116.1233 - val_mse: 116.1233 - val_mae: 7.3713\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 188.3685 - mse: 188.3685 - mae: 9.6189 - val_loss: 115.7191 - val_mse: 115.7191 - val_mae: 7.4509\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 184.2150 - mse: 184.2150 - mae: 9.5355 - val_loss: 117.5096 - val_mse: 117.5096 - val_mae: 7.6971\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 185.3275 - mse: 185.3275 - mae: 9.6670 - val_loss: 114.3542 - val_mse: 114.3542 - val_mae: 7.4071\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 182.6134 - mse: 182.6134 - mae: 9.4909 - val_loss: 113.7398 - val_mse: 113.7398 - val_mae: 7.2101\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 184.7819 - mse: 184.7819 - mae: 9.5405 - val_loss: 114.7591 - val_mse: 114.7591 - val_mae: 7.2659\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 183.6872 - mse: 183.6872 - mae: 9.4405 - val_loss: 112.6030 - val_mse: 112.6030 - val_mae: 7.2228\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 182.5582 - mse: 182.5582 - mae: 9.4965 - val_loss: 112.4895 - val_mse: 112.4895 - val_mae: 7.1411\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 182.9244 - mse: 182.9244 - mae: 9.4014 - val_loss: 124.7122 - val_mse: 124.7122 - val_mae: 8.3043\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 185.4393 - mse: 185.4393 - mae: 9.6468 - val_loss: 111.6862 - val_mse: 111.6862 - val_mae: 7.1295\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 182.9708 - mse: 182.9708 - mae: 9.4378 - val_loss: 114.0508 - val_mse: 114.0508 - val_mae: 7.2568\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 185.2249 - mse: 185.2249 - mae: 9.4750 - val_loss: 114.5790 - val_mse: 114.5790 - val_mae: 7.4739\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 181.0429 - mse: 181.0429 - mae: 9.3989 - val_loss: 113.6761 - val_mse: 113.6761 - val_mae: 7.3943\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 181.9716 - mse: 181.9716 - mae: 9.4755 - val_loss: 111.3566 - val_mse: 111.3566 - val_mae: 7.1264\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 183.5243 - mse: 183.5243 - mae: 9.4990 - val_loss: 121.5381 - val_mse: 121.5381 - val_mae: 8.0897\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.8023 - mse: 182.8023 - mae: 9.5916 - val_loss: 110.6599 - val_mse: 110.6599 - val_mae: 7.0103\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 181.1041 - mse: 181.1041 - mae: 9.3192 - val_loss: 113.2294 - val_mse: 113.2294 - val_mae: 7.3505\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 181.5046 - mse: 181.5046 - mae: 9.3688 - val_loss: 112.5451 - val_mse: 112.5451 - val_mae: 7.2862\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 181.3141 - mse: 181.3141 - mae: 9.4404 - val_loss: 112.8552 - val_mse: 112.8552 - val_mae: 7.3062\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 180.3373 - mse: 180.3373 - mae: 9.3478 - val_loss: 111.9581 - val_mse: 111.9581 - val_mae: 7.2015\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 180.7681 - mse: 180.7681 - mae: 9.3443 - val_loss: 121.3283 - val_mse: 121.3283 - val_mae: 8.0770\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 180.3264 - mse: 180.3264 - mae: 9.4253 - val_loss: 110.6432 - val_mse: 110.6432 - val_mae: 7.0222\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 181.5627 - mse: 181.5627 - mae: 9.3469 - val_loss: 121.9474 - val_mse: 121.9474 - val_mae: 8.1243\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 183.2466 - mse: 183.2466 - mae: 9.5833 - val_loss: 111.3507 - val_mse: 111.3507 - val_mae: 7.0746\n",
            "Epoch 89: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 28ms/step - loss: 442.5235 - mse: 442.5235 - mae: 17.5449 - val_loss: 239.9388 - val_mse: 239.9388 - val_mae: 12.7963\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 345.1814 - mse: 345.1814 - mae: 14.7826 - val_loss: 213.8705 - val_mse: 213.8705 - val_mae: 11.6036\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 323.4138 - mse: 323.4138 - mae: 14.0304 - val_loss: 202.3054 - val_mse: 202.3054 - val_mae: 11.2185\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 302.3058 - mse: 302.3058 - mae: 13.4425 - val_loss: 190.5084 - val_mse: 190.5084 - val_mae: 10.7102\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 293.7149 - mse: 293.7149 - mae: 13.1579 - val_loss: 184.1077 - val_mse: 184.1077 - val_mae: 10.3341\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 291.9175 - mse: 291.9175 - mae: 13.0577 - val_loss: 178.0150 - val_mse: 178.0150 - val_mae: 10.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 287.4895 - mse: 287.4895 - mae: 12.8630 - val_loss: 177.6122 - val_mse: 177.6122 - val_mae: 10.0195\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 288.7447 - mse: 288.7447 - mae: 12.8580 - val_loss: 181.7393 - val_mse: 181.7393 - val_mae: 10.3024\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 281.8526 - mse: 281.8526 - mae: 12.6943 - val_loss: 184.8514 - val_mse: 184.8514 - val_mae: 10.5238\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 281.3423 - mse: 281.3423 - mae: 12.7127 - val_loss: 184.9525 - val_mse: 184.9525 - val_mae: 10.5304\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 281.8955 - mse: 281.8955 - mae: 12.7574 - val_loss: 173.6183 - val_mse: 173.6183 - val_mae: 9.7083\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 280.6297 - mse: 280.6297 - mae: 12.5935 - val_loss: 180.0443 - val_mse: 180.0443 - val_mae: 10.2138\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 278.5387 - mse: 278.5387 - mae: 12.6037 - val_loss: 175.9172 - val_mse: 175.9172 - val_mae: 10.0290\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 277.3952 - mse: 277.3952 - mae: 12.6037 - val_loss: 170.6024 - val_mse: 170.6024 - val_mae: 9.6234\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 274.1687 - mse: 274.1687 - mae: 12.4035 - val_loss: 170.8597 - val_mse: 170.8597 - val_mae: 9.6444\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 274.2933 - mse: 274.2933 - mae: 12.4203 - val_loss: 169.4166 - val_mse: 169.4166 - val_mae: 9.5399\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 275.1753 - mse: 275.1753 - mae: 12.3807 - val_loss: 178.3912 - val_mse: 178.3912 - val_mae: 10.2358\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 275.9768 - mse: 275.9768 - mae: 12.4808 - val_loss: 170.0150 - val_mse: 170.0150 - val_mae: 9.6065\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 272.5179 - mse: 272.5179 - mae: 12.2998 - val_loss: 171.3265 - val_mse: 171.3265 - val_mae: 9.7420\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 271.7131 - mse: 271.7131 - mae: 12.3201 - val_loss: 171.6791 - val_mse: 171.6791 - val_mae: 9.8693\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 267.8328 - mse: 267.8328 - mae: 12.2630 - val_loss: 165.6657 - val_mse: 165.6657 - val_mae: 9.3887\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 269.1111 - mse: 269.1111 - mae: 12.1872 - val_loss: 164.1133 - val_mse: 164.1133 - val_mae: 9.2493\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 267.1671 - mse: 267.1671 - mae: 12.0814 - val_loss: 167.0640 - val_mse: 167.0640 - val_mae: 9.5735\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 265.8719 - mse: 265.8719 - mae: 12.0765 - val_loss: 168.5873 - val_mse: 168.5873 - val_mae: 9.7651\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 263.9745 - mse: 263.9745 - mae: 12.1050 - val_loss: 161.9454 - val_mse: 161.9454 - val_mae: 9.2312\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 265.0626 - mse: 265.0626 - mae: 12.0730 - val_loss: 168.4165 - val_mse: 168.4165 - val_mae: 9.7644\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.8729 - mse: 263.8729 - mae: 12.0636 - val_loss: 159.6950 - val_mse: 159.6950 - val_mae: 8.9977\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 266.0427 - mse: 266.0427 - mae: 12.0929 - val_loss: 159.3192 - val_mse: 159.3192 - val_mae: 8.9789\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262.9808 - mse: 262.9808 - mae: 11.9619 - val_loss: 160.0240 - val_mse: 160.0240 - val_mae: 9.0639\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.3953 - mse: 262.3953 - mae: 11.9620 - val_loss: 160.2508 - val_mse: 160.2508 - val_mae: 9.0824\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.7427 - mse: 263.7427 - mae: 12.0549 - val_loss: 160.9453 - val_mse: 160.9453 - val_mae: 9.0753\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 265.6312 - mse: 265.6312 - mae: 11.8859 - val_loss: 174.1006 - val_mse: 174.1006 - val_mae: 10.1906\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.5198 - mse: 262.5198 - mae: 12.0358 - val_loss: 164.2204 - val_mse: 164.2204 - val_mae: 9.4462\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.9103 - mse: 260.9103 - mae: 11.9131 - val_loss: 167.9539 - val_mse: 167.9539 - val_mae: 9.7463\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 261.1980 - mse: 261.1980 - mae: 11.9595 - val_loss: 163.1467 - val_mse: 163.1467 - val_mae: 9.3574\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 261.6650 - mse: 261.6650 - mae: 11.9471 - val_loss: 161.6239 - val_mse: 161.6239 - val_mae: 9.2196\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.3742 - mse: 262.3742 - mae: 11.9887 - val_loss: 158.3042 - val_mse: 158.3042 - val_mae: 8.8797\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262.8741 - mse: 262.8741 - mae: 11.9172 - val_loss: 158.1483 - val_mse: 158.1483 - val_mae: 8.8687\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.8013 - mse: 260.8013 - mae: 11.8975 - val_loss: 159.0502 - val_mse: 159.0502 - val_mae: 8.9769\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.1166 - mse: 260.1166 - mae: 11.8342 - val_loss: 165.1915 - val_mse: 165.1915 - val_mae: 9.5389\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 261.8314 - mse: 261.8314 - mae: 11.9468 - val_loss: 158.5829 - val_mse: 158.5829 - val_mae: 8.9310\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.6626 - mse: 260.6626 - mae: 11.8968 - val_loss: 157.6728 - val_mse: 157.6728 - val_mae: 8.8290\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 260.7792 - mse: 260.7793 - mae: 11.8865 - val_loss: 158.5970 - val_mse: 158.5970 - val_mae: 8.9322\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 259.9287 - mse: 259.9287 - mae: 11.7985 - val_loss: 160.5225 - val_mse: 160.5225 - val_mae: 9.1323\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 259.7462 - mse: 259.7462 - mae: 11.9479 - val_loss: 156.4872 - val_mse: 156.4872 - val_mae: 8.7931\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 257.7919 - mse: 257.7919 - mae: 11.7996 - val_loss: 154.9068 - val_mse: 154.9068 - val_mae: 8.7314\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 257.1313 - mse: 257.1313 - mae: 11.7114 - val_loss: 154.0612 - val_mse: 154.0612 - val_mae: 8.6955\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.2145 - mse: 256.2145 - mae: 11.6668 - val_loss: 160.2375 - val_mse: 160.2375 - val_mae: 9.2981\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 254.1150 - mse: 254.1150 - mae: 11.5786 - val_loss: 183.5546 - val_mse: 183.5546 - val_mae: 11.0146\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 255.5095 - mse: 255.5095 - mae: 11.8583 - val_loss: 151.0668 - val_mse: 151.0668 - val_mae: 8.5475\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 253.1217 - mse: 253.1217 - mae: 11.5831 - val_loss: 154.4628 - val_mse: 154.4628 - val_mae: 8.9416\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.6498 - mse: 256.6498 - mae: 11.8119 - val_loss: 153.3487 - val_mse: 153.3487 - val_mae: 8.8406\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 250.3226 - mse: 250.3226 - mae: 11.5067 - val_loss: 154.0668 - val_mse: 154.0668 - val_mae: 8.9116\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 250.8849 - mse: 250.8849 - mae: 11.5992 - val_loss: 150.1789 - val_mse: 150.1789 - val_mae: 8.5165\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 250.3484 - mse: 250.3484 - mae: 11.4678 - val_loss: 163.5570 - val_mse: 163.5570 - val_mae: 9.7063\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 252.5532 - mse: 252.5532 - mae: 11.6517 - val_loss: 152.1842 - val_mse: 152.1842 - val_mae: 8.7681\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.0148 - mse: 249.0148 - mae: 11.4636 - val_loss: 148.6930 - val_mse: 148.6930 - val_mae: 8.5811\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 244.9977 - mse: 244.9977 - mae: 11.3264 - val_loss: 149.3581 - val_mse: 149.3581 - val_mae: 8.7948\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 243.8728 - mse: 243.8728 - mae: 11.3175 - val_loss: 151.5258 - val_mse: 151.5258 - val_mae: 9.0152\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 244.6074 - mse: 244.6074 - mae: 11.3429 - val_loss: 151.7750 - val_mse: 151.7750 - val_mae: 9.0408\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 245.4109 - mse: 245.4109 - mae: 11.4505 - val_loss: 143.9975 - val_mse: 143.9975 - val_mae: 8.2898\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 240.4609 - mse: 240.4609 - mae: 11.2176 - val_loss: 141.1781 - val_mse: 141.1781 - val_mae: 8.1592\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 237.5642 - mse: 237.5642 - mae: 11.1750 - val_loss: 140.3876 - val_mse: 140.3876 - val_mae: 8.1442\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 236.5574 - mse: 236.5574 - mae: 11.0937 - val_loss: 142.1833 - val_mse: 142.1833 - val_mae: 8.3451\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 237.3804 - mse: 237.3804 - mae: 11.0837 - val_loss: 141.0204 - val_mse: 141.0204 - val_mae: 8.2177\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 236.6166 - mse: 236.6166 - mae: 11.0299 - val_loss: 145.7268 - val_mse: 145.7268 - val_mae: 8.6920\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 236.7799 - mse: 236.7799 - mae: 11.1685 - val_loss: 139.9040 - val_mse: 139.9040 - val_mae: 8.0637\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 239.1284 - mse: 239.1284 - mae: 11.1561 - val_loss: 147.2838 - val_mse: 147.2838 - val_mae: 8.8283\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 239.0612 - mse: 239.0612 - mae: 11.2645 - val_loss: 141.8031 - val_mse: 141.8031 - val_mae: 8.3120\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 235.9576 - mse: 235.9576 - mae: 11.0735 - val_loss: 139.5758 - val_mse: 139.5758 - val_mae: 8.0418\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 236.9868 - mse: 236.9868 - mae: 11.1326 - val_loss: 140.5181 - val_mse: 140.5181 - val_mae: 8.1036\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 237.2415 - mse: 237.2415 - mae: 11.0404 - val_loss: 140.1501 - val_mse: 140.1501 - val_mae: 8.1375\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 238.0555 - mse: 238.0555 - mae: 11.1366 - val_loss: 140.1678 - val_mse: 140.1678 - val_mae: 8.1389\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 235.1694 - mse: 235.1694 - mae: 10.9789 - val_loss: 163.2443 - val_mse: 163.2443 - val_mae: 10.0306\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 240.7873 - mse: 240.7873 - mae: 11.3771 - val_loss: 144.1362 - val_mse: 144.1362 - val_mae: 8.5502\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 237.1951 - mse: 237.1951 - mae: 11.1245 - val_loss: 139.9201 - val_mse: 139.9201 - val_mae: 8.0583\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 237.0896 - mse: 237.0896 - mae: 11.0048 - val_loss: 161.4255 - val_mse: 161.4255 - val_mae: 9.9020\n",
            "Epoch 77: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 348.3429 - mse: 348.3429 - mae: 15.3775 - val_loss: 282.9728 - val_mse: 282.9728 - val_mae: 13.5151\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 267.8824 - mse: 267.8824 - mae: 12.7935 - val_loss: 244.4539 - val_mse: 244.4539 - val_mae: 12.0291\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 244.5849 - mse: 244.5849 - mae: 11.9034 - val_loss: 230.2757 - val_mse: 230.2757 - val_mae: 11.5033\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 233.1452 - mse: 233.1452 - mae: 11.3868 - val_loss: 224.7141 - val_mse: 224.7141 - val_mae: 11.2876\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 229.9247 - mse: 229.9247 - mae: 11.2661 - val_loss: 221.2707 - val_mse: 221.2707 - val_mae: 11.0493\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 226.6911 - mse: 226.6911 - mae: 11.1138 - val_loss: 215.9901 - val_mse: 215.9901 - val_mae: 10.8865\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.9000 - mse: 224.9000 - mae: 11.1238 - val_loss: 212.0935 - val_mse: 212.0935 - val_mae: 10.9107\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 219.6346 - mse: 219.6346 - mae: 10.9701 - val_loss: 209.5084 - val_mse: 209.5084 - val_mae: 10.5978\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 216.5636 - mse: 216.5636 - mae: 10.7210 - val_loss: 206.1348 - val_mse: 206.1348 - val_mae: 10.5472\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 215.3034 - mse: 215.3034 - mae: 10.7243 - val_loss: 204.9512 - val_mse: 204.9512 - val_mae: 10.4864\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 214.4135 - mse: 214.4135 - mae: 10.6576 - val_loss: 203.8481 - val_mse: 203.8481 - val_mae: 10.4798\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 214.7608 - mse: 214.7608 - mae: 10.7008 - val_loss: 206.6361 - val_mse: 206.6361 - val_mae: 10.5791\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 214.4559 - mse: 214.4559 - mae: 10.6261 - val_loss: 202.4072 - val_mse: 202.4072 - val_mae: 10.4079\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 213.6405 - mse: 213.6405 - mae: 10.6014 - val_loss: 201.5815 - val_mse: 201.5815 - val_mae: 10.3869\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.3002 - mse: 212.3002 - mae: 10.5771 - val_loss: 201.6779 - val_mse: 201.6779 - val_mae: 10.3375\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 211.8040 - mse: 211.8040 - mae: 10.5130 - val_loss: 199.0371 - val_mse: 199.0371 - val_mae: 10.3759\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 208.5061 - mse: 208.5061 - mae: 10.5275 - val_loss: 196.8998 - val_mse: 196.8998 - val_mae: 10.1990\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 205.7318 - mse: 205.7318 - mae: 10.3489 - val_loss: 192.2557 - val_mse: 192.2557 - val_mae: 10.0553\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 203.7050 - mse: 203.7050 - mae: 10.3102 - val_loss: 192.8865 - val_mse: 192.8865 - val_mae: 10.0587\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 203.1599 - mse: 203.1599 - mae: 10.2201 - val_loss: 189.9765 - val_mse: 189.9765 - val_mae: 9.9873\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 201.9743 - mse: 201.9743 - mae: 10.2663 - val_loss: 188.8216 - val_mse: 188.8216 - val_mae: 9.9230\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 199.4198 - mse: 199.4198 - mae: 10.0638 - val_loss: 187.3768 - val_mse: 187.3768 - val_mae: 9.9250\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 198.7896 - mse: 198.7896 - mae: 10.1624 - val_loss: 188.8533 - val_mse: 188.8533 - val_mae: 9.9368\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 199.1994 - mse: 199.1994 - mae: 10.0412 - val_loss: 189.5556 - val_mse: 189.5556 - val_mae: 10.1387\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 199.4085 - mse: 199.4085 - mae: 10.1688 - val_loss: 186.0235 - val_mse: 186.0235 - val_mae: 9.8408\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 197.9525 - mse: 197.9525 - mae: 10.0277 - val_loss: 187.3157 - val_mse: 187.3157 - val_mae: 9.9818\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 199.0298 - mse: 199.0298 - mae: 10.1783 - val_loss: 186.2907 - val_mse: 186.2907 - val_mae: 9.8090\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 198.8569 - mse: 198.8569 - mae: 10.1096 - val_loss: 185.3552 - val_mse: 185.3552 - val_mae: 9.8082\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 197.4254 - mse: 197.4254 - mae: 10.0156 - val_loss: 185.4353 - val_mse: 185.4353 - val_mae: 9.7773\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 197.5207 - mse: 197.5207 - mae: 10.0073 - val_loss: 185.3896 - val_mse: 185.3896 - val_mae: 9.8203\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 196.8329 - mse: 196.8329 - mae: 9.9943 - val_loss: 184.6288 - val_mse: 184.6288 - val_mae: 9.7573\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 197.5447 - mse: 197.5447 - mae: 9.9934 - val_loss: 186.3718 - val_mse: 186.3718 - val_mae: 9.9495\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 197.6407 - mse: 197.6407 - mae: 10.1010 - val_loss: 188.0783 - val_mse: 188.0783 - val_mae: 9.9175\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 198.2439 - mse: 198.2439 - mae: 10.0355 - val_loss: 184.0159 - val_mse: 184.0159 - val_mae: 9.7322\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 196.1254 - mse: 196.1254 - mae: 9.9377 - val_loss: 183.7934 - val_mse: 183.7934 - val_mae: 9.7203\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 197.1597 - mse: 197.1597 - mae: 9.9754 - val_loss: 185.5513 - val_mse: 185.5513 - val_mae: 9.7807\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 197.7767 - mse: 197.7767 - mae: 10.0707 - val_loss: 186.9471 - val_mse: 186.9471 - val_mae: 9.8462\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 198.0346 - mse: 198.0346 - mae: 10.0177 - val_loss: 186.0646 - val_mse: 186.0646 - val_mae: 9.8206\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 197.0270 - mse: 197.0270 - mae: 9.9508 - val_loss: 183.0118 - val_mse: 183.0118 - val_mae: 9.7377\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 195.4086 - mse: 195.4086 - mae: 9.9442 - val_loss: 183.0387 - val_mse: 183.0387 - val_mae: 9.6581\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 196.0348 - mse: 196.0348 - mae: 9.9700 - val_loss: 184.2272 - val_mse: 184.2272 - val_mae: 9.7126\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 194.4087 - mse: 194.4087 - mae: 9.7461 - val_loss: 205.1126 - val_mse: 205.1126 - val_mae: 11.1590\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 196.3874 - mse: 196.3874 - mae: 10.1557 - val_loss: 176.6726 - val_mse: 176.6726 - val_mae: 9.4754\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 189.4968 - mse: 189.4968 - mae: 9.7356 - val_loss: 176.9429 - val_mse: 176.9429 - val_mae: 9.4676\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 188.8102 - mse: 188.8102 - mae: 9.6443 - val_loss: 174.8400 - val_mse: 174.8400 - val_mae: 9.4278\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 186.9623 - mse: 186.9623 - mae: 9.6185 - val_loss: 175.2840 - val_mse: 175.2840 - val_mae: 9.4487\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 187.2734 - mse: 187.2734 - mae: 9.6555 - val_loss: 177.1357 - val_mse: 177.1357 - val_mae: 9.4989\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 186.9473 - mse: 186.9473 - mae: 9.6426 - val_loss: 174.7184 - val_mse: 174.7184 - val_mae: 9.4044\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 184.9086 - mse: 184.9086 - mae: 9.5782 - val_loss: 170.9713 - val_mse: 170.9713 - val_mae: 9.2770\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 184.0096 - mse: 184.0096 - mae: 9.5350 - val_loss: 172.5327 - val_mse: 172.5327 - val_mae: 9.3972\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 182.9764 - mse: 182.9764 - mae: 9.5383 - val_loss: 170.8315 - val_mse: 170.8315 - val_mae: 9.2398\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 184.9602 - mse: 184.9602 - mae: 9.4874 - val_loss: 169.9961 - val_mse: 169.9961 - val_mae: 9.2131\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 182.8927 - mse: 182.8927 - mae: 9.4473 - val_loss: 172.3842 - val_mse: 172.3842 - val_mae: 9.4316\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 182.4480 - mse: 182.4480 - mae: 9.4630 - val_loss: 172.1069 - val_mse: 172.1069 - val_mae: 9.4095\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 184.5993 - mse: 184.5993 - mae: 9.6098 - val_loss: 170.2140 - val_mse: 170.2140 - val_mae: 9.2104\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 184.4690 - mse: 184.4690 - mae: 9.5218 - val_loss: 171.4434 - val_mse: 171.4434 - val_mae: 9.2593\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 183.1906 - mse: 183.1906 - mae: 9.4508 - val_loss: 169.3708 - val_mse: 169.3708 - val_mae: 9.1688\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.9341 - mse: 182.9341 - mae: 9.4458 - val_loss: 169.2410 - val_mse: 169.2410 - val_mae: 9.1798\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 182.8917 - mse: 182.8917 - mae: 9.4861 - val_loss: 171.2737 - val_mse: 171.2737 - val_mae: 9.3549\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 183.4541 - mse: 183.4541 - mae: 9.5937 - val_loss: 175.7674 - val_mse: 175.7674 - val_mae: 9.5634\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.9265 - mse: 182.9265 - mae: 9.3555 - val_loss: 169.4216 - val_mse: 169.4216 - val_mae: 9.2083\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 181.7733 - mse: 181.7733 - mae: 9.4826 - val_loss: 168.7186 - val_mse: 168.7186 - val_mae: 9.1268\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 182.0052 - mse: 182.0052 - mae: 9.4355 - val_loss: 171.4019 - val_mse: 171.4019 - val_mae: 9.2706\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.3583 - mse: 182.3583 - mae: 9.3884 - val_loss: 168.2702 - val_mse: 168.2702 - val_mae: 9.1367\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.0926 - mse: 182.0926 - mae: 9.4080 - val_loss: 169.3086 - val_mse: 169.3086 - val_mae: 9.1172\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 184.0395 - mse: 184.0395 - mae: 9.4462 - val_loss: 169.7761 - val_mse: 169.7761 - val_mae: 9.2109\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 182.3015 - mse: 182.3015 - mae: 9.4382 - val_loss: 170.0525 - val_mse: 170.0525 - val_mae: 9.2306\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 183.7533 - mse: 183.7533 - mae: 9.5559 - val_loss: 170.7596 - val_mse: 170.7596 - val_mae: 9.2130\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 180.8689 - mse: 180.8689 - mae: 9.3245 - val_loss: 168.8707 - val_mse: 168.8707 - val_mae: 9.1795\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 181.1428 - mse: 181.1428 - mae: 9.4408 - val_loss: 173.3963 - val_mse: 173.3963 - val_mae: 9.4166\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 181.4340 - mse: 181.4340 - mae: 9.3561 - val_loss: 168.2630 - val_mse: 168.2630 - val_mae: 9.0668\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 181.5072 - mse: 181.5072 - mae: 9.3758 - val_loss: 169.4009 - val_mse: 169.4009 - val_mae: 9.1399\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 180.8825 - mse: 180.8825 - mae: 9.2605 - val_loss: 167.6743 - val_mse: 167.6743 - val_mae: 9.0680\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 179.4194 - mse: 179.4194 - mae: 9.3334 - val_loss: 164.7944 - val_mse: 164.7944 - val_mae: 8.9369\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 177.5491 - mse: 177.5491 - mae: 9.2433 - val_loss: 164.5455 - val_mse: 164.5455 - val_mae: 8.9301\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 178.6774 - mse: 178.6774 - mae: 9.2337 - val_loss: 167.6595 - val_mse: 167.6595 - val_mae: 9.2195\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 178.1074 - mse: 178.1074 - mae: 9.3072 - val_loss: 163.0714 - val_mse: 163.0714 - val_mae: 8.9144\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 177.2215 - mse: 177.2215 - mae: 9.1835 - val_loss: 165.8087 - val_mse: 165.8087 - val_mae: 9.1033\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 174.9160 - mse: 174.9160 - mae: 9.2287 - val_loss: 167.9087 - val_mse: 167.9087 - val_mae: 9.2169\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 177.3363 - mse: 177.3363 - mae: 9.1389 - val_loss: 163.5938 - val_mse: 163.5938 - val_mae: 8.9239\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 178.4783 - mse: 178.4783 - mae: 9.2396 - val_loss: 165.7374 - val_mse: 165.7374 - val_mae: 9.1160\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 176.2580 - mse: 176.2580 - mae: 9.2914 - val_loss: 160.3363 - val_mse: 160.3363 - val_mae: 8.8340\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 174.6760 - mse: 174.6760 - mae: 9.2145 - val_loss: 163.4364 - val_mse: 163.4364 - val_mae: 9.0642\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174.7965 - mse: 174.7965 - mae: 9.2875 - val_loss: 159.6843 - val_mse: 159.6843 - val_mae: 8.7543\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 173.0039 - mse: 173.0039 - mae: 9.0625 - val_loss: 162.8629 - val_mse: 162.8629 - val_mae: 8.9780\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174.9423 - mse: 174.9423 - mae: 9.1800 - val_loss: 162.0579 - val_mse: 162.0579 - val_mae: 8.9381\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 175.8325 - mse: 175.8325 - mae: 9.2718 - val_loss: 159.6080 - val_mse: 159.6080 - val_mae: 8.7524\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 172.1038 - mse: 172.1038 - mae: 9.0240 - val_loss: 159.4748 - val_mse: 159.4748 - val_mae: 8.7834\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 172.2352 - mse: 172.2352 - mae: 9.0336 - val_loss: 158.7899 - val_mse: 158.7899 - val_mae: 8.7568\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 173.8939 - mse: 173.8939 - mae: 9.1025 - val_loss: 162.5176 - val_mse: 162.5176 - val_mae: 8.9861\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 173.4325 - mse: 173.4325 - mae: 9.1475 - val_loss: 158.9514 - val_mse: 158.9514 - val_mae: 8.7408\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 173.1352 - mse: 173.1352 - mae: 9.0658 - val_loss: 166.4949 - val_mse: 166.4949 - val_mae: 9.2671\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 173.8917 - mse: 173.8917 - mae: 9.2056 - val_loss: 158.4541 - val_mse: 158.4541 - val_mae: 8.7077\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 174.0434 - mse: 174.0434 - mae: 9.1750 - val_loss: 158.5949 - val_mse: 158.5949 - val_mae: 8.7246\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 172.0959 - mse: 172.0959 - mae: 8.9527 - val_loss: 161.3957 - val_mse: 161.3957 - val_mae: 8.9287\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 173.2764 - mse: 173.2764 - mae: 9.1974 - val_loss: 160.6955 - val_mse: 160.6955 - val_mae: 8.8378\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 171.9584 - mse: 171.9584 - mae: 8.9611 - val_loss: 158.5259 - val_mse: 158.5259 - val_mae: 8.7380\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 171.4798 - mse: 171.4798 - mae: 8.9714 - val_loss: 158.1311 - val_mse: 158.1311 - val_mae: 8.7014\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 172.5098 - mse: 172.5098 - mae: 9.0585 - val_loss: 159.2757 - val_mse: 159.2757 - val_mae: 8.7424\n",
            "Epoch 99: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 595.6483 - mse: 595.6483 - mae: 22.5506 - val_loss: 448.8346 - val_mse: 448.8346 - val_mae: 20.1576\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 595.6475 - mse: 595.6475 - mae: 22.5506 - val_loss: 448.8335 - val_mse: 448.8335 - val_mae: 20.1575\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 595.6465 - mse: 595.6465 - mae: 22.5505 - val_loss: 448.8324 - val_mse: 448.8324 - val_mae: 20.1575\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 595.6457 - mse: 595.6457 - mae: 22.5505 - val_loss: 448.8312 - val_mse: 448.8312 - val_mae: 20.1575\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 595.6448 - mse: 595.6448 - mae: 22.5505 - val_loss: 448.8301 - val_mse: 448.8301 - val_mae: 20.1574\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 595.6439 - mse: 595.6439 - mae: 22.5505 - val_loss: 448.8292 - val_mse: 448.8292 - val_mae: 20.1574\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 595.6431 - mse: 595.6431 - mae: 22.5505 - val_loss: 448.8282 - val_mse: 448.8282 - val_mae: 20.1574\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 595.6424 - mse: 595.6424 - mae: 22.5505 - val_loss: 448.8270 - val_mse: 448.8270 - val_mae: 20.1574\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 595.6415 - mse: 595.6415 - mae: 22.5504 - val_loss: 448.8260 - val_mse: 448.8260 - val_mae: 20.1573\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 595.6406 - mse: 595.6406 - mae: 22.5504 - val_loss: 448.8249 - val_mse: 448.8249 - val_mae: 20.1573\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 595.6397 - mse: 595.6397 - mae: 22.5504 - val_loss: 448.8238 - val_mse: 448.8238 - val_mae: 20.1573\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 4716.9609 - mse: 4716.9609 - mae: 51.4801 - val_loss: 5115.6592 - val_mse: 5115.6592 - val_mae: 53.2494\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4715.8716 - mse: 4715.8716 - mae: 51.4737 - val_loss: 5114.4468 - val_mse: 5114.4468 - val_mae: 53.2428\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4714.7915 - mse: 4714.7915 - mae: 51.4674 - val_loss: 5113.2305 - val_mse: 5113.2305 - val_mae: 53.2361\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4713.6973 - mse: 4713.6973 - mae: 51.4609 - val_loss: 5112.0244 - val_mse: 5112.0244 - val_mae: 53.2294\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4712.6265 - mse: 4712.6265 - mae: 51.4546 - val_loss: 5110.8213 - val_mse: 5110.8213 - val_mae: 53.2228\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4711.5386 - mse: 4711.5386 - mae: 51.4482 - val_loss: 5109.6313 - val_mse: 5109.6313 - val_mae: 53.2163\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4710.4897 - mse: 4710.4897 - mae: 51.4419 - val_loss: 5108.4287 - val_mse: 5108.4287 - val_mae: 53.2096\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4709.4399 - mse: 4709.4399 - mae: 51.4356 - val_loss: 5107.2124 - val_mse: 5107.2124 - val_mae: 53.2029\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4708.3286 - mse: 4708.3286 - mae: 51.4291 - val_loss: 5106.0273 - val_mse: 5106.0273 - val_mae: 53.1964\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4707.2656 - mse: 4707.2656 - mae: 51.4229 - val_loss: 5104.8447 - val_mse: 5104.8447 - val_mae: 53.1898\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4706.2183 - mse: 4706.2183 - mae: 51.4166 - val_loss: 5103.6377 - val_mse: 5103.6377 - val_mae: 53.1832\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4705.1387 - mse: 4705.1387 - mae: 51.4102 - val_loss: 5102.4595 - val_mse: 5102.4595 - val_mae: 53.1767\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4704.0771 - mse: 4704.0771 - mae: 51.4039 - val_loss: 5101.2715 - val_mse: 5101.2715 - val_mae: 53.1701\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4703.0518 - mse: 4703.0518 - mae: 51.3977 - val_loss: 5100.0615 - val_mse: 5100.0615 - val_mae: 53.1635\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4701.9658 - mse: 4701.9658 - mae: 51.3913 - val_loss: 5098.8652 - val_mse: 5098.8652 - val_mae: 53.1569\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4700.8926 - mse: 4700.8926 - mae: 51.3850 - val_loss: 5097.6919 - val_mse: 5097.6919 - val_mae: 53.1503\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4699.8584 - mse: 4699.8584 - mae: 51.3788 - val_loss: 5096.5034 - val_mse: 5096.5034 - val_mae: 53.1438\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4698.7817 - mse: 4698.7817 - mae: 51.3725 - val_loss: 5095.3242 - val_mse: 5095.3242 - val_mae: 53.1372\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4697.7300 - mse: 4697.7300 - mae: 51.3661 - val_loss: 5094.1279 - val_mse: 5094.1279 - val_mae: 53.1306\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4696.6621 - mse: 4696.6621 - mae: 51.3598 - val_loss: 5092.9365 - val_mse: 5092.9365 - val_mae: 53.1240\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4695.6074 - mse: 4695.6074 - mae: 51.3535 - val_loss: 5091.7412 - val_mse: 5091.7412 - val_mae: 53.1174\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4694.5293 - mse: 4694.5293 - mae: 51.3472 - val_loss: 5090.5586 - val_mse: 5090.5586 - val_mae: 53.1109\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4693.4595 - mse: 4693.4595 - mae: 51.3408 - val_loss: 5089.3770 - val_mse: 5089.3770 - val_mae: 53.1043\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4692.4189 - mse: 4692.4189 - mae: 51.3346 - val_loss: 5088.1782 - val_mse: 5088.1782 - val_mae: 53.0977\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4691.3501 - mse: 4691.3501 - mae: 51.3283 - val_loss: 5087.0063 - val_mse: 5087.0063 - val_mae: 53.0912\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4690.3062 - mse: 4690.3062 - mae: 51.3220 - val_loss: 5085.8145 - val_mse: 5085.8145 - val_mae: 53.0846\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4689.2290 - mse: 4689.2290 - mae: 51.3156 - val_loss: 5084.6377 - val_mse: 5084.6377 - val_mae: 53.0781\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4688.2139 - mse: 4688.2139 - mae: 51.3094 - val_loss: 5083.4189 - val_mse: 5083.4189 - val_mae: 53.0714\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4687.1040 - mse: 4687.1040 - mae: 51.3031 - val_loss: 5082.2505 - val_mse: 5082.2505 - val_mae: 53.0649\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4686.0542 - mse: 4686.0542 - mae: 51.2968 - val_loss: 5081.0654 - val_mse: 5081.0654 - val_mae: 53.0583\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 4685.0000 - mse: 4685.0000 - mae: 51.2905 - val_loss: 5079.8901 - val_mse: 5079.8901 - val_mae: 53.0518\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4683.9478 - mse: 4683.9478 - mae: 51.2842 - val_loss: 5078.7207 - val_mse: 5078.7207 - val_mae: 53.0453\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4682.8896 - mse: 4682.8896 - mae: 51.2780 - val_loss: 5077.5571 - val_mse: 5077.5571 - val_mae: 53.0388\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4681.8423 - mse: 4681.8423 - mae: 51.2717 - val_loss: 5076.3760 - val_mse: 5076.3760 - val_mae: 53.0322\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4680.8286 - mse: 4680.8286 - mae: 51.2654 - val_loss: 5075.1553 - val_mse: 5075.1553 - val_mae: 53.0256\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4679.7041 - mse: 4679.7041 - mae: 51.2590 - val_loss: 5074.0044 - val_mse: 5074.0044 - val_mae: 53.0191\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4678.6865 - mse: 4678.6865 - mae: 51.2529 - val_loss: 5072.8198 - val_mse: 5072.8198 - val_mae: 53.0126\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4677.6235 - mse: 4677.6235 - mae: 51.2466 - val_loss: 5071.6377 - val_mse: 5071.6377 - val_mae: 53.0060\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4676.5698 - mse: 4676.5698 - mae: 51.2403 - val_loss: 5070.4556 - val_mse: 5070.4556 - val_mae: 52.9994\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4675.5146 - mse: 4675.5146 - mae: 51.2340 - val_loss: 5069.2632 - val_mse: 5069.2632 - val_mae: 52.9928\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4674.4531 - mse: 4674.4531 - mae: 51.2276 - val_loss: 5068.0742 - val_mse: 5068.0742 - val_mae: 52.9863\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4673.3901 - mse: 4673.3901 - mae: 51.2213 - val_loss: 5066.8950 - val_mse: 5066.8950 - val_mae: 52.9797\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4672.3491 - mse: 4672.3491 - mae: 51.2150 - val_loss: 5065.7119 - val_mse: 5065.7119 - val_mae: 52.9732\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4671.3179 - mse: 4671.3179 - mae: 51.2088 - val_loss: 5064.5127 - val_mse: 5064.5127 - val_mae: 52.9665\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4670.2212 - mse: 4670.2212 - mae: 51.2024 - val_loss: 5063.3574 - val_mse: 5063.3574 - val_mae: 52.9601\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4669.1841 - mse: 4669.1841 - mae: 51.1962 - val_loss: 5062.1919 - val_mse: 5062.1919 - val_mae: 52.9536\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4668.1299 - mse: 4668.1299 - mae: 51.1900 - val_loss: 5061.0259 - val_mse: 5061.0259 - val_mae: 52.9471\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4667.0991 - mse: 4667.0991 - mae: 51.1837 - val_loss: 5059.8320 - val_mse: 5059.8320 - val_mae: 52.9405\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4666.0396 - mse: 4666.0396 - mae: 51.1774 - val_loss: 5058.6504 - val_mse: 5058.6504 - val_mae: 52.9339\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4664.9810 - mse: 4664.9810 - mae: 51.1711 - val_loss: 5057.4653 - val_mse: 5057.4653 - val_mae: 52.9274\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4663.9146 - mse: 4663.9146 - mae: 51.1648 - val_loss: 5056.2686 - val_mse: 5056.2686 - val_mae: 52.9208\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4662.8364 - mse: 4662.8364 - mae: 51.1584 - val_loss: 5055.0737 - val_mse: 5055.0737 - val_mae: 52.9142\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4661.7915 - mse: 4661.7915 - mae: 51.1521 - val_loss: 5053.8657 - val_mse: 5053.8657 - val_mae: 52.9075\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4660.6973 - mse: 4660.6973 - mae: 51.1457 - val_loss: 5052.6909 - val_mse: 5052.6909 - val_mae: 52.9010\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4659.6445 - mse: 4659.6445 - mae: 51.1395 - val_loss: 5051.5063 - val_mse: 5051.5063 - val_mae: 52.8944\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4658.6030 - mse: 4658.6030 - mae: 51.1333 - val_loss: 5050.3193 - val_mse: 5050.3193 - val_mae: 52.8879\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4657.5425 - mse: 4657.5425 - mae: 51.1269 - val_loss: 5049.1528 - val_mse: 5049.1528 - val_mae: 52.8814\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4656.5073 - mse: 4656.5073 - mae: 51.1207 - val_loss: 5047.9722 - val_mse: 5047.9722 - val_mae: 52.8748\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4655.4453 - mse: 4655.4453 - mae: 51.1144 - val_loss: 5046.7939 - val_mse: 5046.7939 - val_mae: 52.8683\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4654.3960 - mse: 4654.3960 - mae: 51.1082 - val_loss: 5045.6318 - val_mse: 5045.6318 - val_mae: 52.8618\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4653.3433 - mse: 4653.3433 - mae: 51.1019 - val_loss: 5044.4819 - val_mse: 5044.4819 - val_mae: 52.8553\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4652.3286 - mse: 4652.3286 - mae: 51.0957 - val_loss: 5043.3086 - val_mse: 5043.3086 - val_mae: 52.8488\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4651.2739 - mse: 4651.2739 - mae: 51.0894 - val_loss: 5042.1343 - val_mse: 5042.1343 - val_mae: 52.8423\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4650.2061 - mse: 4650.2061 - mae: 51.0831 - val_loss: 5040.9790 - val_mse: 5040.9790 - val_mae: 52.8358\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4649.1792 - mse: 4649.1792 - mae: 51.0769 - val_loss: 5039.7915 - val_mse: 5039.7915 - val_mae: 52.8292\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4648.1426 - mse: 4648.1426 - mae: 51.0707 - val_loss: 5038.5869 - val_mse: 5038.5869 - val_mae: 52.8226\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4647.0488 - mse: 4647.0488 - mae: 51.0643 - val_loss: 5037.4287 - val_mse: 5037.4287 - val_mae: 52.8161\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4646.0371 - mse: 4646.0371 - mae: 51.0581 - val_loss: 5036.2432 - val_mse: 5036.2432 - val_mae: 52.8095\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4644.9663 - mse: 4644.9663 - mae: 51.0518 - val_loss: 5035.0840 - val_mse: 5035.0840 - val_mae: 52.8031\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4643.9434 - mse: 4643.9434 - mae: 51.0456 - val_loss: 5033.9150 - val_mse: 5033.9150 - val_mae: 52.7965\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4642.9028 - mse: 4642.9028 - mae: 51.0394 - val_loss: 5032.7407 - val_mse: 5032.7407 - val_mae: 52.7900\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4641.8691 - mse: 4641.8691 - mae: 51.0331 - val_loss: 5031.5615 - val_mse: 5031.5615 - val_mae: 52.7835\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4640.7852 - mse: 4640.7852 - mae: 51.0268 - val_loss: 5030.4185 - val_mse: 5030.4185 - val_mae: 52.7770\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4639.7607 - mse: 4639.7607 - mae: 51.0206 - val_loss: 5029.2524 - val_mse: 5029.2524 - val_mae: 52.7705\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4638.7104 - mse: 4638.7104 - mae: 51.0143 - val_loss: 5028.0874 - val_mse: 5028.0874 - val_mae: 52.7640\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4637.6841 - mse: 4637.6841 - mae: 51.0081 - val_loss: 5026.9028 - val_mse: 5026.9028 - val_mae: 52.7575\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4636.6494 - mse: 4636.6494 - mae: 51.0018 - val_loss: 5025.7119 - val_mse: 5025.7119 - val_mae: 52.7509\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4635.5562 - mse: 4635.5562 - mae: 50.9955 - val_loss: 5024.5742 - val_mse: 5024.5742 - val_mae: 52.7445\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4634.5547 - mse: 4634.5547 - mae: 50.9894 - val_loss: 5023.3838 - val_mse: 5023.3838 - val_mae: 52.7379\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4633.4912 - mse: 4633.4912 - mae: 50.9831 - val_loss: 5022.2222 - val_mse: 5022.2222 - val_mae: 52.7314\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4632.4668 - mse: 4632.4668 - mae: 50.9768 - val_loss: 5021.0571 - val_mse: 5021.0571 - val_mae: 52.7249\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4631.4214 - mse: 4631.4214 - mae: 50.9706 - val_loss: 5019.8848 - val_mse: 5019.8848 - val_mae: 52.7183\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4630.3584 - mse: 4630.3584 - mae: 50.9643 - val_loss: 5018.7188 - val_mse: 5018.7188 - val_mae: 52.7119\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4629.3076 - mse: 4629.3076 - mae: 50.9581 - val_loss: 5017.5630 - val_mse: 5017.5630 - val_mae: 52.7054\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4628.2676 - mse: 4628.2676 - mae: 50.9518 - val_loss: 5016.3896 - val_mse: 5016.3896 - val_mae: 52.6988\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4627.2305 - mse: 4627.2305 - mae: 50.9456 - val_loss: 5015.2061 - val_mse: 5015.2061 - val_mae: 52.6923\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4626.1982 - mse: 4626.1982 - mae: 50.9393 - val_loss: 5014.0181 - val_mse: 5014.0181 - val_mae: 52.6857\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4625.1401 - mse: 4625.1401 - mae: 50.9330 - val_loss: 5012.8574 - val_mse: 5012.8574 - val_mae: 52.6792\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4624.1279 - mse: 4624.1279 - mae: 50.9268 - val_loss: 5011.6802 - val_mse: 5011.6802 - val_mae: 52.6727\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4623.0513 - mse: 4623.0513 - mae: 50.9206 - val_loss: 5010.5366 - val_mse: 5010.5366 - val_mae: 52.6663\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4622.0420 - mse: 4622.0420 - mae: 50.9144 - val_loss: 5009.3589 - val_mse: 5009.3589 - val_mae: 52.6597\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4620.9614 - mse: 4620.9614 - mae: 50.9080 - val_loss: 5008.2124 - val_mse: 5008.2124 - val_mae: 52.6533\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4619.9727 - mse: 4619.9727 - mae: 50.9019 - val_loss: 5007.0366 - val_mse: 5007.0366 - val_mae: 52.6467\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4618.8901 - mse: 4618.8901 - mae: 50.8957 - val_loss: 5005.9062 - val_mse: 5005.9062 - val_mae: 52.6403\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4617.8481 - mse: 4617.8481 - mae: 50.8895 - val_loss: 5004.7622 - val_mse: 5004.7622 - val_mae: 52.6339\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4616.8613 - mse: 4616.8613 - mae: 50.8833 - val_loss: 5003.5654 - val_mse: 5003.5654 - val_mae: 52.6273\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4615.7832 - mse: 4615.7832 - mae: 50.8769 - val_loss: 5002.4092 - val_mse: 5002.4092 - val_mae: 52.6208\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4614.7363 - mse: 4614.7363 - mae: 50.8708 - val_loss: 5001.2490 - val_mse: 5001.2490 - val_mae: 52.6143\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4613.6978 - mse: 4613.6978 - mae: 50.8645 - val_loss: 5000.0713 - val_mse: 5000.0713 - val_mae: 52.6078\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4612.6685 - mse: 4612.6685 - mae: 50.8583 - val_loss: 4998.8950 - val_mse: 4998.8950 - val_mae: 52.6012\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 23308.0898 - mse: 23308.0898 - mae: 83.9822 - val_loss: 25078.5762 - val_mse: 25078.5762 - val_mae: 86.8555\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23303.8398 - mse: 23303.8398 - mae: 83.9760 - val_loss: 25074.0508 - val_mse: 25074.0508 - val_mae: 86.8492\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23299.6758 - mse: 23299.6758 - mae: 83.9699 - val_loss: 25069.5039 - val_mse: 25069.5039 - val_mae: 86.8429\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23295.4570 - mse: 23295.4570 - mae: 83.9637 - val_loss: 25064.9785 - val_mse: 25064.9785 - val_mae: 86.8365\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23291.2461 - mse: 23291.2461 - mae: 83.9576 - val_loss: 25060.4609 - val_mse: 25060.4609 - val_mae: 86.8302\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23287.0137 - mse: 23287.0137 - mae: 83.9514 - val_loss: 25055.9375 - val_mse: 25055.9375 - val_mae: 86.8239\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23282.8242 - mse: 23282.8242 - mae: 83.9453 - val_loss: 25051.4121 - val_mse: 25051.4121 - val_mae: 86.8176\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23278.6055 - mse: 23278.6055 - mae: 83.9391 - val_loss: 25046.9082 - val_mse: 25046.9082 - val_mae: 86.8113\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23274.4180 - mse: 23274.4180 - mae: 83.9330 - val_loss: 25042.3867 - val_mse: 25042.3867 - val_mae: 86.8050\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23270.2168 - mse: 23270.2168 - mae: 83.9268 - val_loss: 25037.8398 - val_mse: 25037.8398 - val_mae: 86.7986\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23266.0176 - mse: 23266.0176 - mae: 83.9206 - val_loss: 25033.3125 - val_mse: 25033.3125 - val_mae: 86.7923\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23261.8027 - mse: 23261.8027 - mae: 83.9145 - val_loss: 25028.8008 - val_mse: 25028.8008 - val_mae: 86.7860\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23257.6660 - mse: 23257.6660 - mae: 83.9084 - val_loss: 25024.2656 - val_mse: 25024.2656 - val_mae: 86.7796\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23253.4277 - mse: 23253.4277 - mae: 83.9022 - val_loss: 25019.7344 - val_mse: 25019.7344 - val_mae: 86.7733\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23249.2227 - mse: 23249.2227 - mae: 83.8960 - val_loss: 25015.2227 - val_mse: 25015.2227 - val_mae: 86.7670\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23244.9961 - mse: 23244.9980 - mae: 83.8899 - val_loss: 25010.7617 - val_mse: 25010.7617 - val_mae: 86.7607\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23240.8145 - mse: 23240.8145 - mae: 83.8838 - val_loss: 25006.3027 - val_mse: 25006.3027 - val_mae: 86.7545\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23236.6953 - mse: 23236.6953 - mae: 83.8777 - val_loss: 25001.7480 - val_mse: 25001.7480 - val_mae: 86.7481\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23232.5117 - mse: 23232.5117 - mae: 83.8715 - val_loss: 24997.2148 - val_mse: 24997.2148 - val_mae: 86.7418\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23228.2676 - mse: 23228.2676 - mae: 83.8654 - val_loss: 24992.7266 - val_mse: 24992.7266 - val_mae: 86.7355\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23224.1328 - mse: 23224.1328 - mae: 83.8592 - val_loss: 24988.2012 - val_mse: 24988.2012 - val_mae: 86.7292\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23219.9004 - mse: 23219.9004 - mae: 83.8531 - val_loss: 24983.6875 - val_mse: 24983.6875 - val_mae: 86.7229\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23215.6992 - mse: 23215.6992 - mae: 83.8469 - val_loss: 24979.1895 - val_mse: 24979.1895 - val_mae: 86.7166\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23211.5449 - mse: 23211.5449 - mae: 83.8408 - val_loss: 24974.6777 - val_mse: 24974.6777 - val_mae: 86.7102\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23207.3320 - mse: 23207.3320 - mae: 83.8347 - val_loss: 24970.1836 - val_mse: 24970.1836 - val_mae: 86.7039\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23203.1406 - mse: 23203.1406 - mae: 83.8285 - val_loss: 24965.7461 - val_mse: 24965.7461 - val_mae: 86.6977\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23198.9980 - mse: 23198.9980 - mae: 83.8225 - val_loss: 24961.2578 - val_mse: 24961.2578 - val_mae: 86.6914\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23194.8887 - mse: 23194.8887 - mae: 83.8164 - val_loss: 24956.7266 - val_mse: 24956.7266 - val_mae: 86.6851\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23190.6426 - mse: 23190.6426 - mae: 83.8102 - val_loss: 24952.2324 - val_mse: 24952.2324 - val_mae: 86.6788\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23186.4336 - mse: 23186.4336 - mae: 83.8041 - val_loss: 24947.7832 - val_mse: 24947.7832 - val_mae: 86.6726\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23182.3301 - mse: 23182.3301 - mae: 83.7980 - val_loss: 24943.2676 - val_mse: 24943.2676 - val_mae: 86.6662\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23178.1758 - mse: 23178.1758 - mae: 83.7919 - val_loss: 24938.7520 - val_mse: 24938.7520 - val_mae: 86.6599\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23173.9570 - mse: 23173.9570 - mae: 83.7857 - val_loss: 24934.2656 - val_mse: 24934.2656 - val_mae: 86.6536\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23169.7695 - mse: 23169.7695 - mae: 83.7796 - val_loss: 24929.7969 - val_mse: 24929.7969 - val_mae: 86.6474\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23165.6172 - mse: 23165.6172 - mae: 83.7735 - val_loss: 24925.3047 - val_mse: 24925.3047 - val_mae: 86.6411\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23161.4355 - mse: 23161.4355 - mae: 83.7673 - val_loss: 24920.8164 - val_mse: 24920.8164 - val_mae: 86.6348\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23157.1973 - mse: 23157.1973 - mae: 83.7611 - val_loss: 24916.3359 - val_mse: 24916.3359 - val_mae: 86.6285\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23153.0723 - mse: 23153.0723 - mae: 83.7551 - val_loss: 24911.8223 - val_mse: 24911.8223 - val_mae: 86.6222\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23148.9258 - mse: 23148.9258 - mae: 83.7489 - val_loss: 24907.2910 - val_mse: 24907.2910 - val_mae: 86.6158\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23144.7090 - mse: 23144.7090 - mae: 83.7427 - val_loss: 24902.7891 - val_mse: 24902.7891 - val_mae: 86.6095\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23140.5020 - mse: 23140.5020 - mae: 83.7366 - val_loss: 24898.3223 - val_mse: 24898.3223 - val_mae: 86.6033\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23136.3301 - mse: 23136.3301 - mae: 83.7305 - val_loss: 24893.8398 - val_mse: 24893.8398 - val_mae: 86.5970\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23132.1699 - mse: 23132.1699 - mae: 83.7244 - val_loss: 24889.3535 - val_mse: 24889.3535 - val_mae: 86.5907\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23127.9785 - mse: 23127.9785 - mae: 83.7182 - val_loss: 24884.8633 - val_mse: 24884.8633 - val_mae: 86.5844\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23123.8223 - mse: 23123.8223 - mae: 83.7121 - val_loss: 24880.3477 - val_mse: 24880.3477 - val_mae: 86.5780\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23119.6094 - mse: 23119.6094 - mae: 83.7059 - val_loss: 24875.8750 - val_mse: 24875.8750 - val_mae: 86.5718\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23115.4473 - mse: 23115.4473 - mae: 83.6998 - val_loss: 24871.3984 - val_mse: 24871.3984 - val_mae: 86.5655\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23111.3027 - mse: 23111.3027 - mae: 83.6937 - val_loss: 24866.8848 - val_mse: 24866.8848 - val_mae: 86.5591\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23107.1641 - mse: 23107.1641 - mae: 83.6876 - val_loss: 24862.3652 - val_mse: 24862.3652 - val_mae: 86.5528\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23102.9336 - mse: 23102.9336 - mae: 83.6814 - val_loss: 24857.9199 - val_mse: 24857.9199 - val_mae: 86.5466\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23098.7832 - mse: 23098.7832 - mae: 83.6753 - val_loss: 24853.4355 - val_mse: 24853.4355 - val_mae: 86.5403\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23094.5840 - mse: 23094.5840 - mae: 83.6691 - val_loss: 24848.9395 - val_mse: 24848.9395 - val_mae: 86.5340\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23090.4648 - mse: 23090.4648 - mae: 83.6630 - val_loss: 24844.4355 - val_mse: 24844.4355 - val_mae: 86.5276\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23086.2988 - mse: 23086.2988 - mae: 83.6569 - val_loss: 24839.9922 - val_mse: 24839.9922 - val_mae: 86.5214\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23082.1504 - mse: 23082.1504 - mae: 83.6508 - val_loss: 24835.5840 - val_mse: 24835.5840 - val_mae: 86.5152\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23078.0039 - mse: 23078.0039 - mae: 83.6448 - val_loss: 24831.0977 - val_mse: 24831.0977 - val_mae: 86.5089\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23073.8496 - mse: 23073.8496 - mae: 83.6386 - val_loss: 24826.6016 - val_mse: 24826.6016 - val_mae: 86.5026\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23069.6719 - mse: 23069.6719 - mae: 83.6325 - val_loss: 24822.1113 - val_mse: 24822.1113 - val_mae: 86.4963\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23065.4941 - mse: 23065.4941 - mae: 83.6264 - val_loss: 24817.6426 - val_mse: 24817.6426 - val_mae: 86.4900\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23061.3301 - mse: 23061.3301 - mae: 83.6202 - val_loss: 24813.1523 - val_mse: 24813.1523 - val_mae: 86.4837\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23057.1953 - mse: 23057.1953 - mae: 83.6141 - val_loss: 24808.6543 - val_mse: 24808.6543 - val_mae: 86.4774\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23053.0059 - mse: 23053.0059 - mae: 83.6080 - val_loss: 24804.1914 - val_mse: 24804.1914 - val_mae: 86.4711\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23048.8379 - mse: 23048.8379 - mae: 83.6018 - val_loss: 24799.7539 - val_mse: 24799.7539 - val_mae: 86.4649\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23044.7461 - mse: 23044.7461 - mae: 83.5958 - val_loss: 24795.2402 - val_mse: 24795.2402 - val_mae: 86.4585\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23040.5137 - mse: 23040.5137 - mae: 83.5896 - val_loss: 24790.8613 - val_mse: 24790.8613 - val_mae: 86.4524\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23036.4316 - mse: 23036.4316 - mae: 83.5836 - val_loss: 24786.3848 - val_mse: 24786.3848 - val_mae: 86.4461\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23032.2910 - mse: 23032.2910 - mae: 83.5775 - val_loss: 24781.8887 - val_mse: 24781.8887 - val_mae: 86.4398\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23028.0996 - mse: 23028.0996 - mae: 83.5713 - val_loss: 24777.4199 - val_mse: 24777.4199 - val_mae: 86.4335\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23023.9512 - mse: 23023.9512 - mae: 83.5652 - val_loss: 24772.9238 - val_mse: 24772.9238 - val_mae: 86.4272\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 23019.7637 - mse: 23019.7637 - mae: 83.5591 - val_loss: 24768.4531 - val_mse: 24768.4531 - val_mae: 86.4209\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23015.6426 - mse: 23015.6406 - mae: 83.5529 - val_loss: 24763.9531 - val_mse: 24763.9531 - val_mae: 86.4146\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23011.4297 - mse: 23011.4297 - mae: 83.5468 - val_loss: 24759.5039 - val_mse: 24759.5039 - val_mae: 86.4083\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23007.3281 - mse: 23007.3281 - mae: 83.5407 - val_loss: 24755.0293 - val_mse: 24755.0293 - val_mae: 86.4020\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23003.1289 - mse: 23003.1289 - mae: 83.5345 - val_loss: 24750.5918 - val_mse: 24750.5918 - val_mae: 86.3958\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 22998.9824 - mse: 22998.9824 - mae: 83.5284 - val_loss: 24746.1035 - val_mse: 24746.1035 - val_mae: 86.3895\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22994.8457 - mse: 22994.8457 - mae: 83.5223 - val_loss: 24741.6211 - val_mse: 24741.6211 - val_mae: 86.3832\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22990.6367 - mse: 22990.6367 - mae: 83.5161 - val_loss: 24737.1641 - val_mse: 24737.1641 - val_mae: 86.3769\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22986.4844 - mse: 22986.4844 - mae: 83.5100 - val_loss: 24732.7012 - val_mse: 24732.7012 - val_mae: 86.3706\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 22982.3359 - mse: 22982.3359 - mae: 83.5039 - val_loss: 24728.2324 - val_mse: 24728.2324 - val_mae: 86.3643\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 22978.1973 - mse: 22978.1973 - mae: 83.4978 - val_loss: 24723.7461 - val_mse: 24723.7461 - val_mae: 86.3580\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 22974.0430 - mse: 22974.0430 - mae: 83.4917 - val_loss: 24719.2539 - val_mse: 24719.2539 - val_mae: 86.3517\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 22969.8516 - mse: 22969.8516 - mae: 83.4855 - val_loss: 24714.8047 - val_mse: 24714.8047 - val_mae: 86.3454\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22965.7070 - mse: 22965.7070 - mae: 83.4794 - val_loss: 24710.3652 - val_mse: 24710.3652 - val_mae: 86.3392\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22961.5469 - mse: 22961.5469 - mae: 83.4733 - val_loss: 24705.9355 - val_mse: 24705.9355 - val_mae: 86.3329\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22957.3945 - mse: 22957.3945 - mae: 83.4672 - val_loss: 24701.5410 - val_mse: 24701.5410 - val_mae: 86.3268\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22953.2871 - mse: 22953.2871 - mae: 83.4611 - val_loss: 24697.0859 - val_mse: 24697.0859 - val_mae: 86.3205\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22949.1875 - mse: 22949.1875 - mae: 83.4550 - val_loss: 24692.5742 - val_mse: 24692.5742 - val_mae: 86.3141\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 22944.9902 - mse: 22944.9902 - mae: 83.4488 - val_loss: 24688.1113 - val_mse: 24688.1113 - val_mae: 86.3078\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22940.8203 - mse: 22940.8203 - mae: 83.4427 - val_loss: 24683.6543 - val_mse: 24683.6543 - val_mae: 86.3016\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22936.6836 - mse: 22936.6836 - mae: 83.4366 - val_loss: 24679.1875 - val_mse: 24679.1875 - val_mae: 86.2953\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22932.5332 - mse: 22932.5332 - mae: 83.4305 - val_loss: 24674.7383 - val_mse: 24674.7383 - val_mae: 86.2890\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22928.3984 - mse: 22928.3984 - mae: 83.4244 - val_loss: 24670.2891 - val_mse: 24670.2891 - val_mae: 86.2827\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22924.2715 - mse: 22924.2715 - mae: 83.4183 - val_loss: 24665.8418 - val_mse: 24665.8418 - val_mae: 86.2765\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22920.0859 - mse: 22920.0859 - mae: 83.4121 - val_loss: 24661.4473 - val_mse: 24661.4473 - val_mae: 86.2703\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 22916.0215 - mse: 22916.0215 - mae: 83.4061 - val_loss: 24656.9629 - val_mse: 24656.9629 - val_mae: 86.2640\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 22911.8477 - mse: 22911.8477 - mae: 83.3999 - val_loss: 24652.4902 - val_mse: 24652.4902 - val_mae: 86.2577\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 22907.7070 - mse: 22907.7070 - mae: 83.3938 - val_loss: 24648.0469 - val_mse: 24648.0469 - val_mae: 86.2514\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 22903.5273 - mse: 22903.5273 - mae: 83.3877 - val_loss: 24643.6211 - val_mse: 24643.6211 - val_mae: 86.2452\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22899.4492 - mse: 22899.4492 - mae: 83.3816 - val_loss: 24639.1543 - val_mse: 24639.1543 - val_mae: 86.2389\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22895.2676 - mse: 22895.2676 - mae: 83.3755 - val_loss: 24634.7617 - val_mse: 24634.7617 - val_mae: 86.2327\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: 1973.0255 - mse: 1973.0255 - mae: 29.8279 - val_loss: 384.5418 - val_mse: 384.5418 - val_mae: 16.8561\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 436.5167 - mse: 436.5167 - mae: 17.7469 - val_loss: 316.4458 - val_mse: 316.4458 - val_mae: 15.2068\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 405.7023 - mse: 405.7023 - mae: 16.9393 - val_loss: 302.6162 - val_mse: 302.6162 - val_mae: 14.9251\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 393.4286 - mse: 393.4286 - mae: 16.6019 - val_loss: 284.9637 - val_mse: 284.9637 - val_mae: 14.3192\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 375.7917 - mse: 375.7917 - mae: 16.0538 - val_loss: 279.5791 - val_mse: 279.5791 - val_mae: 14.1257\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 371.5194 - mse: 371.5194 - mae: 15.9165 - val_loss: 273.9858 - val_mse: 273.9858 - val_mae: 13.9629\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 368.1953 - mse: 368.1953 - mae: 15.8073 - val_loss: 270.8639 - val_mse: 270.8639 - val_mae: 13.8696\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 365.7526 - mse: 365.7526 - mae: 15.7334 - val_loss: 269.6740 - val_mse: 269.6740 - val_mae: 13.8320\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 362.7431 - mse: 362.7431 - mae: 15.6340 - val_loss: 268.0225 - val_mse: 268.0225 - val_mae: 13.7587\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 360.5713 - mse: 360.5713 - mae: 15.5600 - val_loss: 265.0558 - val_mse: 265.0558 - val_mae: 13.6683\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 358.8296 - mse: 358.8296 - mae: 15.5046 - val_loss: 263.3455 - val_mse: 263.3455 - val_mae: 13.5976\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 356.9608 - mse: 356.9608 - mae: 15.4339 - val_loss: 260.9055 - val_mse: 260.9055 - val_mae: 13.4976\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 355.3258 - mse: 355.3258 - mae: 15.3693 - val_loss: 261.5327 - val_mse: 261.5327 - val_mae: 13.5673\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 354.1987 - mse: 354.1987 - mae: 15.3612 - val_loss: 258.3531 - val_mse: 258.3531 - val_mae: 13.4257\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 351.9564 - mse: 351.9564 - mae: 15.2802 - val_loss: 256.0564 - val_mse: 256.0564 - val_mae: 13.3164\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 349.4768 - mse: 349.4768 - mae: 15.1989 - val_loss: 253.4926 - val_mse: 253.4926 - val_mae: 13.2483\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 344.2245 - mse: 344.2245 - mae: 15.0407 - val_loss: 247.2618 - val_mse: 247.2618 - val_mae: 13.0288\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 336.4671 - mse: 336.4671 - mae: 14.7922 - val_loss: 245.1693 - val_mse: 245.1693 - val_mae: 12.9153\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 332.5999 - mse: 332.5999 - mae: 14.6513 - val_loss: 244.0564 - val_mse: 244.0564 - val_mae: 12.8902\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 330.3392 - mse: 330.3392 - mae: 14.6121 - val_loss: 241.5894 - val_mse: 241.5894 - val_mae: 12.8118\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 327.9290 - mse: 327.9290 - mae: 14.5630 - val_loss: 238.2356 - val_mse: 238.2356 - val_mae: 12.6926\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 326.3144 - mse: 326.3144 - mae: 14.4879 - val_loss: 234.8080 - val_mse: 234.8080 - val_mae: 12.5380\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 324.0465 - mse: 324.0465 - mae: 14.3944 - val_loss: 234.7618 - val_mse: 234.7618 - val_mae: 12.5750\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 322.5082 - mse: 322.5082 - mae: 14.3664 - val_loss: 233.1852 - val_mse: 233.1852 - val_mae: 12.4999\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 321.3166 - mse: 321.3166 - mae: 14.3644 - val_loss: 230.5731 - val_mse: 230.5731 - val_mae: 12.3813\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 319.6472 - mse: 319.6472 - mae: 14.2822 - val_loss: 230.3202 - val_mse: 230.3202 - val_mae: 12.4036\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 318.9504 - mse: 318.9504 - mae: 14.2785 - val_loss: 228.8135 - val_mse: 228.8135 - val_mae: 12.3297\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 318.1299 - mse: 318.1299 - mae: 14.2423 - val_loss: 227.0274 - val_mse: 227.0274 - val_mae: 12.2391\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 317.2526 - mse: 317.2526 - mae: 14.1983 - val_loss: 227.7264 - val_mse: 227.7264 - val_mae: 12.3003\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 316.2021 - mse: 316.2021 - mae: 14.2203 - val_loss: 225.4507 - val_mse: 225.4507 - val_mae: 12.1626\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 315.6103 - mse: 315.6103 - mae: 14.1620 - val_loss: 223.9714 - val_mse: 223.9714 - val_mae: 12.0807\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 315.0084 - mse: 315.0084 - mae: 14.0936 - val_loss: 225.9972 - val_mse: 225.9972 - val_mae: 12.2698\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 314.5077 - mse: 314.5077 - mae: 14.1615 - val_loss: 224.7136 - val_mse: 224.7136 - val_mae: 12.1635\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 313.6528 - mse: 313.6528 - mae: 14.1035 - val_loss: 224.4955 - val_mse: 224.4955 - val_mae: 12.1754\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 313.1494 - mse: 313.1494 - mae: 14.0783 - val_loss: 224.4066 - val_mse: 224.4066 - val_mae: 12.2044\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 313.0670 - mse: 313.0670 - mae: 14.1126 - val_loss: 222.4787 - val_mse: 222.4787 - val_mae: 12.0683\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 312.0853 - mse: 312.0853 - mae: 14.0530 - val_loss: 221.6193 - val_mse: 221.6193 - val_mae: 11.9936\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 312.0660 - mse: 312.0660 - mae: 14.0501 - val_loss: 220.6645 - val_mse: 220.6645 - val_mae: 11.9156\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 311.3892 - mse: 311.3892 - mae: 13.9833 - val_loss: 222.2809 - val_mse: 222.2809 - val_mae: 12.0999\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 311.1976 - mse: 311.1976 - mae: 14.0157 - val_loss: 222.3611 - val_mse: 222.3611 - val_mae: 12.1121\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 310.6128 - mse: 310.6128 - mae: 14.0585 - val_loss: 219.9949 - val_mse: 219.9949 - val_mae: 11.9290\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 309.9731 - mse: 309.9731 - mae: 13.9853 - val_loss: 219.8041 - val_mse: 219.8041 - val_mae: 11.9238\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 309.9766 - mse: 309.9766 - mae: 13.9851 - val_loss: 219.2385 - val_mse: 219.2385 - val_mae: 11.8883\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 309.2644 - mse: 309.2644 - mae: 13.9548 - val_loss: 219.1804 - val_mse: 219.1804 - val_mae: 11.9031\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 309.0798 - mse: 309.0798 - mae: 13.9844 - val_loss: 218.1510 - val_mse: 218.1510 - val_mae: 11.8424\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 308.5773 - mse: 308.5773 - mae: 13.9254 - val_loss: 217.8869 - val_mse: 217.8869 - val_mae: 11.8287\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 309.0494 - mse: 309.0494 - mae: 13.9161 - val_loss: 217.9367 - val_mse: 217.9367 - val_mae: 11.8440\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 308.0694 - mse: 308.0694 - mae: 13.9044 - val_loss: 218.0980 - val_mse: 218.0980 - val_mae: 11.8839\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 307.6899 - mse: 307.6899 - mae: 13.8878 - val_loss: 219.2125 - val_mse: 219.2125 - val_mae: 11.9936\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 307.4660 - mse: 307.4660 - mae: 13.9098 - val_loss: 217.5364 - val_mse: 217.5364 - val_mae: 11.8524\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 307.0534 - mse: 307.0534 - mae: 13.8619 - val_loss: 217.7580 - val_mse: 217.7580 - val_mae: 11.8809\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 307.0931 - mse: 307.0931 - mae: 13.8996 - val_loss: 217.3601 - val_mse: 217.3601 - val_mae: 11.8718\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 306.5593 - mse: 306.5593 - mae: 13.8569 - val_loss: 217.1295 - val_mse: 217.1295 - val_mae: 11.8618\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 306.3614 - mse: 306.3614 - mae: 13.9095 - val_loss: 216.0919 - val_mse: 216.0919 - val_mae: 11.6593\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 306.4108 - mse: 306.4108 - mae: 13.7802 - val_loss: 216.4038 - val_mse: 216.4038 - val_mae: 11.8116\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 305.5808 - mse: 305.5808 - mae: 13.8285 - val_loss: 215.6986 - val_mse: 215.6986 - val_mae: 11.7363\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 305.4753 - mse: 305.4753 - mae: 13.7856 - val_loss: 215.7674 - val_mse: 215.7674 - val_mae: 11.7484\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 305.3407 - mse: 305.3407 - mae: 13.7864 - val_loss: 216.4442 - val_mse: 216.4442 - val_mae: 11.8244\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 305.1255 - mse: 305.1255 - mae: 13.8253 - val_loss: 214.8967 - val_mse: 214.8967 - val_mae: 11.6702\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 304.8534 - mse: 304.8534 - mae: 13.7647 - val_loss: 215.0055 - val_mse: 215.0055 - val_mae: 11.7054\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 305.4622 - mse: 305.4622 - mae: 13.7685 - val_loss: 215.0662 - val_mse: 215.0662 - val_mae: 11.7377\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 304.1328 - mse: 304.1328 - mae: 13.7857 - val_loss: 214.1873 - val_mse: 214.1873 - val_mae: 11.6044\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 304.1541 - mse: 304.1541 - mae: 13.7169 - val_loss: 214.2122 - val_mse: 214.2122 - val_mae: 11.6555\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 304.1436 - mse: 304.1436 - mae: 13.7263 - val_loss: 214.8937 - val_mse: 214.8937 - val_mae: 11.7556\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 303.9094 - mse: 303.9094 - mae: 13.7556 - val_loss: 213.9063 - val_mse: 213.9063 - val_mae: 11.6149\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 303.6991 - mse: 303.6991 - mae: 13.6989 - val_loss: 213.9562 - val_mse: 213.9562 - val_mae: 11.6834\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 303.4216 - mse: 303.4216 - mae: 13.7028 - val_loss: 214.2424 - val_mse: 214.2424 - val_mae: 11.7294\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 303.0296 - mse: 303.0296 - mae: 13.7191 - val_loss: 213.9807 - val_mse: 213.9807 - val_mae: 11.7106\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 303.2141 - mse: 303.2141 - mae: 13.7553 - val_loss: 213.0425 - val_mse: 213.0425 - val_mae: 11.5711\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 302.6170 - mse: 302.6170 - mae: 13.6650 - val_loss: 212.9904 - val_mse: 212.9904 - val_mae: 11.6039\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 302.6538 - mse: 302.6538 - mae: 13.7130 - val_loss: 212.2833 - val_mse: 212.2833 - val_mae: 11.5019\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 302.5527 - mse: 302.5527 - mae: 13.5999 - val_loss: 213.3277 - val_mse: 213.3277 - val_mae: 11.6664\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 302.1491 - mse: 302.1491 - mae: 13.6936 - val_loss: 212.3311 - val_mse: 212.3311 - val_mae: 11.5348\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 302.1121 - mse: 302.1121 - mae: 13.6249 - val_loss: 213.8341 - val_mse: 213.8341 - val_mae: 11.7248\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 302.2097 - mse: 302.2097 - mae: 13.6850 - val_loss: 212.5739 - val_mse: 212.5739 - val_mae: 11.5807\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 301.8889 - mse: 301.8889 - mae: 13.6281 - val_loss: 212.1765 - val_mse: 212.1765 - val_mae: 11.5608\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 301.7395 - mse: 301.7395 - mae: 13.6414 - val_loss: 212.1212 - val_mse: 212.1212 - val_mae: 11.5590\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 301.1339 - mse: 301.1339 - mae: 13.6274 - val_loss: 211.9625 - val_mse: 211.9625 - val_mae: 11.5345\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 301.2275 - mse: 301.2275 - mae: 13.6133 - val_loss: 211.9931 - val_mse: 211.9931 - val_mae: 11.5383\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 301.5847 - mse: 301.5847 - mae: 13.6343 - val_loss: 212.2866 - val_mse: 212.2866 - val_mae: 11.6144\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 300.9963 - mse: 300.9963 - mae: 13.6571 - val_loss: 211.2518 - val_mse: 211.2518 - val_mae: 11.4620\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 300.7385 - mse: 300.7385 - mae: 13.5454 - val_loss: 211.9989 - val_mse: 211.9989 - val_mae: 11.6005\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 300.4726 - mse: 300.4726 - mae: 13.6045 - val_loss: 211.4492 - val_mse: 211.4492 - val_mae: 11.5260\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 300.3689 - mse: 300.3689 - mae: 13.5720 - val_loss: 211.9625 - val_mse: 211.9625 - val_mae: 11.5676\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 300.2936 - mse: 300.2936 - mae: 13.5448 - val_loss: 212.0194 - val_mse: 212.0194 - val_mae: 11.6164\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 300.3031 - mse: 300.3031 - mae: 13.5822 - val_loss: 212.4237 - val_mse: 212.4237 - val_mae: 11.6524\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 299.9978 - mse: 299.9978 - mae: 13.6138 - val_loss: 210.7639 - val_mse: 210.7639 - val_mae: 11.5000\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 300.1017 - mse: 300.1017 - mae: 13.5543 - val_loss: 211.3294 - val_mse: 211.3294 - val_mae: 11.5750\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 300.1829 - mse: 300.1829 - mae: 13.5679 - val_loss: 210.8742 - val_mse: 210.8742 - val_mae: 11.5461\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 299.6994 - mse: 299.6994 - mae: 13.6034 - val_loss: 209.9763 - val_mse: 209.9763 - val_mae: 11.4214\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 299.7329 - mse: 299.7329 - mae: 13.5113 - val_loss: 210.3533 - val_mse: 210.3533 - val_mae: 11.4987\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 299.7076 - mse: 299.7076 - mae: 13.5715 - val_loss: 209.3491 - val_mse: 209.3491 - val_mae: 11.3882\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 299.4386 - mse: 299.4386 - mae: 13.5031 - val_loss: 209.4971 - val_mse: 209.4971 - val_mae: 11.4404\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 298.9164 - mse: 298.9164 - mae: 13.4778 - val_loss: 210.4742 - val_mse: 210.4742 - val_mae: 11.5494\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 298.8036 - mse: 298.8036 - mae: 13.5345 - val_loss: 209.7248 - val_mse: 209.7248 - val_mae: 11.4662\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 298.7118 - mse: 298.7118 - mae: 13.5086 - val_loss: 209.4414 - val_mse: 209.4414 - val_mae: 11.4325\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 298.7080 - mse: 298.7080 - mae: 13.4792 - val_loss: 210.1877 - val_mse: 210.1877 - val_mae: 11.5307\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 298.3690 - mse: 298.3690 - mae: 13.5399 - val_loss: 209.0822 - val_mse: 209.0822 - val_mae: 11.3994\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 298.3142 - mse: 298.3142 - mae: 13.4885 - val_loss: 209.2894 - val_mse: 209.2894 - val_mae: 11.4547\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 298.1875 - mse: 298.1875 - mae: 13.4564 - val_loss: 210.0139 - val_mse: 210.0139 - val_mae: 11.5285\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: 1351.6404 - mse: 1351.6404 - mae: 27.5282 - val_loss: 552.2985 - val_mse: 552.2985 - val_mae: 19.2801\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 582.2559 - mse: 582.2559 - mae: 19.9752 - val_loss: 406.3879 - val_mse: 406.3879 - val_mae: 16.9297\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 513.4050 - mse: 513.4050 - mae: 18.9467 - val_loss: 363.4505 - val_mse: 363.4505 - val_mae: 16.3070\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 488.2444 - mse: 488.2444 - mae: 18.5639 - val_loss: 342.6401 - val_mse: 342.6401 - val_mae: 15.9236\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 474.4060 - mse: 474.4060 - mae: 18.2944 - val_loss: 329.3127 - val_mse: 329.3127 - val_mae: 15.6813\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 463.9218 - mse: 463.9218 - mae: 18.0791 - val_loss: 319.1082 - val_mse: 319.1082 - val_mae: 15.4446\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 455.2561 - mse: 455.2561 - mae: 17.8553 - val_loss: 311.2527 - val_mse: 311.2527 - val_mae: 15.2282\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 448.8419 - mse: 448.8419 - mae: 17.6821 - val_loss: 305.4656 - val_mse: 305.4656 - val_mae: 14.9928\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 443.1115 - mse: 443.1115 - mae: 17.5178 - val_loss: 302.2423 - val_mse: 302.2423 - val_mae: 14.8387\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 440.1854 - mse: 440.1854 - mae: 17.4418 - val_loss: 300.3932 - val_mse: 300.3932 - val_mae: 14.7861\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 437.9632 - mse: 437.9632 - mae: 17.3977 - val_loss: 298.1181 - val_mse: 298.1181 - val_mae: 14.6885\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 435.9164 - mse: 435.9164 - mae: 17.3324 - val_loss: 296.7040 - val_mse: 296.7040 - val_mae: 14.6557\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 433.6454 - mse: 433.6454 - mae: 17.3054 - val_loss: 293.9194 - val_mse: 293.9194 - val_mae: 14.5987\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 430.4245 - mse: 430.4245 - mae: 17.2478 - val_loss: 289.8227 - val_mse: 289.8227 - val_mae: 14.4926\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 425.4990 - mse: 425.4990 - mae: 17.1156 - val_loss: 286.3229 - val_mse: 286.3229 - val_mae: 14.3720\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.6309 - mse: 420.6309 - mae: 16.9805 - val_loss: 282.2047 - val_mse: 282.2047 - val_mae: 14.2082\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 416.6768 - mse: 416.6768 - mae: 16.8235 - val_loss: 280.0774 - val_mse: 280.0774 - val_mae: 14.0925\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 414.1999 - mse: 414.1999 - mae: 16.7403 - val_loss: 278.5872 - val_mse: 278.5872 - val_mae: 14.0266\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 412.7185 - mse: 412.7185 - mae: 16.6691 - val_loss: 277.8972 - val_mse: 277.8972 - val_mae: 13.9973\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 411.8799 - mse: 411.8799 - mae: 16.6461 - val_loss: 277.0791 - val_mse: 277.0791 - val_mae: 13.9540\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 411.1172 - mse: 411.1172 - mae: 16.6251 - val_loss: 276.0174 - val_mse: 276.0174 - val_mae: 13.9256\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 410.7485 - mse: 410.7485 - mae: 16.5817 - val_loss: 275.6046 - val_mse: 275.6046 - val_mae: 13.8964\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 410.1149 - mse: 410.1149 - mae: 16.5528 - val_loss: 275.5423 - val_mse: 275.5423 - val_mae: 13.8886\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 409.9398 - mse: 409.9398 - mae: 16.5588 - val_loss: 275.2979 - val_mse: 275.2979 - val_mae: 13.8772\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 409.4497 - mse: 409.4497 - mae: 16.5493 - val_loss: 274.6573 - val_mse: 274.6573 - val_mae: 13.8350\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 409.1904 - mse: 409.1904 - mae: 16.5189 - val_loss: 274.6748 - val_mse: 274.6748 - val_mae: 13.8243\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 408.6639 - mse: 408.6639 - mae: 16.4967 - val_loss: 273.5211 - val_mse: 273.5211 - val_mae: 13.7864\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 407.5335 - mse: 407.5335 - mae: 16.4574 - val_loss: 272.9871 - val_mse: 272.9871 - val_mae: 13.7553\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.5775 - mse: 406.5775 - mae: 16.4250 - val_loss: 272.7183 - val_mse: 272.7183 - val_mae: 13.7407\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.0768 - mse: 406.0768 - mae: 16.4039 - val_loss: 272.5966 - val_mse: 272.5966 - val_mae: 13.7288\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 405.8416 - mse: 405.8416 - mae: 16.4087 - val_loss: 271.7388 - val_mse: 271.7388 - val_mae: 13.6859\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 405.4919 - mse: 405.4919 - mae: 16.3696 - val_loss: 272.3648 - val_mse: 272.3648 - val_mae: 13.7251\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.8293 - mse: 404.8293 - mae: 16.3937 - val_loss: 271.7211 - val_mse: 271.7211 - val_mae: 13.6967\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 404.4796 - mse: 404.4796 - mae: 16.3710 - val_loss: 271.5142 - val_mse: 271.5142 - val_mae: 13.6917\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.0338 - mse: 404.0338 - mae: 16.3610 - val_loss: 271.1190 - val_mse: 271.1190 - val_mae: 13.6823\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 403.6578 - mse: 403.6578 - mae: 16.3712 - val_loss: 270.4513 - val_mse: 270.4513 - val_mae: 13.6505\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 403.3495 - mse: 403.3495 - mae: 16.3478 - val_loss: 269.9299 - val_mse: 269.9299 - val_mae: 13.6248\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 402.7967 - mse: 402.7967 - mae: 16.3343 - val_loss: 269.3170 - val_mse: 269.3170 - val_mae: 13.6043\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 402.0726 - mse: 402.0726 - mae: 16.3322 - val_loss: 268.6382 - val_mse: 268.6382 - val_mae: 13.5935\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 400.8825 - mse: 400.8825 - mae: 16.2793 - val_loss: 268.4752 - val_mse: 268.4752 - val_mae: 13.6012\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 398.3419 - mse: 398.3419 - mae: 16.2788 - val_loss: 266.9265 - val_mse: 266.9265 - val_mae: 13.5209\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 394.3500 - mse: 394.3500 - mae: 16.1652 - val_loss: 263.9177 - val_mse: 263.9177 - val_mae: 13.4311\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 391.1026 - mse: 391.1026 - mae: 16.0586 - val_loss: 261.3616 - val_mse: 261.3616 - val_mae: 13.3924\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 388.4892 - mse: 388.4892 - mae: 16.0078 - val_loss: 259.1548 - val_mse: 259.1548 - val_mae: 13.3252\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 386.6075 - mse: 386.6075 - mae: 16.0036 - val_loss: 256.6938 - val_mse: 256.6938 - val_mae: 13.2093\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 384.9229 - mse: 384.9229 - mae: 15.8699 - val_loss: 255.5217 - val_mse: 255.5217 - val_mae: 13.1721\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 383.4927 - mse: 383.4927 - mae: 15.8363 - val_loss: 254.7213 - val_mse: 254.7213 - val_mae: 13.1304\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 382.4352 - mse: 382.4352 - mae: 15.8205 - val_loss: 253.8076 - val_mse: 253.8076 - val_mae: 13.1036\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 380.8380 - mse: 380.8380 - mae: 15.7549 - val_loss: 254.5392 - val_mse: 254.5392 - val_mae: 13.1598\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 379.2611 - mse: 379.2611 - mae: 15.7691 - val_loss: 250.9905 - val_mse: 250.9905 - val_mae: 13.0177\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 377.3327 - mse: 377.3327 - mae: 15.6753 - val_loss: 249.7500 - val_mse: 249.7500 - val_mae: 12.9818\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 375.9374 - mse: 375.9374 - mae: 15.6370 - val_loss: 248.7702 - val_mse: 248.7702 - val_mae: 12.9557\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 374.0268 - mse: 374.0268 - mae: 15.5820 - val_loss: 247.6293 - val_mse: 247.6293 - val_mae: 12.9097\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 373.0781 - mse: 373.0781 - mae: 15.5682 - val_loss: 245.8898 - val_mse: 245.8898 - val_mae: 12.8451\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 371.9756 - mse: 371.9756 - mae: 15.5029 - val_loss: 246.1296 - val_mse: 246.1296 - val_mae: 12.8524\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 371.3509 - mse: 371.3509 - mae: 15.5219 - val_loss: 245.0181 - val_mse: 245.0181 - val_mae: 12.7940\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 370.3182 - mse: 370.3182 - mae: 15.4618 - val_loss: 244.8177 - val_mse: 244.8177 - val_mae: 12.7823\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 369.5504 - mse: 369.5504 - mae: 15.4530 - val_loss: 244.0133 - val_mse: 244.0133 - val_mae: 12.7343\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 368.9980 - mse: 368.9980 - mae: 15.4158 - val_loss: 243.7282 - val_mse: 243.7282 - val_mae: 12.7372\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 368.5178 - mse: 368.5178 - mae: 15.4091 - val_loss: 243.3551 - val_mse: 243.3551 - val_mae: 12.6936\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 368.2117 - mse: 368.2117 - mae: 15.4081 - val_loss: 242.3172 - val_mse: 242.3172 - val_mae: 12.6409\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 367.3706 - mse: 367.3706 - mae: 15.3743 - val_loss: 241.3481 - val_mse: 241.3481 - val_mae: 12.5971\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 367.0088 - mse: 367.0088 - mae: 15.3522 - val_loss: 241.0594 - val_mse: 241.0594 - val_mae: 12.5893\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 366.6938 - mse: 366.6938 - mae: 15.3371 - val_loss: 240.8235 - val_mse: 240.8235 - val_mae: 12.5724\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 366.5915 - mse: 366.5915 - mae: 15.2941 - val_loss: 241.3739 - val_mse: 241.3739 - val_mae: 12.5793\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 366.0240 - mse: 366.0240 - mae: 15.3155 - val_loss: 241.5518 - val_mse: 241.5518 - val_mae: 12.5976\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 365.7728 - mse: 365.7728 - mae: 15.3322 - val_loss: 240.3070 - val_mse: 240.3070 - val_mae: 12.5241\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 365.6686 - mse: 365.6686 - mae: 15.3021 - val_loss: 240.5011 - val_mse: 240.5011 - val_mae: 12.5311\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 365.5174 - mse: 365.5174 - mae: 15.3098 - val_loss: 240.3246 - val_mse: 240.3246 - val_mae: 12.5034\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 365.1466 - mse: 365.1466 - mae: 15.2692 - val_loss: 240.0923 - val_mse: 240.0923 - val_mae: 12.5118\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 364.9249 - mse: 364.9249 - mae: 15.3055 - val_loss: 238.9170 - val_mse: 238.9170 - val_mae: 12.4447\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 365.0636 - mse: 365.0636 - mae: 15.2514 - val_loss: 239.4257 - val_mse: 239.4257 - val_mae: 12.4633\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 364.6949 - mse: 364.6949 - mae: 15.2612 - val_loss: 239.5690 - val_mse: 239.5690 - val_mae: 12.4734\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 364.3825 - mse: 364.3825 - mae: 15.2479 - val_loss: 240.3081 - val_mse: 240.3081 - val_mae: 12.5088\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 364.4778 - mse: 364.4778 - mae: 15.2833 - val_loss: 239.5529 - val_mse: 239.5529 - val_mae: 12.4493\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 364.4008 - mse: 364.4008 - mae: 15.2392 - val_loss: 239.5290 - val_mse: 239.5290 - val_mae: 12.4520\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 363.9873 - mse: 363.9873 - mae: 15.2474 - val_loss: 239.7536 - val_mse: 239.7536 - val_mae: 12.4640\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 363.7810 - mse: 363.7810 - mae: 15.2866 - val_loss: 238.0577 - val_mse: 238.0577 - val_mae: 12.3720\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 363.7872 - mse: 363.7872 - mae: 15.2170 - val_loss: 238.4493 - val_mse: 238.4493 - val_mae: 12.3885\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 363.7133 - mse: 363.7133 - mae: 15.2308 - val_loss: 238.5693 - val_mse: 238.5693 - val_mae: 12.3908\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 363.6048 - mse: 363.6048 - mae: 15.2395 - val_loss: 237.7437 - val_mse: 237.7437 - val_mae: 12.3516\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 363.4097 - mse: 363.4097 - mae: 15.1904 - val_loss: 238.7913 - val_mse: 238.7913 - val_mae: 12.4016\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 363.3317 - mse: 363.3317 - mae: 15.2129 - val_loss: 239.2073 - val_mse: 239.2073 - val_mae: 12.4153\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 363.1960 - mse: 363.1960 - mae: 15.2258 - val_loss: 238.6127 - val_mse: 238.6127 - val_mae: 12.3843\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 363.0968 - mse: 363.0968 - mae: 15.1843 - val_loss: 239.2657 - val_mse: 239.2657 - val_mae: 12.4208\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 363.0085 - mse: 363.0085 - mae: 15.2241 - val_loss: 238.4174 - val_mse: 238.4174 - val_mae: 12.3708\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 362.7842 - mse: 362.7842 - mae: 15.1897 - val_loss: 238.4314 - val_mse: 238.4314 - val_mae: 12.3839\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 362.8526 - mse: 362.8526 - mae: 15.1984 - val_loss: 239.0185 - val_mse: 239.0185 - val_mae: 12.4071\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 362.7357 - mse: 362.7357 - mae: 15.2154 - val_loss: 237.8213 - val_mse: 237.8213 - val_mae: 12.3435\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 362.4808 - mse: 362.4808 - mae: 15.2070 - val_loss: 236.8674 - val_mse: 236.8674 - val_mae: 12.3000\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 362.6505 - mse: 362.6505 - mae: 15.1575 - val_loss: 237.4863 - val_mse: 237.4863 - val_mae: 12.3245\n",
            "Epoch 91: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 1346.6929 - mse: 1346.6929 - mae: 25.3239 - val_loss: 363.6078 - val_mse: 363.6078 - val_mae: 16.2174\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 360.3062 - mse: 360.3062 - mae: 15.9633 - val_loss: 311.2430 - val_mse: 311.2430 - val_mae: 14.5838\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 329.7295 - mse: 329.7295 - mae: 14.9830 - val_loss: 300.1657 - val_mse: 300.1657 - val_mae: 14.1666\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 318.3779 - mse: 318.3779 - mae: 14.5805 - val_loss: 294.2498 - val_mse: 294.2498 - val_mae: 13.9631\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 312.2714 - mse: 312.2714 - mae: 14.3938 - val_loss: 290.9293 - val_mse: 290.9293 - val_mae: 13.8309\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 307.9046 - mse: 307.9046 - mae: 14.2292 - val_loss: 287.8419 - val_mse: 287.8419 - val_mae: 13.7131\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 304.3167 - mse: 304.3167 - mae: 14.1033 - val_loss: 285.2032 - val_mse: 285.2032 - val_mae: 13.6434\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 300.8568 - mse: 300.8568 - mae: 14.0169 - val_loss: 282.2259 - val_mse: 282.2259 - val_mae: 13.4875\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 297.4694 - mse: 297.4694 - mae: 13.8565 - val_loss: 279.6812 - val_mse: 279.6812 - val_mae: 13.3996\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 294.2526 - mse: 294.2526 - mae: 13.7629 - val_loss: 277.6427 - val_mse: 277.6427 - val_mae: 13.3001\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 291.6992 - mse: 291.6992 - mae: 13.6471 - val_loss: 275.7691 - val_mse: 275.7691 - val_mae: 13.2350\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 289.9102 - mse: 289.9102 - mae: 13.5905 - val_loss: 273.9984 - val_mse: 273.9984 - val_mae: 13.1328\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 288.0158 - mse: 288.0158 - mae: 13.4936 - val_loss: 273.0187 - val_mse: 273.0187 - val_mae: 13.0873\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 286.5091 - mse: 286.5091 - mae: 13.4250 - val_loss: 272.1483 - val_mse: 272.1483 - val_mae: 13.0553\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 285.3395 - mse: 285.3395 - mae: 13.3975 - val_loss: 271.0242 - val_mse: 271.0242 - val_mae: 12.9900\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 284.1922 - mse: 284.1922 - mae: 13.3271 - val_loss: 270.2817 - val_mse: 270.2817 - val_mae: 12.9681\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 283.0531 - mse: 283.0531 - mae: 13.3190 - val_loss: 269.5887 - val_mse: 269.5887 - val_mae: 12.9098\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 282.5910 - mse: 282.5910 - mae: 13.2407 - val_loss: 268.7308 - val_mse: 268.7308 - val_mae: 12.8954\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 281.6377 - mse: 281.6377 - mae: 13.1990 - val_loss: 268.1822 - val_mse: 268.1822 - val_mae: 12.8731\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 280.7374 - mse: 280.7374 - mae: 13.1706 - val_loss: 267.1357 - val_mse: 267.1357 - val_mae: 12.8308\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 279.7141 - mse: 279.7141 - mae: 13.1263 - val_loss: 265.7525 - val_mse: 265.7525 - val_mae: 12.7634\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 277.9788 - mse: 277.9788 - mae: 13.0447 - val_loss: 263.9028 - val_mse: 263.9028 - val_mae: 12.7091\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 275.9430 - mse: 275.9430 - mae: 12.9875 - val_loss: 261.8479 - val_mse: 261.8479 - val_mae: 12.6538\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 273.5745 - mse: 273.5745 - mae: 12.9172 - val_loss: 261.4566 - val_mse: 261.4566 - val_mae: 12.6073\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 271.1516 - mse: 271.1516 - mae: 12.8474 - val_loss: 260.8287 - val_mse: 260.8287 - val_mae: 12.5455\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 269.2460 - mse: 269.2460 - mae: 12.7544 - val_loss: 261.0128 - val_mse: 261.0128 - val_mae: 12.5861\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 268.0571 - mse: 268.0571 - mae: 12.7719 - val_loss: 259.2319 - val_mse: 259.2319 - val_mae: 12.5018\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 266.7383 - mse: 266.7383 - mae: 12.6975 - val_loss: 258.7603 - val_mse: 258.7603 - val_mae: 12.4865\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 265.5056 - mse: 265.5056 - mae: 12.6394 - val_loss: 257.6646 - val_mse: 257.6646 - val_mae: 12.4493\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 264.6895 - mse: 264.6895 - mae: 12.6093 - val_loss: 256.8605 - val_mse: 256.8605 - val_mae: 12.4418\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 263.7798 - mse: 263.7798 - mae: 12.5916 - val_loss: 256.3004 - val_mse: 256.3004 - val_mae: 12.4085\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262.9685 - mse: 262.9685 - mae: 12.5222 - val_loss: 256.5872 - val_mse: 256.5872 - val_mae: 12.4590\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262.5783 - mse: 262.5783 - mae: 12.5645 - val_loss: 255.0728 - val_mse: 255.0728 - val_mae: 12.3535\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 261.9757 - mse: 261.9757 - mae: 12.5012 - val_loss: 254.6636 - val_mse: 254.6636 - val_mae: 12.3469\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 261.1353 - mse: 261.1353 - mae: 12.4517 - val_loss: 254.3525 - val_mse: 254.3525 - val_mae: 12.3608\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 261.1222 - mse: 261.1222 - mae: 12.4746 - val_loss: 253.5031 - val_mse: 253.5031 - val_mae: 12.3003\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.5240 - mse: 260.5240 - mae: 12.4626 - val_loss: 252.7020 - val_mse: 252.7020 - val_mae: 12.2555\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.3754 - mse: 260.3754 - mae: 12.4220 - val_loss: 252.2034 - val_mse: 252.2034 - val_mae: 12.2383\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.1617 - mse: 260.1617 - mae: 12.4108 - val_loss: 252.2593 - val_mse: 252.2593 - val_mae: 12.2520\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 259.1631 - mse: 259.1631 - mae: 12.3716 - val_loss: 252.1134 - val_mse: 252.1134 - val_mae: 12.2650\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 259.1386 - mse: 259.1386 - mae: 12.3817 - val_loss: 251.5701 - val_mse: 251.5701 - val_mae: 12.2381\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 258.5738 - mse: 258.5738 - mae: 12.3266 - val_loss: 253.0779 - val_mse: 253.0779 - val_mae: 12.3653\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 258.6463 - mse: 258.6463 - mae: 12.4224 - val_loss: 250.8909 - val_mse: 250.8909 - val_mae: 12.2213\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 258.1380 - mse: 258.1380 - mae: 12.3479 - val_loss: 250.2298 - val_mse: 250.2298 - val_mae: 12.1749\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 257.6370 - mse: 257.6370 - mae: 12.3041 - val_loss: 250.1976 - val_mse: 250.1976 - val_mae: 12.1994\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 257.1358 - mse: 257.1358 - mae: 12.3001 - val_loss: 249.9348 - val_mse: 249.9348 - val_mae: 12.1928\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.8802 - mse: 256.8802 - mae: 12.3480 - val_loss: 249.2649 - val_mse: 249.2649 - val_mae: 12.1084\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 256.8301 - mse: 256.8301 - mae: 12.2508 - val_loss: 248.7345 - val_mse: 248.7345 - val_mae: 12.1133\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 256.5598 - mse: 256.5598 - mae: 12.2781 - val_loss: 248.5905 - val_mse: 248.5905 - val_mae: 12.1223\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 256.2795 - mse: 256.2795 - mae: 12.2574 - val_loss: 248.5942 - val_mse: 248.5942 - val_mae: 12.1313\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 255.9552 - mse: 255.9552 - mae: 12.2651 - val_loss: 247.8239 - val_mse: 247.8239 - val_mae: 12.0701\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.0371 - mse: 256.0371 - mae: 12.2317 - val_loss: 247.5494 - val_mse: 247.5494 - val_mae: 12.0758\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 255.4214 - mse: 255.4214 - mae: 12.2091 - val_loss: 247.9537 - val_mse: 247.9537 - val_mae: 12.1182\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 255.1523 - mse: 255.1523 - mae: 12.2347 - val_loss: 247.4960 - val_mse: 247.4960 - val_mae: 12.0986\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 255.2259 - mse: 255.2259 - mae: 12.2111 - val_loss: 247.1127 - val_mse: 247.1127 - val_mae: 12.0748\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 254.8685 - mse: 254.8685 - mae: 12.2264 - val_loss: 246.2587 - val_mse: 246.2587 - val_mae: 11.9966\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 254.5946 - mse: 254.5946 - mae: 12.1615 - val_loss: 246.0861 - val_mse: 246.0861 - val_mae: 12.0127\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 254.5387 - mse: 254.5387 - mae: 12.1887 - val_loss: 246.0553 - val_mse: 246.0553 - val_mae: 12.0161\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 254.0919 - mse: 254.0919 - mae: 12.1494 - val_loss: 245.9613 - val_mse: 245.9613 - val_mae: 12.0429\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 253.9967 - mse: 253.9967 - mae: 12.1862 - val_loss: 245.3962 - val_mse: 245.3962 - val_mae: 11.9804\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 253.6616 - mse: 253.6616 - mae: 12.1387 - val_loss: 245.5303 - val_mse: 245.5303 - val_mae: 12.0077\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 253.4087 - mse: 253.4087 - mae: 12.1592 - val_loss: 244.9523 - val_mse: 244.9523 - val_mae: 11.9675\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 253.4388 - mse: 253.4388 - mae: 12.1460 - val_loss: 244.5425 - val_mse: 244.5425 - val_mae: 11.9340\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 253.1590 - mse: 253.1590 - mae: 12.1041 - val_loss: 244.5875 - val_mse: 244.5875 - val_mae: 11.9648\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 252.8471 - mse: 252.8471 - mae: 12.1363 - val_loss: 244.2567 - val_mse: 244.2567 - val_mae: 11.9292\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 253.0023 - mse: 253.0023 - mae: 12.1035 - val_loss: 244.2606 - val_mse: 244.2606 - val_mae: 11.9563\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 252.6689 - mse: 252.6689 - mae: 12.1108 - val_loss: 244.1253 - val_mse: 244.1253 - val_mae: 11.9588\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 252.6472 - mse: 252.6472 - mae: 12.1360 - val_loss: 243.5056 - val_mse: 243.5056 - val_mae: 11.8852\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 252.4501 - mse: 252.4501 - mae: 12.0704 - val_loss: 243.5286 - val_mse: 243.5286 - val_mae: 11.9374\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 252.0473 - mse: 252.0473 - mae: 12.0900 - val_loss: 243.1591 - val_mse: 243.1591 - val_mae: 11.9005\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.7853 - mse: 251.7853 - mae: 12.0492 - val_loss: 243.2799 - val_mse: 243.2799 - val_mae: 11.9288\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.6737 - mse: 251.6737 - mae: 12.0798 - val_loss: 242.7715 - val_mse: 242.7715 - val_mae: 11.8881\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.5836 - mse: 251.5836 - mae: 12.0173 - val_loss: 243.1197 - val_mse: 243.1197 - val_mae: 11.9400\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 251.5882 - mse: 251.5882 - mae: 12.0734 - val_loss: 242.7692 - val_mse: 242.7692 - val_mae: 11.9093\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 251.3413 - mse: 251.3413 - mae: 12.0515 - val_loss: 242.3956 - val_mse: 242.3956 - val_mae: 11.8812\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 251.2673 - mse: 251.2673 - mae: 12.0336 - val_loss: 242.4387 - val_mse: 242.4387 - val_mae: 11.9047\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.1460 - mse: 251.1460 - mae: 12.0578 - val_loss: 241.8842 - val_mse: 241.8842 - val_mae: 11.8629\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.0363 - mse: 251.0363 - mae: 12.0363 - val_loss: 241.9516 - val_mse: 241.9516 - val_mae: 11.8734\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 250.5860 - mse: 250.5860 - mae: 12.0597 - val_loss: 241.3729 - val_mse: 241.3729 - val_mae: 11.8063\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 250.7861 - mse: 250.7861 - mae: 11.9816 - val_loss: 241.1016 - val_mse: 241.1016 - val_mae: 11.8035\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 250.6757 - mse: 250.6757 - mae: 11.9925 - val_loss: 241.3220 - val_mse: 241.3220 - val_mae: 11.8543\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 250.3737 - mse: 250.3737 - mae: 12.0276 - val_loss: 240.9904 - val_mse: 240.9904 - val_mae: 11.8319\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 250.3011 - mse: 250.3011 - mae: 11.9804 - val_loss: 241.2790 - val_mse: 241.2790 - val_mae: 11.8674\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 250.2162 - mse: 250.2162 - mae: 12.0116 - val_loss: 240.7578 - val_mse: 240.7578 - val_mae: 11.8325\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.9255 - mse: 249.9255 - mae: 12.0125 - val_loss: 240.2584 - val_mse: 240.2584 - val_mae: 11.7711\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 250.2792 - mse: 250.2792 - mae: 11.9867 - val_loss: 240.0759 - val_mse: 240.0759 - val_mae: 11.7552\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 249.9969 - mse: 249.9969 - mae: 11.9544 - val_loss: 240.1514 - val_mse: 240.1514 - val_mae: 11.7701\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.6236 - mse: 249.6236 - mae: 11.9561 - val_loss: 239.9144 - val_mse: 239.9144 - val_mae: 11.7576\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.5031 - mse: 249.5031 - mae: 11.9455 - val_loss: 239.8617 - val_mse: 239.8617 - val_mae: 11.7779\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.9667 - mse: 249.9667 - mae: 11.9706 - val_loss: 240.2022 - val_mse: 240.2022 - val_mae: 11.8092\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.3255 - mse: 249.3255 - mae: 11.9618 - val_loss: 240.0032 - val_mse: 240.0032 - val_mae: 11.7993\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.3649 - mse: 249.3650 - mae: 11.9614 - val_loss: 240.1897 - val_mse: 240.1897 - val_mae: 11.8239\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.1455 - mse: 249.1455 - mae: 11.9884 - val_loss: 239.3663 - val_mse: 239.3663 - val_mae: 11.7543\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.1478 - mse: 249.1478 - mae: 11.9683 - val_loss: 239.0741 - val_mse: 239.0741 - val_mae: 11.7184\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.1483 - mse: 249.1483 - mae: 11.9241 - val_loss: 239.0590 - val_mse: 239.0590 - val_mae: 11.7385\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 248.8047 - mse: 248.8047 - mae: 11.9339 - val_loss: 238.9579 - val_mse: 238.9579 - val_mae: 11.7303\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 248.7339 - mse: 248.7339 - mae: 11.9321 - val_loss: 238.6908 - val_mse: 238.6908 - val_mae: 11.7022\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 248.6708 - mse: 248.6708 - mae: 11.9141 - val_loss: 238.5586 - val_mse: 238.5586 - val_mae: 11.7013\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 248.4376 - mse: 248.4376 - mae: 11.9002 - val_loss: 238.4972 - val_mse: 238.4972 - val_mae: 11.7043\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 248.3363 - mse: 248.3363 - mae: 11.9282 - val_loss: 238.3213 - val_mse: 238.3213 - val_mae: 11.6824\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 426.0894 - mse: 426.0894 - mae: 17.1377 - val_loss: 208.3847 - val_mse: 208.3847 - val_mae: 11.6805\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 274.3473 - mse: 274.3473 - mae: 12.7464 - val_loss: 172.1255 - val_mse: 172.1255 - val_mae: 9.9935\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 250.2020 - mse: 250.2020 - mae: 11.7891 - val_loss: 162.5269 - val_mse: 162.5269 - val_mae: 9.5504\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 239.9208 - mse: 239.9208 - mae: 11.4407 - val_loss: 159.2494 - val_mse: 159.2494 - val_mae: 9.5504\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 233.9828 - mse: 233.9828 - mae: 11.2250 - val_loss: 154.0183 - val_mse: 154.0183 - val_mae: 9.1992\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 229.5711 - mse: 229.5711 - mae: 11.0752 - val_loss: 148.1456 - val_mse: 148.1456 - val_mae: 8.7777\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 227.3434 - mse: 227.3434 - mae: 11.0000 - val_loss: 147.1151 - val_mse: 147.1151 - val_mae: 8.7010\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 227.1909 - mse: 227.1909 - mae: 10.9320 - val_loss: 156.7969 - val_mse: 156.7969 - val_mae: 9.5904\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 225.7108 - mse: 225.7108 - mae: 10.9047 - val_loss: 160.6547 - val_mse: 160.6547 - val_mae: 9.8603\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 225.3559 - mse: 225.3559 - mae: 10.9993 - val_loss: 148.9339 - val_mse: 148.9339 - val_mae: 8.9701\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 224.0244 - mse: 224.0244 - mae: 10.9076 - val_loss: 145.9443 - val_mse: 145.9443 - val_mae: 8.6623\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.3801 - mse: 224.3801 - mae: 10.8921 - val_loss: 145.2942 - val_mse: 145.2942 - val_mae: 8.5474\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 223.3100 - mse: 223.3100 - mae: 10.8144 - val_loss: 151.4754 - val_mse: 151.4754 - val_mae: 9.1995\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.5033 - mse: 224.5033 - mae: 10.9728 - val_loss: 145.2390 - val_mse: 145.2390 - val_mae: 8.6095\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 222.4631 - mse: 222.4631 - mae: 10.8598 - val_loss: 144.3244 - val_mse: 144.3244 - val_mae: 8.4558\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 221.4663 - mse: 221.4663 - mae: 10.8455 - val_loss: 141.8853 - val_mse: 141.8853 - val_mae: 8.3697\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 219.4333 - mse: 219.4333 - mae: 10.6176 - val_loss: 142.3667 - val_mse: 142.3667 - val_mae: 8.5640\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 217.8869 - mse: 217.8869 - mae: 10.6855 - val_loss: 138.8976 - val_mse: 138.8976 - val_mae: 8.3417\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 214.9627 - mse: 214.9627 - mae: 10.5673 - val_loss: 141.5848 - val_mse: 141.5848 - val_mae: 8.6840\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 214.0774 - mse: 214.0774 - mae: 10.5138 - val_loss: 146.0136 - val_mse: 146.0136 - val_mae: 9.0530\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.0259 - mse: 213.0259 - mae: 10.5582 - val_loss: 142.7695 - val_mse: 142.7695 - val_mae: 8.7853\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.1055 - mse: 213.1055 - mae: 10.5138 - val_loss: 137.6353 - val_mse: 137.6353 - val_mae: 8.3075\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 213.6257 - mse: 213.6257 - mae: 10.4761 - val_loss: 139.9601 - val_mse: 139.9601 - val_mae: 8.5491\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 212.4288 - mse: 212.4288 - mae: 10.4363 - val_loss: 141.0101 - val_mse: 141.0101 - val_mae: 8.6428\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 211.4539 - mse: 211.4539 - mae: 10.4447 - val_loss: 136.6985 - val_mse: 136.6985 - val_mae: 8.1971\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 211.0514 - mse: 211.0514 - mae: 10.3491 - val_loss: 141.5616 - val_mse: 141.5616 - val_mae: 8.6961\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.9537 - mse: 212.9537 - mae: 10.5458 - val_loss: 136.3135 - val_mse: 136.3135 - val_mae: 8.1354\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 212.5287 - mse: 212.5287 - mae: 10.3736 - val_loss: 136.3890 - val_mse: 136.3890 - val_mae: 8.1578\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 211.6529 - mse: 211.6529 - mae: 10.3910 - val_loss: 137.2676 - val_mse: 137.2676 - val_mae: 8.2570\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 211.2876 - mse: 211.2876 - mae: 10.3890 - val_loss: 135.8138 - val_mse: 135.8138 - val_mae: 8.1088\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 210.5765 - mse: 210.5765 - mae: 10.3727 - val_loss: 136.6452 - val_mse: 136.6452 - val_mae: 8.0487\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.3922 - mse: 212.3922 - mae: 10.2137 - val_loss: 144.7242 - val_mse: 144.7242 - val_mae: 8.9718\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.0266 - mse: 212.0266 - mae: 10.4818 - val_loss: 137.0358 - val_mse: 137.0358 - val_mae: 8.2645\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 209.8135 - mse: 209.8135 - mae: 10.3056 - val_loss: 142.0412 - val_mse: 142.0412 - val_mae: 8.7559\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 210.6138 - mse: 210.6138 - mae: 10.3578 - val_loss: 142.8784 - val_mse: 142.8784 - val_mae: 8.8230\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 210.6308 - mse: 210.6308 - mae: 10.3331 - val_loss: 136.6965 - val_mse: 136.6965 - val_mae: 8.2300\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 209.5204 - mse: 209.5204 - mae: 10.3630 - val_loss: 132.5887 - val_mse: 132.5887 - val_mae: 7.9573\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 208.0446 - mse: 208.0446 - mae: 10.2367 - val_loss: 130.5283 - val_mse: 130.5283 - val_mae: 7.9026\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 200.3153 - mse: 200.3153 - mae: 9.9797 - val_loss: 126.9581 - val_mse: 126.9581 - val_mae: 7.8835\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 197.9349 - mse: 197.9349 - mae: 9.9812 - val_loss: 132.5450 - val_mse: 132.5450 - val_mae: 8.4139\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 198.3160 - mse: 198.3160 - mae: 10.0044 - val_loss: 124.4006 - val_mse: 124.4006 - val_mae: 7.5677\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 198.5464 - mse: 198.5464 - mae: 9.9533 - val_loss: 124.4791 - val_mse: 124.4791 - val_mae: 7.5677\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 199.3157 - mse: 199.3157 - mae: 9.9759 - val_loss: 124.3870 - val_mse: 124.3870 - val_mae: 7.5614\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 196.2118 - mse: 196.2118 - mae: 9.8503 - val_loss: 120.6311 - val_mse: 120.6311 - val_mae: 7.5334\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 189.2417 - mse: 189.2417 - mae: 9.7831 - val_loss: 115.4648 - val_mse: 115.4648 - val_mae: 7.2400\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 185.0309 - mse: 185.0309 - mae: 9.5795 - val_loss: 114.0959 - val_mse: 114.0959 - val_mae: 7.1245\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 187.5087 - mse: 187.5087 - mae: 9.6146 - val_loss: 119.5093 - val_mse: 119.5093 - val_mae: 7.9457\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 181.7666 - mse: 181.7666 - mae: 9.3909 - val_loss: 116.6215 - val_mse: 116.6215 - val_mae: 7.6920\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 179.3873 - mse: 179.3873 - mae: 9.4034 - val_loss: 123.2941 - val_mse: 123.2941 - val_mae: 8.2574\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.1613 - mse: 182.1613 - mae: 9.5170 - val_loss: 110.1423 - val_mse: 110.1423 - val_mae: 6.9648\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 178.5538 - mse: 178.5538 - mae: 9.2170 - val_loss: 114.8392 - val_mse: 114.8392 - val_mae: 7.5306\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 181.0176 - mse: 181.0176 - mae: 9.4770 - val_loss: 109.7109 - val_mse: 109.7109 - val_mae: 6.9302\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 180.9554 - mse: 180.9554 - mae: 9.3990 - val_loss: 116.6113 - val_mse: 116.6113 - val_mae: 7.6812\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 179.2005 - mse: 179.2005 - mae: 9.4154 - val_loss: 117.6988 - val_mse: 117.6988 - val_mae: 7.5367\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.1100 - mse: 182.1100 - mae: 9.4096 - val_loss: 113.4451 - val_mse: 113.4451 - val_mae: 7.4046\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 178.1292 - mse: 178.1292 - mae: 9.3018 - val_loss: 110.2771 - val_mse: 110.2771 - val_mae: 7.0670\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 176.9241 - mse: 176.9241 - mae: 9.2315 - val_loss: 108.4611 - val_mse: 108.4611 - val_mae: 7.0699\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 175.1684 - mse: 175.1684 - mae: 9.0731 - val_loss: 115.3710 - val_mse: 115.3710 - val_mae: 7.8931\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 171.6043 - mse: 171.6043 - mae: 9.1035 - val_loss: 102.8863 - val_mse: 102.8863 - val_mae: 6.6307\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 170.3732 - mse: 170.3732 - mae: 9.0620 - val_loss: 102.6829 - val_mse: 102.6829 - val_mae: 6.6615\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 172.8058 - mse: 172.8058 - mae: 9.0358 - val_loss: 104.7086 - val_mse: 104.7086 - val_mae: 6.9409\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.1970 - mse: 169.1970 - mae: 8.9789 - val_loss: 101.9686 - val_mse: 101.9686 - val_mae: 6.5908\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 171.5605 - mse: 171.5605 - mae: 9.0976 - val_loss: 106.6143 - val_mse: 106.6143 - val_mae: 7.1416\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 171.5923 - mse: 171.5923 - mae: 9.0090 - val_loss: 111.7698 - val_mse: 111.7698 - val_mae: 7.5963\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 173.0122 - mse: 173.0122 - mae: 9.2391 - val_loss: 101.9335 - val_mse: 101.9335 - val_mae: 6.6109\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.3564 - mse: 169.3564 - mae: 8.8838 - val_loss: 102.4132 - val_mse: 102.4132 - val_mae: 6.6793\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 172.1088 - mse: 172.1088 - mae: 9.0241 - val_loss: 107.6000 - val_mse: 107.6000 - val_mae: 7.2463\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 168.2642 - mse: 168.2642 - mae: 8.9716 - val_loss: 107.8341 - val_mse: 107.8341 - val_mae: 7.2776\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 169.7921 - mse: 169.7921 - mae: 9.0726 - val_loss: 101.8556 - val_mse: 101.8556 - val_mae: 6.5587\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 170.6544 - mse: 170.6544 - mae: 9.0659 - val_loss: 105.0082 - val_mse: 105.0082 - val_mae: 6.7807\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 171.5599 - mse: 171.5599 - mae: 8.9624 - val_loss: 108.3322 - val_mse: 108.3322 - val_mae: 7.0601\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 174.9108 - mse: 174.9108 - mae: 9.1561 - val_loss: 107.4339 - val_mse: 107.4339 - val_mae: 7.2354\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 168.0804 - mse: 168.0804 - mae: 8.9046 - val_loss: 102.8304 - val_mse: 102.8304 - val_mae: 6.5879\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 169.5779 - mse: 169.5779 - mae: 8.8804 - val_loss: 112.7287 - val_mse: 112.7287 - val_mae: 7.7194\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 171.4391 - mse: 171.4391 - mae: 9.1090 - val_loss: 102.9393 - val_mse: 102.9393 - val_mae: 6.7135\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 170.2214 - mse: 170.2214 - mae: 8.9419 - val_loss: 102.6087 - val_mse: 102.6087 - val_mae: 6.6042\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 170.1984 - mse: 170.1984 - mae: 8.8351 - val_loss: 110.5941 - val_mse: 110.5941 - val_mae: 7.5537\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 167.5693 - mse: 167.5693 - mae: 8.8751 - val_loss: 104.4144 - val_mse: 104.4144 - val_mae: 6.9203\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.4983 - mse: 169.4983 - mae: 8.9896 - val_loss: 101.8479 - val_mse: 101.8479 - val_mae: 6.5444\n",
            "Epoch 79: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 409.2893 - mse: 409.2893 - mae: 16.2391 - val_loss: 180.0240 - val_mse: 180.0240 - val_mae: 10.6462\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 253.1523 - mse: 253.1523 - mae: 12.1139 - val_loss: 135.7719 - val_mse: 135.7719 - val_mae: 8.7093\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 227.1061 - mse: 227.1061 - mae: 11.1731 - val_loss: 134.4179 - val_mse: 134.4179 - val_mae: 8.6639\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.8916 - mse: 213.8916 - mae: 10.7595 - val_loss: 125.0637 - val_mse: 125.0637 - val_mae: 8.1590\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 208.8348 - mse: 208.8348 - mae: 10.5887 - val_loss: 114.3132 - val_mse: 114.3132 - val_mae: 7.6242\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 198.2243 - mse: 198.2243 - mae: 10.1860 - val_loss: 103.1176 - val_mse: 103.1176 - val_mae: 7.0031\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 192.7334 - mse: 192.7334 - mae: 9.9951 - val_loss: 102.9120 - val_mse: 102.9120 - val_mae: 7.0414\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 195.2317 - mse: 195.2317 - mae: 9.9772 - val_loss: 117.7267 - val_mse: 117.7267 - val_mae: 8.1875\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 187.0696 - mse: 187.0696 - mae: 9.8088 - val_loss: 114.5259 - val_mse: 114.5259 - val_mae: 7.9821\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 184.8095 - mse: 184.8095 - mae: 9.7676 - val_loss: 115.7936 - val_mse: 115.7936 - val_mae: 8.1850\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 183.4971 - mse: 183.4971 - mae: 9.8227 - val_loss: 97.3638 - val_mse: 97.3638 - val_mae: 6.7699\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 181.6009 - mse: 181.6009 - mae: 9.6217 - val_loss: 103.6258 - val_mse: 103.6258 - val_mae: 7.3291\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 180.5944 - mse: 180.5944 - mae: 9.6084 - val_loss: 99.9475 - val_mse: 99.9475 - val_mae: 7.0340\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 180.9659 - mse: 180.9659 - mae: 9.6270 - val_loss: 97.5422 - val_mse: 97.5422 - val_mae: 6.8185\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 178.9255 - mse: 178.9255 - mae: 9.5296 - val_loss: 95.0643 - val_mse: 95.0643 - val_mae: 6.5428\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 178.8418 - mse: 178.8418 - mae: 9.4698 - val_loss: 94.5759 - val_mse: 94.5759 - val_mae: 6.5192\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 180.2166 - mse: 180.2166 - mae: 9.4802 - val_loss: 106.5429 - val_mse: 106.5429 - val_mae: 7.5877\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 180.4211 - mse: 180.4211 - mae: 9.5773 - val_loss: 96.2154 - val_mse: 96.2154 - val_mae: 6.6930\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 177.6465 - mse: 177.6465 - mae: 9.3769 - val_loss: 98.4777 - val_mse: 98.4777 - val_mae: 6.9207\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 177.7098 - mse: 177.7098 - mae: 9.4368 - val_loss: 102.4556 - val_mse: 102.4556 - val_mae: 7.2892\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 175.1621 - mse: 175.1621 - mae: 9.3856 - val_loss: 94.9208 - val_mse: 94.9208 - val_mae: 6.5903\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 177.0368 - mse: 177.0368 - mae: 9.3731 - val_loss: 94.0295 - val_mse: 94.0295 - val_mae: 6.4888\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 175.4476 - mse: 175.4476 - mae: 9.2734 - val_loss: 96.1986 - val_mse: 96.1986 - val_mae: 6.7185\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 178.1695 - mse: 178.1695 - mae: 9.2972 - val_loss: 108.0663 - val_mse: 108.0663 - val_mae: 7.7897\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 172.7868 - mse: 172.7868 - mae: 9.3491 - val_loss: 93.9084 - val_mse: 93.9084 - val_mae: 6.7171\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 171.7450 - mse: 171.7450 - mae: 9.2560 - val_loss: 96.3388 - val_mse: 96.3388 - val_mae: 7.0925\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 168.1746 - mse: 168.1746 - mae: 9.1790 - val_loss: 85.7104 - val_mse: 85.7104 - val_mae: 6.1154\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 167.8938 - mse: 167.8938 - mae: 9.0972 - val_loss: 82.4054 - val_mse: 82.4054 - val_mae: 5.8454\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 162.1757 - mse: 162.1757 - mae: 8.8612 - val_loss: 81.8946 - val_mse: 81.8946 - val_mae: 5.8613\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 160.8899 - mse: 160.8899 - mae: 8.8632 - val_loss: 82.5488 - val_mse: 82.5488 - val_mae: 5.9562\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 164.3881 - mse: 164.3881 - mae: 9.0649 - val_loss: 89.7023 - val_mse: 89.7023 - val_mae: 6.4668\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 164.9266 - mse: 164.9266 - mae: 8.7042 - val_loss: 101.8733 - val_mse: 101.8733 - val_mae: 7.7834\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 158.1115 - mse: 158.1115 - mae: 8.8518 - val_loss: 83.2012 - val_mse: 83.2012 - val_mae: 6.2560\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.1226 - mse: 155.1226 - mae: 8.6930 - val_loss: 93.9493 - val_mse: 93.9493 - val_mae: 7.1809\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.8840 - mse: 155.8840 - mae: 8.7788 - val_loss: 82.8130 - val_mse: 82.8130 - val_mae: 6.2318\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 155.7367 - mse: 155.7367 - mae: 8.6552 - val_loss: 82.9007 - val_mse: 82.9007 - val_mae: 6.2507\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 157.7232 - mse: 157.7232 - mae: 8.8820 - val_loss: 77.3306 - val_mse: 77.3306 - val_mae: 5.5729\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 158.4701 - mse: 158.4701 - mae: 8.6888 - val_loss: 78.0103 - val_mse: 78.0103 - val_mae: 5.7062\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.7429 - mse: 154.7429 - mae: 8.6287 - val_loss: 77.4113 - val_mse: 77.4113 - val_mae: 5.6380\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 154.4388 - mse: 154.4388 - mae: 8.6374 - val_loss: 83.3077 - val_mse: 83.3077 - val_mae: 6.2913\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 156.1023 - mse: 156.1023 - mae: 8.7044 - val_loss: 76.5289 - val_mse: 76.5289 - val_mae: 5.5272\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 156.7583 - mse: 156.7583 - mae: 8.7455 - val_loss: 76.9669 - val_mse: 76.9669 - val_mae: 5.5205\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 156.4303 - mse: 156.4303 - mae: 8.7715 - val_loss: 76.3586 - val_mse: 76.3586 - val_mae: 5.4915\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 153.9729 - mse: 153.9729 - mae: 8.5170 - val_loss: 80.5887 - val_mse: 80.5887 - val_mae: 6.0249\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.9552 - mse: 155.9552 - mae: 8.8676 - val_loss: 76.9511 - val_mse: 76.9511 - val_mae: 5.5699\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 160.5395 - mse: 160.5395 - mae: 9.0311 - val_loss: 87.4493 - val_mse: 87.4493 - val_mae: 6.4518\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 164.0994 - mse: 164.0994 - mae: 8.9814 - val_loss: 78.2879 - val_mse: 78.2879 - val_mae: 5.7387\n",
            "Epoch 47: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 371.9982 - mse: 371.9982 - mae: 15.9154 - val_loss: 264.4666 - val_mse: 264.4666 - val_mae: 12.8624\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.0644 - mse: 251.0644 - mae: 12.1971 - val_loss: 222.8039 - val_mse: 222.8039 - val_mae: 11.2960\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 219.7940 - mse: 219.7940 - mae: 11.0503 - val_loss: 198.5486 - val_mse: 198.5486 - val_mae: 10.4333\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 203.1738 - mse: 203.1738 - mae: 10.3976 - val_loss: 189.7417 - val_mse: 189.7417 - val_mae: 10.2264\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 192.8687 - mse: 192.8687 - mae: 10.0589 - val_loss: 178.0751 - val_mse: 178.0751 - val_mae: 9.6259\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 186.7800 - mse: 186.7800 - mae: 9.8111 - val_loss: 173.1688 - val_mse: 173.1688 - val_mae: 9.5027\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 188.0710 - mse: 188.0710 - mae: 9.9400 - val_loss: 171.7936 - val_mse: 171.7936 - val_mae: 9.4148\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 183.0135 - mse: 183.0134 - mae: 9.7553 - val_loss: 175.0344 - val_mse: 175.0344 - val_mae: 9.5783\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 179.5164 - mse: 179.5164 - mae: 9.5043 - val_loss: 164.3806 - val_mse: 164.3806 - val_mae: 9.2566\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 172.5520 - mse: 172.5520 - mae: 9.4092 - val_loss: 157.4398 - val_mse: 157.4398 - val_mae: 8.9070\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 166.2476 - mse: 166.2476 - mae: 9.1192 - val_loss: 153.2362 - val_mse: 153.2362 - val_mae: 8.8128\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 165.8056 - mse: 165.8056 - mae: 9.1072 - val_loss: 153.0812 - val_mse: 153.0812 - val_mae: 8.7769\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 164.7769 - mse: 164.7769 - mae: 8.9639 - val_loss: 151.1891 - val_mse: 151.1891 - val_mae: 8.6981\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 163.9865 - mse: 163.9865 - mae: 8.9274 - val_loss: 150.6910 - val_mse: 150.6910 - val_mae: 8.6475\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 162.7338 - mse: 162.7338 - mae: 8.9154 - val_loss: 154.8451 - val_mse: 154.8451 - val_mae: 8.8415\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 163.8546 - mse: 163.8546 - mae: 8.9404 - val_loss: 150.7980 - val_mse: 150.7980 - val_mae: 8.7389\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 161.2450 - mse: 161.2450 - mae: 8.9319 - val_loss: 153.1415 - val_mse: 153.1415 - val_mae: 8.7160\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 162.8339 - mse: 162.8339 - mae: 8.9216 - val_loss: 148.8305 - val_mse: 148.8305 - val_mae: 8.5474\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 160.8952 - mse: 160.8952 - mae: 8.8782 - val_loss: 150.5657 - val_mse: 150.5657 - val_mae: 8.6101\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 162.3400 - mse: 162.3400 - mae: 8.7989 - val_loss: 147.4320 - val_mse: 147.4320 - val_mae: 8.5496\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 162.8941 - mse: 162.8941 - mae: 8.9287 - val_loss: 147.1489 - val_mse: 147.1489 - val_mae: 8.4993\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 159.8514 - mse: 159.8514 - mae: 8.6531 - val_loss: 149.6189 - val_mse: 149.6189 - val_mae: 8.7194\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 161.7914 - mse: 161.7914 - mae: 9.0329 - val_loss: 148.6923 - val_mse: 148.6923 - val_mae: 8.5474\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 159.7897 - mse: 159.7897 - mae: 8.6313 - val_loss: 148.6160 - val_mse: 148.6160 - val_mae: 8.6411\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 159.0707 - mse: 159.0707 - mae: 8.7594 - val_loss: 145.5431 - val_mse: 145.5431 - val_mae: 8.4219\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 158.3615 - mse: 158.3615 - mae: 8.6441 - val_loss: 147.0768 - val_mse: 147.0768 - val_mae: 8.5391\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 159.0751 - mse: 159.0751 - mae: 8.8072 - val_loss: 147.5345 - val_mse: 147.5345 - val_mae: 8.5663\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 156.9907 - mse: 156.9907 - mae: 8.6487 - val_loss: 141.3238 - val_mse: 141.3238 - val_mae: 8.2609\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.3398 - mse: 154.3398 - mae: 8.5452 - val_loss: 141.4031 - val_mse: 141.4031 - val_mae: 8.2394\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.3847 - mse: 154.3847 - mae: 8.4725 - val_loss: 140.7090 - val_mse: 140.7090 - val_mae: 8.2213\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 153.1794 - mse: 153.1794 - mae: 8.4661 - val_loss: 140.3391 - val_mse: 140.3391 - val_mae: 8.1965\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 155.6492 - mse: 155.6492 - mae: 8.5636 - val_loss: 141.2070 - val_mse: 141.2070 - val_mae: 8.2294\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 153.3498 - mse: 153.3498 - mae: 8.5229 - val_loss: 143.7821 - val_mse: 143.7821 - val_mae: 8.3708\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.9245 - mse: 155.9245 - mae: 8.5659 - val_loss: 139.6892 - val_mse: 139.6892 - val_mae: 8.1410\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 152.4608 - mse: 152.4608 - mae: 8.4041 - val_loss: 139.6915 - val_mse: 139.6915 - val_mae: 8.1166\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 153.0469 - mse: 153.0469 - mae: 8.4060 - val_loss: 140.0411 - val_mse: 140.0411 - val_mae: 8.1646\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 152.4109 - mse: 152.4109 - mae: 8.5180 - val_loss: 139.2698 - val_mse: 139.2698 - val_mae: 8.1666\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 151.1178 - mse: 151.1178 - mae: 8.3634 - val_loss: 137.2651 - val_mse: 137.2651 - val_mae: 8.0724\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 153.4731 - mse: 153.4731 - mae: 8.4767 - val_loss: 137.4662 - val_mse: 137.4662 - val_mae: 8.1748\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 148.5962 - mse: 148.5962 - mae: 8.3607 - val_loss: 135.0956 - val_mse: 135.0956 - val_mae: 7.9418\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 149.1391 - mse: 149.1391 - mae: 8.3584 - val_loss: 139.3459 - val_mse: 139.3459 - val_mae: 8.2222\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 148.3187 - mse: 148.3187 - mae: 8.0871 - val_loss: 166.1250 - val_mse: 166.1250 - val_mae: 9.9441\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.2172 - mse: 154.2172 - mae: 8.6500 - val_loss: 134.6110 - val_mse: 134.6110 - val_mae: 7.9304\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 149.6813 - mse: 149.6813 - mae: 8.3392 - val_loss: 137.1745 - val_mse: 137.1745 - val_mae: 8.0669\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 148.5313 - mse: 148.5313 - mae: 8.1834 - val_loss: 134.3153 - val_mse: 134.3153 - val_mae: 7.9099\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 147.1433 - mse: 147.1433 - mae: 8.1886 - val_loss: 137.1239 - val_mse: 137.1239 - val_mae: 8.0680\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 147.2697 - mse: 147.2697 - mae: 8.2123 - val_loss: 137.2754 - val_mse: 137.2754 - val_mae: 8.0799\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 148.2146 - mse: 148.2146 - mae: 8.3367 - val_loss: 141.2315 - val_mse: 141.2315 - val_mae: 8.3921\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 151.1417 - mse: 151.1417 - mae: 8.4281 - val_loss: 135.4241 - val_mse: 135.4241 - val_mae: 7.9843\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 148.0558 - mse: 148.0558 - mae: 8.2947 - val_loss: 136.6574 - val_mse: 136.6574 - val_mae: 8.0494\n",
            "Epoch 50: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 1791.5344 - mse: 1791.5344 - mae: 30.1304 - val_loss: 867.7452 - val_mse: 867.7452 - val_mae: 22.0751\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 843.5187 - mse: 843.5187 - mae: 22.2193 - val_loss: 539.1613 - val_mse: 539.1613 - val_mae: 18.8573\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 603.0616 - mse: 603.0616 - mae: 19.8830 - val_loss: 423.8261 - val_mse: 423.8261 - val_mae: 17.6670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 508.0063 - mse: 508.0063 - mae: 18.8564 - val_loss: 374.0847 - val_mse: 374.0847 - val_mae: 17.0263\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 463.1109 - mse: 463.1109 - mae: 18.2516 - val_loss: 348.8856 - val_mse: 348.8856 - val_mae: 16.6538\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 438.4946 - mse: 438.4946 - mae: 17.8482 - val_loss: 333.3729 - val_mse: 333.3729 - val_mae: 16.3545\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 422.9275 - mse: 422.9275 - mae: 17.5495 - val_loss: 322.5934 - val_mse: 322.5934 - val_mae: 16.1013\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 412.3872 - mse: 412.3872 - mae: 17.3216 - val_loss: 315.5107 - val_mse: 315.5107 - val_mae: 15.8905\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 404.5153 - mse: 404.5153 - mae: 17.0936 - val_loss: 309.6675 - val_mse: 309.6675 - val_mae: 15.6992\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 397.8778 - mse: 397.8778 - mae: 16.8905 - val_loss: 304.3463 - val_mse: 304.3463 - val_mae: 15.5234\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 391.9263 - mse: 391.9263 - mae: 16.7203 - val_loss: 299.9940 - val_mse: 299.9940 - val_mae: 15.3620\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 386.5379 - mse: 386.5379 - mae: 16.5541 - val_loss: 296.0295 - val_mse: 296.0295 - val_mae: 15.2142\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 381.9330 - mse: 381.9330 - mae: 16.4175 - val_loss: 292.9693 - val_mse: 292.9693 - val_mae: 15.0797\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 377.5331 - mse: 377.5331 - mae: 16.2795 - val_loss: 289.6398 - val_mse: 289.6398 - val_mae: 14.9493\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 373.7246 - mse: 373.7246 - mae: 16.1676 - val_loss: 286.5017 - val_mse: 286.5017 - val_mae: 14.8231\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 370.0493 - mse: 370.0493 - mae: 16.0518 - val_loss: 283.2195 - val_mse: 283.2195 - val_mae: 14.7089\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 366.6574 - mse: 366.6574 - mae: 15.9512 - val_loss: 279.8892 - val_mse: 279.8892 - val_mae: 14.5927\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 363.4005 - mse: 363.4005 - mae: 15.8566 - val_loss: 276.6516 - val_mse: 276.6516 - val_mae: 14.4791\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 360.2756 - mse: 360.2756 - mae: 15.7657 - val_loss: 273.5046 - val_mse: 273.5046 - val_mae: 14.3712\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 357.3262 - mse: 357.3262 - mae: 15.6818 - val_loss: 270.6872 - val_mse: 270.6872 - val_mae: 14.2688\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 354.6187 - mse: 354.6187 - mae: 15.6079 - val_loss: 268.0993 - val_mse: 268.0993 - val_mae: 14.1750\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 352.1298 - mse: 352.1298 - mae: 15.5446 - val_loss: 265.7115 - val_mse: 265.7115 - val_mae: 14.0849\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 349.7949 - mse: 349.7949 - mae: 15.4741 - val_loss: 263.4784 - val_mse: 263.4784 - val_mae: 14.0013\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 347.7142 - mse: 347.7142 - mae: 15.4047 - val_loss: 261.6158 - val_mse: 261.6158 - val_mae: 13.9243\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 345.8703 - mse: 345.8703 - mae: 15.3672 - val_loss: 259.9089 - val_mse: 259.9089 - val_mae: 13.8538\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 344.0146 - mse: 344.0146 - mae: 15.3043 - val_loss: 258.4851 - val_mse: 258.4851 - val_mae: 13.7972\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 342.5167 - mse: 342.5167 - mae: 15.2552 - val_loss: 257.1961 - val_mse: 257.1961 - val_mae: 13.7523\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 340.9800 - mse: 340.9800 - mae: 15.2204 - val_loss: 256.0152 - val_mse: 256.0152 - val_mae: 13.7116\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 339.7007 - mse: 339.7007 - mae: 15.1963 - val_loss: 254.8769 - val_mse: 254.8769 - val_mae: 13.6595\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 338.4525 - mse: 338.4525 - mae: 15.1513 - val_loss: 253.7979 - val_mse: 253.7979 - val_mae: 13.6203\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 337.2595 - mse: 337.2595 - mae: 15.1178 - val_loss: 252.8233 - val_mse: 252.8233 - val_mae: 13.5889\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 336.2653 - mse: 336.2653 - mae: 15.0865 - val_loss: 251.9807 - val_mse: 251.9807 - val_mae: 13.5715\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 335.2148 - mse: 335.2148 - mae: 15.0742 - val_loss: 251.0968 - val_mse: 251.0968 - val_mae: 13.5304\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 334.2225 - mse: 334.2225 - mae: 15.0324 - val_loss: 250.3721 - val_mse: 250.3721 - val_mae: 13.5121\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 333.4779 - mse: 333.4779 - mae: 15.0077 - val_loss: 249.6186 - val_mse: 249.6186 - val_mae: 13.4959\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 332.6928 - mse: 332.6928 - mae: 14.9934 - val_loss: 248.8771 - val_mse: 248.8771 - val_mae: 13.4631\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 331.7910 - mse: 331.7910 - mae: 14.9671 - val_loss: 248.1598 - val_mse: 248.1598 - val_mae: 13.4318\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 331.0671 - mse: 331.0671 - mae: 14.9401 - val_loss: 247.4780 - val_mse: 247.4780 - val_mae: 13.4075\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 330.3590 - mse: 330.3590 - mae: 14.9172 - val_loss: 246.8445 - val_mse: 246.8445 - val_mae: 13.3988\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 329.7329 - mse: 329.7329 - mae: 14.9014 - val_loss: 246.2334 - val_mse: 246.2334 - val_mae: 13.3886\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 329.0460 - mse: 329.0460 - mae: 14.8883 - val_loss: 245.6628 - val_mse: 245.6628 - val_mae: 13.3671\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 328.4059 - mse: 328.4059 - mae: 14.8668 - val_loss: 245.1040 - val_mse: 245.1040 - val_mae: 13.3414\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 327.8372 - mse: 327.8372 - mae: 14.8468 - val_loss: 244.5727 - val_mse: 244.5727 - val_mae: 13.3285\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 327.2512 - mse: 327.2512 - mae: 14.8299 - val_loss: 244.0375 - val_mse: 244.0375 - val_mae: 13.3133\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 326.7573 - mse: 326.7573 - mae: 14.8193 - val_loss: 243.5410 - val_mse: 243.5410 - val_mae: 13.3085\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 326.1383 - mse: 326.1383 - mae: 14.7998 - val_loss: 243.0626 - val_mse: 243.0626 - val_mae: 13.2803\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 325.7403 - mse: 325.7403 - mae: 14.7806 - val_loss: 242.6111 - val_mse: 242.6111 - val_mae: 13.2572\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 325.1541 - mse: 325.1541 - mae: 14.7621 - val_loss: 242.0781 - val_mse: 242.0781 - val_mae: 13.2622\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 324.6503 - mse: 324.6503 - mae: 14.7487 - val_loss: 241.6804 - val_mse: 241.6804 - val_mae: 13.2605\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 324.2753 - mse: 324.2753 - mae: 14.7388 - val_loss: 241.2121 - val_mse: 241.2121 - val_mae: 13.2454\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 323.8229 - mse: 323.8229 - mae: 14.7270 - val_loss: 240.7622 - val_mse: 240.7622 - val_mae: 13.2231\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 323.3817 - mse: 323.3817 - mae: 14.7101 - val_loss: 240.3752 - val_mse: 240.3752 - val_mae: 13.2057\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 323.0066 - mse: 323.0066 - mae: 14.6966 - val_loss: 239.9634 - val_mse: 239.9634 - val_mae: 13.1977\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 322.7097 - mse: 322.7097 - mae: 14.6955 - val_loss: 239.6787 - val_mse: 239.6787 - val_mae: 13.1369\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 322.2390 - mse: 322.2390 - mae: 14.6595 - val_loss: 239.1563 - val_mse: 239.1563 - val_mae: 13.1553\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 321.8307 - mse: 321.8307 - mae: 14.6592 - val_loss: 238.8183 - val_mse: 238.8183 - val_mae: 13.1377\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 321.5213 - mse: 321.5213 - mae: 14.6490 - val_loss: 238.5012 - val_mse: 238.5012 - val_mae: 13.1013\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 321.2118 - mse: 321.2118 - mae: 14.6197 - val_loss: 238.1923 - val_mse: 238.1923 - val_mae: 13.1178\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 320.8582 - mse: 320.8582 - mae: 14.6288 - val_loss: 237.8540 - val_mse: 237.8540 - val_mae: 13.0818\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 320.5404 - mse: 320.5404 - mae: 14.6093 - val_loss: 237.5761 - val_mse: 237.5761 - val_mae: 13.0668\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 320.2249 - mse: 320.2249 - mae: 14.5935 - val_loss: 237.2875 - val_mse: 237.2875 - val_mae: 13.0609\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 319.8665 - mse: 319.8665 - mae: 14.5907 - val_loss: 237.0005 - val_mse: 237.0005 - val_mae: 13.0349\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 319.5863 - mse: 319.5863 - mae: 14.5712 - val_loss: 236.6963 - val_mse: 236.6963 - val_mae: 13.0271\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 319.4117 - mse: 319.4117 - mae: 14.5678 - val_loss: 236.3306 - val_mse: 236.3306 - val_mae: 13.0411\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 319.0417 - mse: 319.0417 - mae: 14.5596 - val_loss: 236.0605 - val_mse: 236.0605 - val_mae: 13.0109\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 318.7646 - mse: 318.7646 - mae: 14.5460 - val_loss: 235.7814 - val_mse: 235.7814 - val_mae: 12.9924\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 318.4739 - mse: 318.4739 - mae: 14.5269 - val_loss: 235.4682 - val_mse: 235.4682 - val_mae: 13.0144\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 318.2289 - mse: 318.2289 - mae: 14.5307 - val_loss: 235.2311 - val_mse: 235.2311 - val_mae: 12.9976\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 318.0003 - mse: 318.0003 - mae: 14.5251 - val_loss: 234.9984 - val_mse: 234.9984 - val_mae: 12.9674\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 317.6689 - mse: 317.6689 - mae: 14.5014 - val_loss: 234.7771 - val_mse: 234.7771 - val_mae: 12.9462\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 317.3719 - mse: 317.3719 - mae: 14.4936 - val_loss: 234.4855 - val_mse: 234.4855 - val_mae: 12.9306\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 317.1595 - mse: 317.1595 - mae: 14.4774 - val_loss: 234.2402 - val_mse: 234.2402 - val_mae: 12.9367\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 316.9124 - mse: 316.9124 - mae: 14.4817 - val_loss: 234.0001 - val_mse: 234.0001 - val_mae: 12.9141\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 316.6126 - mse: 316.6126 - mae: 14.4622 - val_loss: 233.7938 - val_mse: 233.7938 - val_mae: 12.9249\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 316.3472 - mse: 316.3472 - mae: 14.4589 - val_loss: 233.5843 - val_mse: 233.5843 - val_mae: 12.9064\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 316.1570 - mse: 316.1570 - mae: 14.4475 - val_loss: 233.3574 - val_mse: 233.3574 - val_mae: 12.8884\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 315.9361 - mse: 315.9361 - mae: 14.4370 - val_loss: 233.1789 - val_mse: 233.1789 - val_mae: 12.8860\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 315.7211 - mse: 315.7211 - mae: 14.4350 - val_loss: 232.9843 - val_mse: 232.9843 - val_mae: 12.8721\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 315.5128 - mse: 315.5128 - mae: 14.4268 - val_loss: 232.8109 - val_mse: 232.8109 - val_mae: 12.8524\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 315.2973 - mse: 315.2973 - mae: 14.4134 - val_loss: 232.5449 - val_mse: 232.5449 - val_mae: 12.8654\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 315.0924 - mse: 315.0924 - mae: 14.4158 - val_loss: 232.3404 - val_mse: 232.3404 - val_mae: 12.8339\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 314.7925 - mse: 314.7925 - mae: 14.3921 - val_loss: 232.1527 - val_mse: 232.1527 - val_mae: 12.8471\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 314.6000 - mse: 314.6000 - mae: 14.3923 - val_loss: 231.9560 - val_mse: 231.9560 - val_mae: 12.8339\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 314.3719 - mse: 314.3719 - mae: 14.3824 - val_loss: 231.8098 - val_mse: 231.8098 - val_mae: 12.8239\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 314.2059 - mse: 314.2059 - mae: 14.3724 - val_loss: 231.6023 - val_mse: 231.6023 - val_mae: 12.8291\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 314.0573 - mse: 314.0573 - mae: 14.3732 - val_loss: 231.4581 - val_mse: 231.4581 - val_mae: 12.8292\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 313.8120 - mse: 313.8120 - mae: 14.3705 - val_loss: 231.2344 - val_mse: 231.2344 - val_mae: 12.8152\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 313.6211 - mse: 313.6211 - mae: 14.3565 - val_loss: 231.0741 - val_mse: 231.0741 - val_mae: 12.8256\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 313.5414 - mse: 313.5414 - mae: 14.3490 - val_loss: 230.8916 - val_mse: 230.8916 - val_mae: 12.8253\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 313.2233 - mse: 313.2233 - mae: 14.3516 - val_loss: 230.7119 - val_mse: 230.7119 - val_mae: 12.7823\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 313.0356 - mse: 313.0356 - mae: 14.3327 - val_loss: 230.5305 - val_mse: 230.5305 - val_mae: 12.7824\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 312.8388 - mse: 312.8388 - mae: 14.3306 - val_loss: 230.3258 - val_mse: 230.3258 - val_mae: 12.7644\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 312.5901 - mse: 312.5901 - mae: 14.3091 - val_loss: 230.1522 - val_mse: 230.1522 - val_mae: 12.7713\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 312.3609 - mse: 312.3609 - mae: 14.3095 - val_loss: 229.9818 - val_mse: 229.9818 - val_mae: 12.7834\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 312.1931 - mse: 312.1931 - mae: 14.3094 - val_loss: 229.8226 - val_mse: 229.8226 - val_mae: 12.7587\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 312.0857 - mse: 312.0857 - mae: 14.2995 - val_loss: 229.6392 - val_mse: 229.6392 - val_mae: 12.7471\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 311.8402 - mse: 311.8402 - mae: 14.2880 - val_loss: 229.4893 - val_mse: 229.4893 - val_mae: 12.7546\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 311.7150 - mse: 311.7150 - mae: 14.2973 - val_loss: 229.3528 - val_mse: 229.3528 - val_mae: 12.7201\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 311.4149 - mse: 311.4149 - mae: 14.2753 - val_loss: 229.1183 - val_mse: 229.1183 - val_mae: 12.7190\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 311.3553 - mse: 311.3553 - mae: 14.2700 - val_loss: 228.8995 - val_mse: 228.8995 - val_mae: 12.7231\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 2171.7776 - mse: 2171.7776 - mae: 35.1495 - val_loss: 1511.2600 - val_mse: 1511.2600 - val_mae: 30.2531\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1529.2596 - mse: 1529.2596 - mae: 30.7048 - val_loss: 1132.3390 - val_mse: 1132.3390 - val_mae: 26.8673\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1220.7006 - mse: 1220.7006 - mae: 28.0608 - val_loss: 912.9282 - val_mse: 912.9282 - val_mae: 24.6554\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1034.4895 - mse: 1034.4895 - mae: 26.3005 - val_loss: 769.3352 - val_mse: 769.3352 - val_mae: 23.0052\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 910.2568 - mse: 910.2568 - mae: 25.0193 - val_loss: 666.4500 - val_mse: 666.4500 - val_mae: 21.7390\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 818.6079 - mse: 818.6079 - mae: 24.0373 - val_loss: 595.4227 - val_mse: 595.4227 - val_mae: 20.7527\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 749.0079 - mse: 749.0079 - mae: 23.2020 - val_loss: 543.3606 - val_mse: 543.3606 - val_mae: 20.0745\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 695.5386 - mse: 695.5386 - mae: 22.5188 - val_loss: 504.2734 - val_mse: 504.2734 - val_mae: 19.4889\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 655.3668 - mse: 655.3668 - mae: 21.9267 - val_loss: 473.9017 - val_mse: 473.9017 - val_mae: 18.9971\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 624.7556 - mse: 624.7556 - mae: 21.4567 - val_loss: 448.5888 - val_mse: 448.5888 - val_mae: 18.5836\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 599.4391 - mse: 599.4391 - mae: 21.0886 - val_loss: 428.4533 - val_mse: 428.4533 - val_mae: 18.1952\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 579.1131 - mse: 579.1131 - mae: 20.7297 - val_loss: 412.6460 - val_mse: 412.6460 - val_mae: 17.9080\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 562.1715 - mse: 562.1715 - mae: 20.4304 - val_loss: 399.0968 - val_mse: 399.0968 - val_mae: 17.6563\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 547.6039 - mse: 547.6039 - mae: 20.1783 - val_loss: 386.9309 - val_mse: 386.9309 - val_mae: 17.4190\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 534.8714 - mse: 534.8714 - mae: 19.9311 - val_loss: 377.0190 - val_mse: 377.0190 - val_mae: 17.1976\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 524.5849 - mse: 524.5849 - mae: 19.7313 - val_loss: 368.5222 - val_mse: 368.5222 - val_mae: 16.9950\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 515.6895 - mse: 515.6895 - mae: 19.5414 - val_loss: 361.3291 - val_mse: 361.3291 - val_mae: 16.8230\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 507.8648 - mse: 507.8648 - mae: 19.3859 - val_loss: 354.8200 - val_mse: 354.8200 - val_mae: 16.6725\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 500.6563 - mse: 500.6563 - mae: 19.2430 - val_loss: 349.4395 - val_mse: 349.4395 - val_mae: 16.5395\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 494.8913 - mse: 494.8913 - mae: 19.1224 - val_loss: 344.6070 - val_mse: 344.6070 - val_mae: 16.4278\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 489.7101 - mse: 489.7101 - mae: 19.0082 - val_loss: 340.3182 - val_mse: 340.3182 - val_mae: 16.3213\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 485.0299 - mse: 485.0299 - mae: 18.8886 - val_loss: 336.1291 - val_mse: 336.1291 - val_mae: 16.2155\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 480.6116 - mse: 480.6116 - mae: 18.7850 - val_loss: 332.5070 - val_mse: 332.5070 - val_mae: 16.1297\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 476.6361 - mse: 476.6361 - mae: 18.6909 - val_loss: 329.2933 - val_mse: 329.2933 - val_mae: 16.0402\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 473.1323 - mse: 473.1323 - mae: 18.6123 - val_loss: 326.1332 - val_mse: 326.1332 - val_mae: 15.9421\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 469.9574 - mse: 469.9574 - mae: 18.5329 - val_loss: 323.3103 - val_mse: 323.3103 - val_mae: 15.8440\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 467.0281 - mse: 467.0281 - mae: 18.4619 - val_loss: 320.6981 - val_mse: 320.6981 - val_mae: 15.7473\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 464.5074 - mse: 464.5074 - mae: 18.4060 - val_loss: 318.3664 - val_mse: 318.3664 - val_mae: 15.6550\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 462.0953 - mse: 462.0953 - mae: 18.3365 - val_loss: 316.2969 - val_mse: 316.2969 - val_mae: 15.5691\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 459.9308 - mse: 459.9308 - mae: 18.2799 - val_loss: 314.5125 - val_mse: 314.5125 - val_mae: 15.4893\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 458.1026 - mse: 458.1026 - mae: 18.2303 - val_loss: 312.8386 - val_mse: 312.8386 - val_mae: 15.4114\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 456.4195 - mse: 456.4195 - mae: 18.1794 - val_loss: 311.5438 - val_mse: 311.5438 - val_mae: 15.3465\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 454.9709 - mse: 454.9709 - mae: 18.1409 - val_loss: 310.3293 - val_mse: 310.3293 - val_mae: 15.2864\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 453.5567 - mse: 453.5567 - mae: 18.1038 - val_loss: 309.2303 - val_mse: 309.2303 - val_mae: 15.2316\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 452.3925 - mse: 452.3925 - mae: 18.0676 - val_loss: 308.2350 - val_mse: 308.2350 - val_mae: 15.1822\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 451.3682 - mse: 451.3682 - mae: 18.0333 - val_loss: 307.3781 - val_mse: 307.3781 - val_mae: 15.1425\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 450.4427 - mse: 450.4427 - mae: 18.0056 - val_loss: 306.5780 - val_mse: 306.5780 - val_mae: 15.1167\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 449.5165 - mse: 449.5165 - mae: 17.9762 - val_loss: 305.8465 - val_mse: 305.8465 - val_mae: 15.0942\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 448.7142 - mse: 448.7142 - mae: 17.9524 - val_loss: 305.2492 - val_mse: 305.2492 - val_mae: 15.0798\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 447.9888 - mse: 447.9888 - mae: 17.9262 - val_loss: 304.7460 - val_mse: 304.7460 - val_mae: 15.0691\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 447.3306 - mse: 447.3306 - mae: 17.9101 - val_loss: 304.2715 - val_mse: 304.2715 - val_mae: 15.0565\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 446.7163 - mse: 446.7163 - mae: 17.8916 - val_loss: 303.8174 - val_mse: 303.8174 - val_mae: 15.0439\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 446.1639 - mse: 446.1639 - mae: 17.8716 - val_loss: 303.4360 - val_mse: 303.4360 - val_mae: 15.0326\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 445.6680 - mse: 445.6680 - mae: 17.8549 - val_loss: 303.1034 - val_mse: 303.1034 - val_mae: 15.0218\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 445.2339 - mse: 445.2339 - mae: 17.8453 - val_loss: 302.7764 - val_mse: 302.7764 - val_mae: 15.0101\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 444.7628 - mse: 444.7628 - mae: 17.8210 - val_loss: 302.5107 - val_mse: 302.5107 - val_mae: 15.0002\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 444.2979 - mse: 444.2979 - mae: 17.8064 - val_loss: 302.3246 - val_mse: 302.3246 - val_mae: 14.9919\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 443.8476 - mse: 443.8476 - mae: 17.7963 - val_loss: 302.1818 - val_mse: 302.1818 - val_mae: 14.9855\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 443.3870 - mse: 443.3870 - mae: 17.7792 - val_loss: 302.0734 - val_mse: 302.0734 - val_mae: 14.9781\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 442.9982 - mse: 442.9982 - mae: 17.7749 - val_loss: 301.8768 - val_mse: 301.8768 - val_mae: 14.9700\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 442.5410 - mse: 442.5410 - mae: 17.7644 - val_loss: 301.8548 - val_mse: 301.8548 - val_mae: 14.9739\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 441.9088 - mse: 441.9088 - mae: 17.7492 - val_loss: 301.9571 - val_mse: 301.9571 - val_mae: 14.9891\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 440.9184 - mse: 440.9184 - mae: 17.7306 - val_loss: 301.7129 - val_mse: 301.7129 - val_mae: 14.9848\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 440.0104 - mse: 440.0104 - mae: 17.7072 - val_loss: 300.9512 - val_mse: 300.9512 - val_mae: 14.9637\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 438.7106 - mse: 438.7106 - mae: 17.6747 - val_loss: 299.9446 - val_mse: 299.9446 - val_mae: 14.9315\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 437.5814 - mse: 437.5814 - mae: 17.6462 - val_loss: 298.8188 - val_mse: 298.8188 - val_mae: 14.8877\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 436.5537 - mse: 436.5537 - mae: 17.6137 - val_loss: 297.8382 - val_mse: 297.8382 - val_mae: 14.8505\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 435.5909 - mse: 435.5909 - mae: 17.5813 - val_loss: 297.0807 - val_mse: 297.0807 - val_mae: 14.8211\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 434.8481 - mse: 434.8481 - mae: 17.5485 - val_loss: 296.4713 - val_mse: 296.4713 - val_mae: 14.7948\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 434.1936 - mse: 434.1936 - mae: 17.5253 - val_loss: 295.9751 - val_mse: 295.9751 - val_mae: 14.7696\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 433.5563 - mse: 433.5563 - mae: 17.5016 - val_loss: 295.5058 - val_mse: 295.5058 - val_mae: 14.7433\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 432.8627 - mse: 432.8627 - mae: 17.4808 - val_loss: 295.0846 - val_mse: 295.0846 - val_mae: 14.7263\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 432.3767 - mse: 432.3767 - mae: 17.4662 - val_loss: 294.7678 - val_mse: 294.7678 - val_mae: 14.7178\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 431.9069 - mse: 431.9069 - mae: 17.4522 - val_loss: 294.5072 - val_mse: 294.5072 - val_mae: 14.7094\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 431.5943 - mse: 431.5943 - mae: 17.4365 - val_loss: 294.3307 - val_mse: 294.3307 - val_mae: 14.7019\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 431.2102 - mse: 431.2102 - mae: 17.4279 - val_loss: 294.2290 - val_mse: 294.2290 - val_mae: 14.6956\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 430.8798 - mse: 430.8798 - mae: 17.4226 - val_loss: 294.0970 - val_mse: 294.0970 - val_mae: 14.6883\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 430.5737 - mse: 430.5737 - mae: 17.4142 - val_loss: 294.0223 - val_mse: 294.0223 - val_mae: 14.6812\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 430.3146 - mse: 430.3146 - mae: 17.4095 - val_loss: 294.0258 - val_mse: 294.0258 - val_mae: 14.6750\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 429.9969 - mse: 429.9969 - mae: 17.3996 - val_loss: 293.9829 - val_mse: 293.9829 - val_mae: 14.6690\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 429.8191 - mse: 429.8191 - mae: 17.4036 - val_loss: 293.8184 - val_mse: 293.8184 - val_mae: 14.6657\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 429.6206 - mse: 429.6206 - mae: 17.3903 - val_loss: 293.8609 - val_mse: 293.8609 - val_mae: 14.6683\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 429.4349 - mse: 429.4349 - mae: 17.3865 - val_loss: 293.8160 - val_mse: 293.8160 - val_mae: 14.6682\n",
            "Epoch 73: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: 4068.4595 - mse: 4068.4595 - mae: 44.5027 - val_loss: 778.7184 - val_mse: 778.7184 - val_mae: 24.0201\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 682.1143 - mse: 682.1143 - mae: 22.9212 - val_loss: 447.9818 - val_mse: 447.9818 - val_mae: 19.1097\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 511.5200 - mse: 511.5200 - mae: 19.8745 - val_loss: 408.0020 - val_mse: 408.0020 - val_mae: 17.8281\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 474.8583 - mse: 474.8583 - mae: 18.9059 - val_loss: 396.0165 - val_mse: 396.0165 - val_mae: 17.3281\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 456.1217 - mse: 456.1217 - mae: 18.4334 - val_loss: 390.9711 - val_mse: 390.9711 - val_mae: 17.1694\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 444.7433 - mse: 444.7433 - mae: 18.1716 - val_loss: 388.6051 - val_mse: 388.6051 - val_mae: 17.1132\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 437.2661 - mse: 437.2661 - mae: 18.0303 - val_loss: 387.6006 - val_mse: 387.6006 - val_mae: 17.1028\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 431.3572 - mse: 431.3572 - mae: 17.9272 - val_loss: 387.3333 - val_mse: 387.3333 - val_mae: 17.1305\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 427.7094 - mse: 427.7094 - mae: 17.8471 - val_loss: 386.9325 - val_mse: 386.9325 - val_mae: 17.1322\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 424.3733 - mse: 424.3733 - mae: 17.7841 - val_loss: 386.7784 - val_mse: 386.7784 - val_mae: 17.1531\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 421.3221 - mse: 421.3221 - mae: 17.7123 - val_loss: 387.6547 - val_mse: 387.6547 - val_mae: 17.2266\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 418.6389 - mse: 418.6389 - mae: 17.6489 - val_loss: 388.5784 - val_mse: 388.5784 - val_mae: 17.2927\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 416.4732 - mse: 416.4732 - mae: 17.5898 - val_loss: 390.1357 - val_mse: 390.1357 - val_mae: 17.3709\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 414.2763 - mse: 414.2763 - mae: 17.5151 - val_loss: 389.5367 - val_mse: 389.5367 - val_mae: 17.3593\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 412.7853 - mse: 412.7853 - mae: 17.4952 - val_loss: 390.2958 - val_mse: 390.2958 - val_mae: 17.3923\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 411.4188 - mse: 411.4188 - mae: 17.4568 - val_loss: 391.6713 - val_mse: 391.6713 - val_mae: 17.4413\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 410.2712 - mse: 410.2712 - mae: 17.4108 - val_loss: 390.9868 - val_mse: 390.9868 - val_mae: 17.4216\n",
            "Epoch 17: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 27ms/step - loss: 458.6856 - mse: 458.6856 - mae: 18.0797 - val_loss: 272.2209 - val_mse: 272.2209 - val_mae: 14.1311\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 351.8450 - mse: 351.8450 - mae: 15.5659 - val_loss: 250.9918 - val_mse: 250.9918 - val_mae: 13.3551\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 333.4094 - mse: 333.4094 - mae: 14.9566 - val_loss: 235.9654 - val_mse: 235.9654 - val_mae: 12.8496\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 317.1062 - mse: 317.1062 - mae: 14.4283 - val_loss: 222.3758 - val_mse: 222.3758 - val_mae: 12.3540\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 300.8439 - mse: 300.8439 - mae: 13.8721 - val_loss: 209.5037 - val_mse: 209.5037 - val_mae: 11.8502\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 287.0982 - mse: 287.0982 - mae: 13.3887 - val_loss: 198.3698 - val_mse: 198.3698 - val_mae: 11.3804\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 275.5735 - mse: 275.5735 - mae: 12.9827 - val_loss: 190.0566 - val_mse: 190.0566 - val_mae: 11.0156\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 265.9074 - mse: 265.9074 - mae: 12.6363 - val_loss: 183.9666 - val_mse: 183.9666 - val_mae: 10.7134\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 257.5132 - mse: 257.5132 - mae: 12.3324 - val_loss: 178.4693 - val_mse: 178.4693 - val_mae: 10.4808\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 250.3591 - mse: 250.3591 - mae: 12.0819 - val_loss: 171.9856 - val_mse: 171.9856 - val_mae: 10.2117\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 244.5045 - mse: 244.5045 - mae: 11.8625 - val_loss: 167.4264 - val_mse: 167.4264 - val_mae: 9.9985\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 239.0503 - mse: 239.0503 - mae: 11.6653 - val_loss: 163.1608 - val_mse: 163.1608 - val_mae: 9.8081\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 234.5318 - mse: 234.5318 - mae: 11.5042 - val_loss: 161.2653 - val_mse: 161.2653 - val_mae: 9.7337\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 230.9107 - mse: 230.9107 - mae: 11.4122 - val_loss: 156.8236 - val_mse: 156.8236 - val_mae: 9.5106\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 227.3581 - mse: 227.3581 - mae: 11.2614 - val_loss: 154.1855 - val_mse: 154.1855 - val_mae: 9.3852\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.8063 - mse: 224.8063 - mae: 11.1656 - val_loss: 151.2095 - val_mse: 151.2095 - val_mae: 9.2370\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 222.5179 - mse: 222.5179 - mae: 11.0427 - val_loss: 150.3037 - val_mse: 150.3037 - val_mae: 9.1998\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 220.5411 - mse: 220.5411 - mae: 10.9832 - val_loss: 148.6335 - val_mse: 148.6335 - val_mae: 9.1241\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 218.9499 - mse: 218.9499 - mae: 10.9102 - val_loss: 148.4370 - val_mse: 148.4370 - val_mae: 9.1547\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 217.5077 - mse: 217.5077 - mae: 10.8812 - val_loss: 147.7161 - val_mse: 147.7161 - val_mae: 9.1513\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 215.6955 - mse: 215.6955 - mae: 10.8726 - val_loss: 144.6655 - val_mse: 144.6655 - val_mae: 8.9988\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.9401 - mse: 213.9401 - mae: 10.7692 - val_loss: 142.2857 - val_mse: 142.2857 - val_mae: 8.8842\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 212.1947 - mse: 212.1947 - mae: 10.6723 - val_loss: 142.0740 - val_mse: 142.0740 - val_mae: 8.9087\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 210.7106 - mse: 210.7106 - mae: 10.6335 - val_loss: 140.7001 - val_mse: 140.7001 - val_mae: 8.8354\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.4415 - mse: 209.4415 - mae: 10.6159 - val_loss: 138.5376 - val_mse: 138.5376 - val_mae: 8.6847\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 208.3321 - mse: 208.3321 - mae: 10.5072 - val_loss: 138.8026 - val_mse: 138.8026 - val_mae: 8.7027\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 207.8439 - mse: 207.8439 - mae: 10.5019 - val_loss: 137.3078 - val_mse: 137.3078 - val_mae: 8.5944\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 207.2565 - mse: 207.2565 - mae: 10.4616 - val_loss: 136.5552 - val_mse: 136.5552 - val_mae: 8.5364\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 206.3198 - mse: 206.3198 - mae: 10.4189 - val_loss: 136.6629 - val_mse: 136.6629 - val_mae: 8.5371\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 205.5935 - mse: 205.5935 - mae: 10.4328 - val_loss: 134.5555 - val_mse: 134.5555 - val_mae: 8.3861\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 205.1655 - mse: 205.1655 - mae: 10.3688 - val_loss: 133.8194 - val_mse: 133.8194 - val_mae: 8.3281\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 204.8740 - mse: 204.8740 - mae: 10.3010 - val_loss: 135.1447 - val_mse: 135.1447 - val_mae: 8.4335\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 204.6508 - mse: 204.6508 - mae: 10.3800 - val_loss: 134.4079 - val_mse: 134.4079 - val_mae: 8.3756\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 203.9582 - mse: 203.9582 - mae: 10.3262 - val_loss: 134.1382 - val_mse: 134.1382 - val_mae: 8.3603\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 203.8846 - mse: 203.8846 - mae: 10.3055 - val_loss: 133.9384 - val_mse: 133.9384 - val_mae: 8.3535\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 203.6264 - mse: 203.6264 - mae: 10.3308 - val_loss: 133.2710 - val_mse: 133.2710 - val_mae: 8.2975\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 203.3549 - mse: 203.3549 - mae: 10.3113 - val_loss: 132.2346 - val_mse: 132.2346 - val_mae: 8.2259\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 203.3876 - mse: 203.3876 - mae: 10.2807 - val_loss: 131.9776 - val_mse: 131.9776 - val_mae: 8.1993\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 203.0047 - mse: 203.0047 - mae: 10.2552 - val_loss: 132.4071 - val_mse: 132.4071 - val_mae: 8.2409\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 203.0580 - mse: 203.0580 - mae: 10.2551 - val_loss: 133.5261 - val_mse: 133.5261 - val_mae: 8.3186\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 202.8736 - mse: 202.8736 - mae: 10.3407 - val_loss: 131.7885 - val_mse: 131.7885 - val_mae: 8.1923\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 202.6471 - mse: 202.6471 - mae: 10.2646 - val_loss: 131.8043 - val_mse: 131.8043 - val_mae: 8.1912\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 202.6212 - mse: 202.6212 - mae: 10.2709 - val_loss: 131.2372 - val_mse: 131.2372 - val_mae: 8.1519\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 202.3087 - mse: 202.3087 - mae: 10.2603 - val_loss: 130.8678 - val_mse: 130.8678 - val_mae: 8.1234\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 202.4463 - mse: 202.4463 - mae: 10.2707 - val_loss: 131.0176 - val_mse: 131.0176 - val_mae: 8.1363\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 202.1260 - mse: 202.1260 - mae: 10.2434 - val_loss: 130.7073 - val_mse: 130.7073 - val_mae: 8.1092\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 202.6881 - mse: 202.6881 - mae: 10.2292 - val_loss: 131.1062 - val_mse: 131.1062 - val_mae: 8.1428\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 201.9963 - mse: 201.9963 - mae: 10.2304 - val_loss: 131.6435 - val_mse: 131.6435 - val_mae: 8.1849\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 201.8628 - mse: 201.8628 - mae: 10.2212 - val_loss: 133.1377 - val_mse: 133.1377 - val_mae: 8.2876\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 202.0545 - mse: 202.0545 - mae: 10.2749 - val_loss: 131.4428 - val_mse: 131.4428 - val_mae: 8.1617\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.6082 - mse: 201.6082 - mae: 10.2015 - val_loss: 131.9510 - val_mse: 131.9510 - val_mae: 8.2075\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 202.0042 - mse: 202.0042 - mae: 10.2598 - val_loss: 131.1648 - val_mse: 131.1648 - val_mae: 8.1508\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.7146 - mse: 201.7146 - mae: 10.2023 - val_loss: 132.1322 - val_mse: 132.1322 - val_mae: 8.2248\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.5786 - mse: 201.5786 - mae: 10.3010 - val_loss: 129.4759 - val_mse: 129.4759 - val_mae: 8.0126\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 201.7074 - mse: 201.7074 - mae: 10.1553 - val_loss: 131.1588 - val_mse: 131.1588 - val_mae: 8.1561\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 201.3104 - mse: 201.3104 - mae: 10.2135 - val_loss: 130.7463 - val_mse: 130.7463 - val_mae: 8.1263\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 201.2125 - mse: 201.2125 - mae: 10.1893 - val_loss: 130.6535 - val_mse: 130.6535 - val_mae: 8.1229\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.4436 - mse: 201.4436 - mae: 10.1927 - val_loss: 132.0562 - val_mse: 132.0562 - val_mae: 8.2166\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.2831 - mse: 201.2831 - mae: 10.2531 - val_loss: 129.6260 - val_mse: 129.6260 - val_mae: 8.0286\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 201.2547 - mse: 201.2547 - mae: 10.1897 - val_loss: 130.2674 - val_mse: 130.2674 - val_mae: 8.0897\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.7193 - mse: 201.7193 - mae: 10.1793 - val_loss: 130.2847 - val_mse: 130.2847 - val_mae: 8.0910\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 200.8606 - mse: 200.8606 - mae: 10.2139 - val_loss: 129.1966 - val_mse: 129.1966 - val_mae: 8.0039\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.0207 - mse: 201.0207 - mae: 10.1583 - val_loss: 129.6601 - val_mse: 129.6601 - val_mae: 8.0485\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 201.0553 - mse: 201.0553 - mae: 10.1522 - val_loss: 130.9198 - val_mse: 130.9198 - val_mae: 8.1494\n",
            "Epoch 64: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 523.7853 - mse: 523.7853 - mae: 18.9582 - val_loss: 300.4977 - val_mse: 300.4977 - val_mae: 14.6034\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 380.5271 - mse: 380.5271 - mae: 15.8809 - val_loss: 247.7373 - val_mse: 247.7373 - val_mae: 12.9944\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 344.9269 - mse: 344.9269 - mae: 14.8907 - val_loss: 227.0189 - val_mse: 227.0189 - val_mae: 12.3305\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 325.4832 - mse: 325.4832 - mae: 14.3746 - val_loss: 213.6949 - val_mse: 213.6949 - val_mae: 11.8248\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 314.2455 - mse: 314.2455 - mae: 14.0303 - val_loss: 205.2818 - val_mse: 205.2818 - val_mae: 11.4870\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 307.2293 - mse: 307.2293 - mae: 13.8078 - val_loss: 198.0940 - val_mse: 198.0940 - val_mae: 11.1624\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 301.8597 - mse: 301.8597 - mae: 13.5778 - val_loss: 194.8113 - val_mse: 194.8113 - val_mae: 11.0418\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 298.2546 - mse: 298.2546 - mae: 13.4517 - val_loss: 193.0480 - val_mse: 193.0480 - val_mae: 10.9689\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 294.0982 - mse: 294.0982 - mae: 13.3259 - val_loss: 192.0174 - val_mse: 192.0174 - val_mae: 10.9276\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 291.7040 - mse: 291.7040 - mae: 13.2715 - val_loss: 188.6288 - val_mse: 188.6288 - val_mae: 10.7625\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 289.6176 - mse: 289.6176 - mae: 13.1934 - val_loss: 185.7316 - val_mse: 185.7316 - val_mae: 10.6045\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 288.0299 - mse: 288.0299 - mae: 13.0915 - val_loss: 185.3201 - val_mse: 185.3201 - val_mae: 10.6074\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 285.7638 - mse: 285.7638 - mae: 13.0530 - val_loss: 183.0151 - val_mse: 183.0151 - val_mae: 10.5371\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 282.0334 - mse: 282.0334 - mae: 12.9749 - val_loss: 176.9679 - val_mse: 176.9679 - val_mae: 10.3612\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 274.7328 - mse: 274.7328 - mae: 12.7583 - val_loss: 173.6093 - val_mse: 173.6093 - val_mae: 10.2866\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 270.1156 - mse: 270.1156 - mae: 12.6473 - val_loss: 169.8881 - val_mse: 169.8881 - val_mae: 10.1292\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 267.7019 - mse: 267.7019 - mae: 12.5140 - val_loss: 169.9994 - val_mse: 169.9994 - val_mae: 10.1555\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 266.1258 - mse: 266.1258 - mae: 12.4784 - val_loss: 168.3868 - val_mse: 168.3868 - val_mae: 10.0751\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.4656 - mse: 264.4656 - mae: 12.4028 - val_loss: 167.9431 - val_mse: 167.9431 - val_mae: 10.0553\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 263.7780 - mse: 263.7780 - mae: 12.3963 - val_loss: 166.2192 - val_mse: 166.2192 - val_mae: 9.9740\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262.3540 - mse: 262.3540 - mae: 12.3741 - val_loss: 163.4302 - val_mse: 163.4302 - val_mae: 9.8337\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 261.9385 - mse: 261.9385 - mae: 12.2635 - val_loss: 163.5957 - val_mse: 163.5957 - val_mae: 9.8395\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 260.8581 - mse: 260.8581 - mae: 12.2416 - val_loss: 164.5115 - val_mse: 164.5115 - val_mae: 9.8779\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.7671 - mse: 260.7671 - mae: 12.2788 - val_loss: 162.9175 - val_mse: 162.9175 - val_mae: 9.7916\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 259.8353 - mse: 259.8353 - mae: 12.2321 - val_loss: 161.6536 - val_mse: 161.6536 - val_mae: 9.7308\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 259.5337 - mse: 259.5337 - mae: 12.1901 - val_loss: 162.5761 - val_mse: 162.5761 - val_mae: 9.7775\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 258.9743 - mse: 258.9743 - mae: 12.1811 - val_loss: 160.0098 - val_mse: 160.0098 - val_mae: 9.6428\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 258.9608 - mse: 258.9608 - mae: 12.1646 - val_loss: 159.4096 - val_mse: 159.4096 - val_mae: 9.6082\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 257.8162 - mse: 257.8162 - mae: 12.1056 - val_loss: 159.5424 - val_mse: 159.5424 - val_mae: 9.6171\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 257.2954 - mse: 257.2954 - mae: 12.0836 - val_loss: 160.0901 - val_mse: 160.0901 - val_mae: 9.6383\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 257.2942 - mse: 257.2942 - mae: 12.1264 - val_loss: 157.7412 - val_mse: 157.7412 - val_mae: 9.5058\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 257.0331 - mse: 257.0331 - mae: 12.0249 - val_loss: 162.0429 - val_mse: 162.0429 - val_mae: 9.7407\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.2625 - mse: 256.2625 - mae: 12.1126 - val_loss: 159.5728 - val_mse: 159.5728 - val_mae: 9.6060\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 255.7319 - mse: 255.7319 - mae: 12.0522 - val_loss: 160.2857 - val_mse: 160.2857 - val_mae: 9.6450\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 255.4409 - mse: 255.4409 - mae: 12.0579 - val_loss: 159.1382 - val_mse: 159.1382 - val_mae: 9.5791\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 255.0549 - mse: 255.0549 - mae: 12.0684 - val_loss: 156.8496 - val_mse: 156.8496 - val_mae: 9.4596\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 255.0614 - mse: 255.0614 - mae: 12.0131 - val_loss: 156.5758 - val_mse: 156.5758 - val_mae: 9.4402\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 254.6513 - mse: 254.6513 - mae: 11.9870 - val_loss: 155.9818 - val_mse: 155.9818 - val_mae: 9.4079\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 254.0164 - mse: 254.0164 - mae: 11.9877 - val_loss: 155.7231 - val_mse: 155.7231 - val_mae: 9.3942\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 253.6774 - mse: 253.6774 - mae: 11.9174 - val_loss: 158.1415 - val_mse: 158.1415 - val_mae: 9.5285\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 253.7012 - mse: 253.7012 - mae: 12.0304 - val_loss: 154.7368 - val_mse: 154.7368 - val_mae: 9.3306\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 253.1601 - mse: 253.1601 - mae: 11.9460 - val_loss: 154.2235 - val_mse: 154.2235 - val_mae: 9.3053\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 252.9399 - mse: 252.9399 - mae: 11.9098 - val_loss: 154.9195 - val_mse: 154.9195 - val_mae: 9.3461\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 252.5737 - mse: 252.5737 - mae: 11.9212 - val_loss: 154.6566 - val_mse: 154.6566 - val_mae: 9.3359\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 252.4831 - mse: 252.4831 - mae: 12.0084 - val_loss: 152.8494 - val_mse: 152.8494 - val_mae: 9.2226\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 252.4624 - mse: 252.4624 - mae: 11.8636 - val_loss: 153.4133 - val_mse: 153.4133 - val_mae: 9.2551\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 252.1302 - mse: 252.1302 - mae: 11.8668 - val_loss: 153.7329 - val_mse: 153.7329 - val_mae: 9.2896\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 251.8795 - mse: 251.8795 - mae: 11.8839 - val_loss: 153.8169 - val_mse: 153.8169 - val_mae: 9.3003\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 251.4257 - mse: 251.4257 - mae: 11.8161 - val_loss: 158.3228 - val_mse: 158.3228 - val_mae: 9.5449\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.6460 - mse: 251.6460 - mae: 11.9596 - val_loss: 152.9508 - val_mse: 152.9508 - val_mae: 9.2295\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.1397 - mse: 251.1397 - mae: 11.8544 - val_loss: 153.2618 - val_mse: 153.2618 - val_mae: 9.2495\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.6127 - mse: 251.6127 - mae: 11.8808 - val_loss: 153.4277 - val_mse: 153.4277 - val_mae: 9.2541\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 250.5002 - mse: 250.5002 - mae: 11.8368 - val_loss: 153.2693 - val_mse: 153.2693 - val_mae: 9.2475\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 250.5687 - mse: 250.5687 - mae: 11.8724 - val_loss: 151.3240 - val_mse: 151.3240 - val_mae: 9.1331\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 250.1896 - mse: 250.1896 - mae: 11.7689 - val_loss: 154.0823 - val_mse: 154.0823 - val_mae: 9.2887\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 250.3537 - mse: 250.3537 - mae: 11.8597 - val_loss: 152.3099 - val_mse: 152.3099 - val_mae: 9.1845\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.7614 - mse: 249.7614 - mae: 11.7954 - val_loss: 152.7334 - val_mse: 152.7334 - val_mae: 9.2030\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.5087 - mse: 249.5087 - mae: 11.8090 - val_loss: 152.0747 - val_mse: 152.0747 - val_mae: 9.1783\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.6192 - mse: 249.6192 - mae: 11.7776 - val_loss: 152.5416 - val_mse: 152.5416 - val_mae: 9.2030\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.5620 - mse: 249.5620 - mae: 11.7964 - val_loss: 152.6987 - val_mse: 152.6987 - val_mae: 9.2034\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 249.5196 - mse: 249.5196 - mae: 11.8220 - val_loss: 151.0560 - val_mse: 151.0560 - val_mae: 9.1023\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 248.7805 - mse: 248.7805 - mae: 11.7762 - val_loss: 150.0096 - val_mse: 150.0096 - val_mae: 9.0365\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 248.7115 - mse: 248.7115 - mae: 11.7551 - val_loss: 149.8563 - val_mse: 149.8563 - val_mae: 9.0284\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 248.4954 - mse: 248.4954 - mae: 11.7330 - val_loss: 150.1605 - val_mse: 150.1605 - val_mae: 9.0470\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 248.5906 - mse: 248.5906 - mae: 11.6826 - val_loss: 151.1384 - val_mse: 151.1384 - val_mae: 9.1063\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 248.1514 - mse: 248.1514 - mae: 11.7194 - val_loss: 151.8080 - val_mse: 151.8080 - val_mae: 9.1540\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 248.0297 - mse: 248.0297 - mae: 11.7688 - val_loss: 149.6568 - val_mse: 149.6568 - val_mae: 9.0160\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 248.1074 - mse: 248.1074 - mae: 11.7102 - val_loss: 150.7459 - val_mse: 150.7459 - val_mae: 9.0882\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 248.0986 - mse: 248.0986 - mae: 11.7553 - val_loss: 150.2994 - val_mse: 150.2994 - val_mae: 9.0467\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 247.5610 - mse: 247.5610 - mae: 11.6828 - val_loss: 150.2977 - val_mse: 150.2977 - val_mae: 9.0477\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 247.4120 - mse: 247.4120 - mae: 11.7541 - val_loss: 148.3997 - val_mse: 148.3997 - val_mae: 8.9115\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 247.6694 - mse: 247.6694 - mae: 11.6380 - val_loss: 149.6541 - val_mse: 149.6541 - val_mae: 9.0144\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 247.3501 - mse: 247.3501 - mae: 11.6874 - val_loss: 149.4987 - val_mse: 149.4987 - val_mae: 9.0036\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 246.8028 - mse: 246.8028 - mae: 11.6440 - val_loss: 151.9280 - val_mse: 151.9280 - val_mae: 9.1638\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 247.0976 - mse: 247.0976 - mae: 11.7307 - val_loss: 150.3839 - val_mse: 150.3839 - val_mae: 9.0593\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 247.0670 - mse: 247.0670 - mae: 11.6435 - val_loss: 150.2886 - val_mse: 150.2886 - val_mae: 9.0545\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 246.4789 - mse: 246.4789 - mae: 11.6616 - val_loss: 150.8588 - val_mse: 150.8588 - val_mae: 9.0864\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 245.9923 - mse: 245.9923 - mae: 11.7306 - val_loss: 147.6298 - val_mse: 147.6298 - val_mae: 8.8444\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 246.1429 - mse: 246.1429 - mae: 11.5883 - val_loss: 148.4758 - val_mse: 148.4758 - val_mae: 8.9158\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 246.0394 - mse: 246.0394 - mae: 11.6343 - val_loss: 148.6582 - val_mse: 148.6582 - val_mae: 8.9362\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 245.7183 - mse: 245.7183 - mae: 11.6314 - val_loss: 147.4441 - val_mse: 147.4441 - val_mae: 8.8315\n",
            "Epoch 81: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 672.9058 - mse: 672.9058 - mae: 20.2549 - val_loss: 358.9569 - val_mse: 358.9569 - val_mae: 16.1213\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 359.1076 - mse: 359.1076 - mae: 15.7544 - val_loss: 329.3941 - val_mse: 329.3941 - val_mae: 15.0057\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 341.0456 - mse: 341.0456 - mae: 15.1162 - val_loss: 322.6731 - val_mse: 322.6731 - val_mae: 14.6944\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 334.6208 - mse: 334.6208 - mae: 14.8586 - val_loss: 320.0373 - val_mse: 320.0373 - val_mae: 14.6051\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 331.2375 - mse: 331.2375 - mae: 14.7555 - val_loss: 319.2582 - val_mse: 319.2582 - val_mae: 14.5390\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 328.5260 - mse: 328.5260 - mae: 14.6454 - val_loss: 317.0193 - val_mse: 317.0193 - val_mae: 14.4386\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 326.7378 - mse: 326.7378 - mae: 14.5853 - val_loss: 315.8106 - val_mse: 315.8106 - val_mae: 14.4181\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 324.7714 - mse: 324.7714 - mae: 14.5406 - val_loss: 316.0254 - val_mse: 316.0254 - val_mae: 14.3841\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 323.0776 - mse: 323.0776 - mae: 14.4427 - val_loss: 314.2402 - val_mse: 314.2402 - val_mae: 14.3210\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 321.8017 - mse: 321.8017 - mae: 14.4184 - val_loss: 313.4394 - val_mse: 313.4394 - val_mae: 14.2733\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 320.5482 - mse: 320.5482 - mae: 14.3566 - val_loss: 312.5788 - val_mse: 312.5788 - val_mae: 14.2589\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 319.0942 - mse: 319.0942 - mae: 14.3153 - val_loss: 310.2837 - val_mse: 310.2837 - val_mae: 14.1347\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 317.4771 - mse: 317.4771 - mae: 14.2357 - val_loss: 308.0695 - val_mse: 308.0695 - val_mae: 14.0897\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 315.9887 - mse: 315.9887 - mae: 14.1787 - val_loss: 306.7211 - val_mse: 306.7211 - val_mae: 14.0413\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 314.4423 - mse: 314.4423 - mae: 14.1561 - val_loss: 306.0000 - val_mse: 306.0000 - val_mae: 13.9839\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 313.5963 - mse: 313.5963 - mae: 14.1126 - val_loss: 304.9137 - val_mse: 304.9137 - val_mae: 14.0062\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 312.1824 - mse: 312.1824 - mae: 14.1089 - val_loss: 304.3769 - val_mse: 304.3769 - val_mae: 13.9080\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 311.5724 - mse: 311.5724 - mae: 14.0528 - val_loss: 302.9887 - val_mse: 302.9887 - val_mae: 13.9117\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 310.1396 - mse: 310.1396 - mae: 14.0174 - val_loss: 302.6409 - val_mse: 302.6409 - val_mae: 13.9238\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 308.6821 - mse: 308.6821 - mae: 13.9679 - val_loss: 301.1103 - val_mse: 301.1103 - val_mae: 13.8474\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 306.2344 - mse: 306.2344 - mae: 13.9214 - val_loss: 298.9111 - val_mse: 298.9111 - val_mae: 13.8069\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 302.0380 - mse: 302.0380 - mae: 13.7500 - val_loss: 295.2163 - val_mse: 295.2163 - val_mae: 13.7082\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 298.5349 - mse: 298.5349 - mae: 13.6379 - val_loss: 291.7037 - val_mse: 291.7037 - val_mae: 13.6360\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 295.5723 - mse: 295.5723 - mae: 13.5488 - val_loss: 289.9073 - val_mse: 289.9073 - val_mae: 13.5814\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 292.3190 - mse: 292.3190 - mae: 13.4988 - val_loss: 286.2367 - val_mse: 286.2367 - val_mae: 13.4908\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 288.9315 - mse: 288.9315 - mae: 13.3910 - val_loss: 284.5825 - val_mse: 284.5825 - val_mae: 13.5114\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 287.1088 - mse: 287.1088 - mae: 13.3796 - val_loss: 282.6538 - val_mse: 282.6538 - val_mae: 13.4342\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 285.9053 - mse: 285.9053 - mae: 13.3199 - val_loss: 282.1942 - val_mse: 282.1942 - val_mae: 13.3844\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 284.2216 - mse: 284.2216 - mae: 13.2506 - val_loss: 279.6747 - val_mse: 279.6747 - val_mae: 13.3006\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 283.1174 - mse: 283.1174 - mae: 13.2142 - val_loss: 278.6680 - val_mse: 278.6680 - val_mae: 13.2881\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 282.0035 - mse: 282.0035 - mae: 13.1824 - val_loss: 278.2065 - val_mse: 278.2065 - val_mae: 13.2625\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 281.1127 - mse: 281.1127 - mae: 13.1224 - val_loss: 277.7146 - val_mse: 277.7146 - val_mae: 13.2572\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 280.5214 - mse: 280.5214 - mae: 13.1315 - val_loss: 276.6583 - val_mse: 276.6583 - val_mae: 13.1967\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 279.8769 - mse: 279.8769 - mae: 13.0839 - val_loss: 274.5166 - val_mse: 274.5166 - val_mae: 13.0953\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 278.4698 - mse: 278.4698 - mae: 13.0214 - val_loss: 273.7536 - val_mse: 273.7536 - val_mae: 13.1007\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 278.2125 - mse: 278.2125 - mae: 13.0219 - val_loss: 272.9771 - val_mse: 272.9771 - val_mae: 13.0354\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 277.5408 - mse: 277.5408 - mae: 13.0091 - val_loss: 271.9250 - val_mse: 271.9250 - val_mae: 13.0075\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 277.4843 - mse: 277.4843 - mae: 12.9833 - val_loss: 271.4206 - val_mse: 271.4206 - val_mae: 13.0086\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 276.7289 - mse: 276.7289 - mae: 12.9562 - val_loss: 270.5986 - val_mse: 270.5986 - val_mae: 12.9514\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 275.1963 - mse: 275.1963 - mae: 12.8966 - val_loss: 270.2703 - val_mse: 270.2703 - val_mae: 12.9478\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 275.2975 - mse: 275.2975 - mae: 12.9170 - val_loss: 269.2399 - val_mse: 269.2399 - val_mae: 12.8982\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 274.0800 - mse: 274.0800 - mae: 12.8185 - val_loss: 273.3586 - val_mse: 273.3586 - val_mae: 13.1524\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 274.7954 - mse: 274.7954 - mae: 12.9388 - val_loss: 267.9498 - val_mse: 267.9498 - val_mae: 12.8636\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 273.7334 - mse: 273.7334 - mae: 12.8317 - val_loss: 267.1361 - val_mse: 267.1361 - val_mae: 12.8080\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 272.9555 - mse: 272.9555 - mae: 12.7950 - val_loss: 266.9290 - val_mse: 266.9290 - val_mae: 12.8307\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 272.2192 - mse: 272.2192 - mae: 12.7782 - val_loss: 266.2982 - val_mse: 266.2982 - val_mae: 12.7821\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 271.7401 - mse: 271.7401 - mae: 12.7736 - val_loss: 266.0683 - val_mse: 266.0683 - val_mae: 12.7441\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 271.7726 - mse: 271.7726 - mae: 12.7511 - val_loss: 265.4384 - val_mse: 265.4384 - val_mae: 12.7611\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 271.2210 - mse: 271.2210 - mae: 12.7439 - val_loss: 265.0773 - val_mse: 265.0773 - val_mae: 12.7410\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 271.0613 - mse: 271.0613 - mae: 12.7156 - val_loss: 264.9204 - val_mse: 264.9204 - val_mae: 12.7171\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 270.2992 - mse: 270.2992 - mae: 12.7069 - val_loss: 263.3657 - val_mse: 263.3657 - val_mae: 12.6341\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 270.6803 - mse: 270.6803 - mae: 12.6892 - val_loss: 263.3117 - val_mse: 263.3117 - val_mae: 12.6373\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 269.5300 - mse: 269.5300 - mae: 12.6377 - val_loss: 264.5130 - val_mse: 264.5130 - val_mae: 12.7220\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 269.2986 - mse: 269.2986 - mae: 12.6522 - val_loss: 263.3423 - val_mse: 263.3423 - val_mae: 12.6663\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 269.5634 - mse: 269.5634 - mae: 12.6631 - val_loss: 262.1453 - val_mse: 262.1453 - val_mae: 12.5976\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 269.0235 - mse: 269.0235 - mae: 12.6386 - val_loss: 261.8424 - val_mse: 261.8424 - val_mae: 12.5862\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 268.6201 - mse: 268.6201 - mae: 12.6089 - val_loss: 261.2047 - val_mse: 261.2047 - val_mae: 12.5598\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 268.3573 - mse: 268.3573 - mae: 12.6001 - val_loss: 261.1432 - val_mse: 261.1432 - val_mae: 12.5490\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 267.8890 - mse: 267.8890 - mae: 12.5740 - val_loss: 260.8262 - val_mse: 260.8262 - val_mae: 12.5591\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 267.6996 - mse: 267.6996 - mae: 12.5972 - val_loss: 261.3290 - val_mse: 261.3290 - val_mae: 12.5706\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 267.3623 - mse: 267.3623 - mae: 12.5301 - val_loss: 260.6102 - val_mse: 260.6102 - val_mae: 12.5260\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 266.8416 - mse: 266.8416 - mae: 12.5511 - val_loss: 259.8727 - val_mse: 259.8727 - val_mae: 12.4962\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 266.9037 - mse: 266.9037 - mae: 12.5360 - val_loss: 259.8790 - val_mse: 259.8790 - val_mae: 12.4961\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 266.5377 - mse: 266.5377 - mae: 12.5003 - val_loss: 259.3318 - val_mse: 259.3318 - val_mae: 12.4713\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 266.2044 - mse: 266.2044 - mae: 12.5096 - val_loss: 258.8268 - val_mse: 258.8268 - val_mae: 12.4296\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 266.6491 - mse: 266.6491 - mae: 12.5002 - val_loss: 259.1519 - val_mse: 259.1519 - val_mae: 12.4767\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 266.1167 - mse: 266.1167 - mae: 12.5044 - val_loss: 258.6646 - val_mse: 258.6646 - val_mae: 12.4539\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 266.1022 - mse: 266.1022 - mae: 12.5166 - val_loss: 258.3116 - val_mse: 258.3116 - val_mae: 12.4114\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 265.4138 - mse: 265.4138 - mae: 12.4732 - val_loss: 257.7773 - val_mse: 257.7773 - val_mae: 12.4047\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 265.2599 - mse: 265.2599 - mae: 12.4648 - val_loss: 258.0620 - val_mse: 258.0620 - val_mae: 12.4122\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.9416 - mse: 264.9416 - mae: 12.4238 - val_loss: 257.6721 - val_mse: 257.6721 - val_mae: 12.3983\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 264.7132 - mse: 264.7132 - mae: 12.4449 - val_loss: 257.3439 - val_mse: 257.3439 - val_mae: 12.3734\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.4667 - mse: 264.4667 - mae: 12.3729 - val_loss: 257.3149 - val_mse: 257.3149 - val_mae: 12.3964\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 264.5302 - mse: 264.5302 - mae: 12.4351 - val_loss: 256.4666 - val_mse: 256.4666 - val_mae: 12.3291\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.2205 - mse: 264.2205 - mae: 12.3978 - val_loss: 256.5187 - val_mse: 256.5187 - val_mae: 12.3268\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.5128 - mse: 264.5128 - mae: 12.3967 - val_loss: 256.9904 - val_mse: 256.9904 - val_mae: 12.3839\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.2092 - mse: 264.2092 - mae: 12.4322 - val_loss: 255.6831 - val_mse: 255.6831 - val_mae: 12.2881\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.9381 - mse: 263.9381 - mae: 12.3861 - val_loss: 255.7064 - val_mse: 255.7064 - val_mae: 12.3060\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.0825 - mse: 263.0825 - mae: 12.3960 - val_loss: 256.6135 - val_mse: 256.6135 - val_mae: 12.3156\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.6595 - mse: 263.6595 - mae: 12.3178 - val_loss: 254.9779 - val_mse: 254.9779 - val_mae: 12.2347\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.7687 - mse: 263.7687 - mae: 12.3620 - val_loss: 255.2356 - val_mse: 255.2356 - val_mae: 12.2861\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 263.1906 - mse: 263.1906 - mae: 12.3842 - val_loss: 254.9339 - val_mse: 254.9339 - val_mae: 12.2568\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 263.2283 - mse: 263.2283 - mae: 12.3524 - val_loss: 255.1152 - val_mse: 255.1152 - val_mae: 12.2949\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.0401 - mse: 263.0401 - mae: 12.3707 - val_loss: 254.0748 - val_mse: 254.0748 - val_mae: 12.2160\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.4840 - mse: 262.4840 - mae: 12.3335 - val_loss: 254.6844 - val_mse: 254.6844 - val_mae: 12.2284\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 263.1209 - mse: 263.1209 - mae: 12.3471 - val_loss: 254.4345 - val_mse: 254.4345 - val_mae: 12.2141\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 263.1139 - mse: 263.1139 - mae: 12.3452 - val_loss: 253.7427 - val_mse: 253.7427 - val_mae: 12.1791\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 261.9041 - mse: 261.9041 - mae: 12.2861 - val_loss: 253.6788 - val_mse: 253.6788 - val_mae: 12.1860\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 261.8651 - mse: 261.8651 - mae: 12.2792 - val_loss: 253.2221 - val_mse: 253.2221 - val_mae: 12.1587\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262.4395 - mse: 262.4395 - mae: 12.2879 - val_loss: 254.3779 - val_mse: 254.3779 - val_mae: 12.2616\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262.0297 - mse: 262.0297 - mae: 12.3192 - val_loss: 253.0517 - val_mse: 253.0517 - val_mae: 12.1652\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.0398 - mse: 262.0398 - mae: 12.2887 - val_loss: 254.1782 - val_mse: 254.1782 - val_mae: 12.2634\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 261.6951 - mse: 261.6951 - mae: 12.3309 - val_loss: 252.3707 - val_mse: 252.3707 - val_mae: 12.1067\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 261.6321 - mse: 261.6321 - mae: 12.2981 - val_loss: 252.4077 - val_mse: 252.4077 - val_mae: 12.0932\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 261.4728 - mse: 261.4728 - mae: 12.2196 - val_loss: 252.5764 - val_mse: 252.5764 - val_mae: 12.1383\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 261.1717 - mse: 261.1717 - mae: 12.2820 - val_loss: 252.5233 - val_mse: 252.5233 - val_mae: 12.1242\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 261.0011 - mse: 261.0011 - mae: 12.2261 - val_loss: 252.0581 - val_mse: 252.0581 - val_mae: 12.0849\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.7672 - mse: 260.7672 - mae: 12.2201 - val_loss: 251.8358 - val_mse: 251.8358 - val_mae: 12.0705\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 260.8666 - mse: 260.8666 - mae: 12.2224 - val_loss: 251.7961 - val_mse: 251.7961 - val_mae: 12.0730\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.4856 - mse: 260.4856 - mae: 12.2200 - val_loss: 252.3113 - val_mse: 252.3113 - val_mae: 12.1025\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 27ms/step - loss: 471.6833 - mse: 471.6833 - mae: 18.3216 - val_loss: 286.4529 - val_mse: 286.4529 - val_mae: 14.7382\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 371.6717 - mse: 371.6717 - mae: 16.0282 - val_loss: 242.0379 - val_mse: 242.0379 - val_mae: 13.0225\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 318.5189 - mse: 318.5189 - mae: 14.3904 - val_loss: 216.7021 - val_mse: 216.7021 - val_mae: 11.9329\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 291.6957 - mse: 291.6957 - mae: 13.3584 - val_loss: 193.2809 - val_mse: 193.2809 - val_mae: 10.9772\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 273.6739 - mse: 273.6739 - mae: 12.6741 - val_loss: 182.8909 - val_mse: 182.8909 - val_mae: 10.5188\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.3328 - mse: 260.3328 - mae: 12.2614 - val_loss: 170.5217 - val_mse: 170.5217 - val_mae: 9.9515\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 248.6633 - mse: 248.6633 - mae: 11.7527 - val_loss: 163.1371 - val_mse: 163.1371 - val_mae: 9.5735\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 242.4466 - mse: 242.4466 - mae: 11.5397 - val_loss: 159.6762 - val_mse: 159.6762 - val_mae: 9.4299\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 239.3580 - mse: 239.3580 - mae: 11.4914 - val_loss: 157.6152 - val_mse: 157.6152 - val_mae: 9.3101\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 236.5342 - mse: 236.5342 - mae: 11.4073 - val_loss: 154.5838 - val_mse: 154.5838 - val_mae: 9.1588\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 233.4222 - mse: 233.4222 - mae: 11.1779 - val_loss: 152.4931 - val_mse: 152.4931 - val_mae: 9.0141\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 230.8599 - mse: 230.8599 - mae: 11.1480 - val_loss: 151.0127 - val_mse: 151.0127 - val_mae: 9.0191\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 228.2934 - mse: 228.2934 - mae: 11.0252 - val_loss: 148.2511 - val_mse: 148.2511 - val_mae: 8.8206\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 227.2459 - mse: 227.2459 - mae: 11.1185 - val_loss: 148.6053 - val_mse: 148.6053 - val_mae: 8.8704\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 226.0898 - mse: 226.0898 - mae: 10.9100 - val_loss: 146.7286 - val_mse: 146.7286 - val_mae: 8.7078\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 225.6798 - mse: 225.6798 - mae: 10.9279 - val_loss: 147.6356 - val_mse: 147.6356 - val_mae: 8.8083\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 224.8312 - mse: 224.8312 - mae: 10.8808 - val_loss: 145.7717 - val_mse: 145.7717 - val_mae: 8.6445\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 223.9844 - mse: 223.9844 - mae: 10.8325 - val_loss: 146.2226 - val_mse: 146.2226 - val_mae: 8.7516\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 222.9896 - mse: 222.9896 - mae: 10.8061 - val_loss: 144.3763 - val_mse: 144.3763 - val_mae: 8.6530\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 221.2966 - mse: 221.2966 - mae: 10.8740 - val_loss: 144.4007 - val_mse: 144.4007 - val_mae: 8.6782\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 220.9744 - mse: 220.9744 - mae: 10.8714 - val_loss: 142.8383 - val_mse: 142.8383 - val_mae: 8.5345\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 220.0151 - mse: 220.0151 - mae: 10.6641 - val_loss: 142.4548 - val_mse: 142.4548 - val_mae: 8.5192\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 219.8545 - mse: 219.8545 - mae: 10.6525 - val_loss: 143.1071 - val_mse: 143.1071 - val_mae: 8.5911\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 219.7138 - mse: 219.7138 - mae: 10.8345 - val_loss: 142.8154 - val_mse: 142.8154 - val_mae: 8.5658\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 219.1708 - mse: 219.1708 - mae: 10.7333 - val_loss: 141.6049 - val_mse: 141.6049 - val_mae: 8.4501\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 218.6456 - mse: 218.6456 - mae: 10.5816 - val_loss: 140.5617 - val_mse: 140.5617 - val_mae: 8.4140\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 216.4217 - mse: 216.4217 - mae: 10.6164 - val_loss: 140.4825 - val_mse: 140.4825 - val_mae: 8.5045\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.0410 - mse: 215.0410 - mae: 10.6083 - val_loss: 138.4617 - val_mse: 138.4617 - val_mae: 8.3360\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 214.4770 - mse: 214.4770 - mae: 10.4692 - val_loss: 138.3614 - val_mse: 138.3614 - val_mae: 8.3331\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 214.3281 - mse: 214.3281 - mae: 10.6121 - val_loss: 138.5001 - val_mse: 138.5001 - val_mae: 8.3430\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.8296 - mse: 213.8296 - mae: 10.4395 - val_loss: 137.3427 - val_mse: 137.3427 - val_mae: 8.2359\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 213.7358 - mse: 213.7358 - mae: 10.4023 - val_loss: 137.1427 - val_mse: 137.1427 - val_mae: 8.2140\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.7856 - mse: 213.7856 - mae: 10.5585 - val_loss: 138.6456 - val_mse: 138.6456 - val_mae: 8.3816\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.3764 - mse: 213.3764 - mae: 10.4715 - val_loss: 136.7888 - val_mse: 136.7888 - val_mae: 8.1848\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.2147 - mse: 213.2147 - mae: 10.4526 - val_loss: 137.5240 - val_mse: 137.5240 - val_mae: 8.2740\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.8675 - mse: 212.8675 - mae: 10.4868 - val_loss: 137.1885 - val_mse: 137.1885 - val_mae: 8.2402\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 212.7201 - mse: 212.7201 - mae: 10.4227 - val_loss: 137.0358 - val_mse: 137.0358 - val_mae: 8.2229\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.8275 - mse: 212.8275 - mae: 10.3931 - val_loss: 137.3257 - val_mse: 137.3257 - val_mae: 8.2528\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.4377 - mse: 212.4377 - mae: 10.4117 - val_loss: 136.9909 - val_mse: 136.9909 - val_mae: 8.2252\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.8794 - mse: 212.8794 - mae: 10.3828 - val_loss: 136.7175 - val_mse: 136.7175 - val_mae: 8.2022\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.9714 - mse: 212.9714 - mae: 10.5791 - val_loss: 138.4073 - val_mse: 138.4073 - val_mae: 8.3751\n",
            "Epoch 41: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 499.6533 - mse: 499.6533 - mae: 18.4842 - val_loss: 274.0308 - val_mse: 274.0308 - val_mae: 14.2871\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 386.1269 - mse: 386.1269 - mae: 16.1516 - val_loss: 240.0491 - val_mse: 240.0491 - val_mae: 12.9293\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 333.5674 - mse: 333.5674 - mae: 14.6378 - val_loss: 203.6535 - val_mse: 203.6535 - val_mae: 11.6305\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 297.3501 - mse: 297.3501 - mae: 13.4255 - val_loss: 177.4862 - val_mse: 177.4862 - val_mae: 10.4770\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 268.9082 - mse: 268.9082 - mae: 12.5660 - val_loss: 165.3678 - val_mse: 165.3678 - val_mae: 9.8897\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 259.5346 - mse: 259.5346 - mae: 12.2831 - val_loss: 155.3174 - val_mse: 155.3174 - val_mae: 9.4295\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 250.8051 - mse: 250.8051 - mae: 11.7959 - val_loss: 149.7891 - val_mse: 149.7891 - val_mae: 9.1681\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 245.2987 - mse: 245.2987 - mae: 11.6880 - val_loss: 148.5469 - val_mse: 148.5469 - val_mae: 9.1007\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 241.3875 - mse: 241.3875 - mae: 11.6655 - val_loss: 142.5329 - val_mse: 142.5329 - val_mae: 8.8571\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 234.1483 - mse: 234.1483 - mae: 11.4438 - val_loss: 137.5631 - val_mse: 137.5631 - val_mae: 8.6684\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 227.7711 - mse: 227.7711 - mae: 11.2099 - val_loss: 135.5089 - val_mse: 135.5089 - val_mae: 8.5782\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 225.2238 - mse: 225.2238 - mae: 11.0380 - val_loss: 133.0743 - val_mse: 133.0743 - val_mae: 8.4344\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 223.4620 - mse: 223.4620 - mae: 11.0702 - val_loss: 133.6805 - val_mse: 133.6805 - val_mae: 8.4924\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 222.8394 - mse: 222.8394 - mae: 11.0969 - val_loss: 130.3798 - val_mse: 130.3798 - val_mae: 8.2502\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 222.2960 - mse: 222.2960 - mae: 10.8282 - val_loss: 131.2169 - val_mse: 131.2169 - val_mae: 8.3347\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 221.9240 - mse: 221.9240 - mae: 11.1509 - val_loss: 130.1992 - val_mse: 130.1992 - val_mae: 8.2470\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 221.3035 - mse: 221.3035 - mae: 10.7696 - val_loss: 128.8403 - val_mse: 128.8403 - val_mae: 8.1508\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 219.9290 - mse: 219.9290 - mae: 10.9687 - val_loss: 130.9461 - val_mse: 130.9461 - val_mae: 8.3352\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 220.6458 - mse: 220.6458 - mae: 10.8017 - val_loss: 128.7812 - val_mse: 128.7812 - val_mae: 8.1687\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 218.9334 - mse: 218.9334 - mae: 10.9405 - val_loss: 129.4207 - val_mse: 129.4207 - val_mae: 8.2183\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 218.5427 - mse: 218.5427 - mae: 10.9248 - val_loss: 126.7432 - val_mse: 126.7432 - val_mae: 7.9775\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 218.3898 - mse: 218.3898 - mae: 10.5904 - val_loss: 126.9609 - val_mse: 126.9609 - val_mae: 8.0238\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 217.1042 - mse: 217.1042 - mae: 10.7585 - val_loss: 129.0485 - val_mse: 129.0485 - val_mae: 8.2081\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 217.3154 - mse: 217.3154 - mae: 10.8369 - val_loss: 126.5984 - val_mse: 126.5984 - val_mae: 7.9896\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 216.8294 - mse: 216.8294 - mae: 10.7102 - val_loss: 126.0572 - val_mse: 126.0572 - val_mae: 7.9415\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 216.1965 - mse: 216.1965 - mae: 10.6008 - val_loss: 126.6360 - val_mse: 126.6360 - val_mae: 8.0026\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 215.9797 - mse: 215.9797 - mae: 10.7011 - val_loss: 126.7467 - val_mse: 126.7467 - val_mae: 8.0127\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.8656 - mse: 215.8656 - mae: 10.5840 - val_loss: 126.0227 - val_mse: 126.0227 - val_mae: 7.9593\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.1143 - mse: 215.1143 - mae: 10.5856 - val_loss: 126.1522 - val_mse: 126.1522 - val_mae: 7.9756\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.0161 - mse: 215.0161 - mae: 10.5528 - val_loss: 126.0594 - val_mse: 126.0594 - val_mae: 7.9671\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 214.8014 - mse: 214.8014 - mae: 10.6447 - val_loss: 125.1722 - val_mse: 125.1722 - val_mae: 7.8990\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.0618 - mse: 215.0618 - mae: 10.4673 - val_loss: 123.0816 - val_mse: 123.0816 - val_mae: 7.7096\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.9710 - mse: 213.9710 - mae: 10.6793 - val_loss: 126.6457 - val_mse: 126.6457 - val_mae: 8.0768\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.9397 - mse: 212.9397 - mae: 10.4898 - val_loss: 120.9650 - val_mse: 120.9650 - val_mae: 7.6559\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 209.9818 - mse: 209.9818 - mae: 10.4889 - val_loss: 124.1271 - val_mse: 124.1271 - val_mae: 7.9733\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.1987 - mse: 209.1987 - mae: 10.5006 - val_loss: 120.2086 - val_mse: 120.2086 - val_mae: 7.5953\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.2742 - mse: 209.2742 - mae: 10.2649 - val_loss: 122.1536 - val_mse: 122.1536 - val_mae: 7.7970\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 208.5425 - mse: 208.5425 - mae: 10.3810 - val_loss: 120.5322 - val_mse: 120.5322 - val_mae: 7.6439\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 208.4456 - mse: 208.4456 - mae: 10.3626 - val_loss: 120.2956 - val_mse: 120.2956 - val_mae: 7.6294\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 208.9595 - mse: 208.9595 - mae: 10.1737 - val_loss: 120.3624 - val_mse: 120.3624 - val_mae: 7.6447\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.3780 - mse: 209.3780 - mae: 10.6497 - val_loss: 121.6889 - val_mse: 121.6889 - val_mae: 7.7800\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 206.8314 - mse: 206.8314 - mae: 10.1636 - val_loss: 116.9129 - val_mse: 116.9129 - val_mae: 7.4523\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 204.4501 - mse: 204.4501 - mae: 10.1834 - val_loss: 118.9372 - val_mse: 118.9372 - val_mae: 7.7015\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 203.1158 - mse: 203.1158 - mae: 10.2215 - val_loss: 116.3378 - val_mse: 116.3378 - val_mae: 7.4500\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 205.1675 - mse: 205.1675 - mae: 10.3941 - val_loss: 116.5676 - val_mse: 116.5676 - val_mae: 7.4755\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 204.9648 - mse: 204.9648 - mae: 9.9543 - val_loss: 115.6730 - val_mse: 115.6730 - val_mae: 7.3907\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 202.8076 - mse: 202.8076 - mae: 10.1669 - val_loss: 116.5146 - val_mse: 116.5146 - val_mae: 7.4829\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 202.3823 - mse: 202.3823 - mae: 10.1370 - val_loss: 115.4073 - val_mse: 115.4073 - val_mae: 7.3666\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 202.2650 - mse: 202.2650 - mae: 9.9811 - val_loss: 116.5190 - val_mse: 116.5190 - val_mae: 7.4916\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 202.2902 - mse: 202.2902 - mae: 10.3406 - val_loss: 118.4904 - val_mse: 118.4904 - val_mae: 7.6911\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.6477 - mse: 201.6477 - mae: 9.9784 - val_loss: 113.8048 - val_mse: 113.8048 - val_mae: 7.2295\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.3148 - mse: 201.3148 - mae: 10.1486 - val_loss: 115.8784 - val_mse: 115.8784 - val_mae: 7.5856\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 197.5151 - mse: 197.5151 - mae: 9.9287 - val_loss: 110.1291 - val_mse: 110.1291 - val_mae: 7.2070\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.0664 - mse: 195.0664 - mae: 10.0404 - val_loss: 111.9204 - val_mse: 111.9204 - val_mae: 7.5439\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 190.6835 - mse: 190.6835 - mae: 9.6732 - val_loss: 105.1124 - val_mse: 105.1124 - val_mae: 6.9381\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 189.3159 - mse: 189.3159 - mae: 9.8389 - val_loss: 108.6409 - val_mse: 108.6409 - val_mae: 7.2989\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 189.9504 - mse: 189.9504 - mae: 9.6383 - val_loss: 105.5857 - val_mse: 105.5857 - val_mae: 7.0110\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 188.3893 - mse: 188.3893 - mae: 9.7815 - val_loss: 105.5329 - val_mse: 105.5329 - val_mae: 7.0081\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 188.3678 - mse: 188.3678 - mae: 9.5399 - val_loss: 107.0000 - val_mse: 107.0000 - val_mae: 7.1727\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 187.8521 - mse: 187.8521 - mae: 9.7815 - val_loss: 105.8296 - val_mse: 105.8296 - val_mae: 7.0495\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 187.4906 - mse: 187.4906 - mae: 9.6512 - val_loss: 104.2164 - val_mse: 104.2164 - val_mae: 6.8744\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 187.6208 - mse: 187.6208 - mae: 9.6014 - val_loss: 106.0481 - val_mse: 106.0481 - val_mae: 7.0782\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 186.7941 - mse: 186.7941 - mae: 9.6157 - val_loss: 103.3820 - val_mse: 103.3820 - val_mae: 6.7766\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 187.2429 - mse: 187.2429 - mae: 9.4933 - val_loss: 105.5637 - val_mse: 105.5637 - val_mae: 7.0327\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 188.4277 - mse: 188.4277 - mae: 9.5372 - val_loss: 108.3883 - val_mse: 108.3883 - val_mae: 7.3137\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 187.9194 - mse: 187.9194 - mae: 9.7797 - val_loss: 103.2137 - val_mse: 103.2137 - val_mae: 6.7667\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 186.6761 - mse: 186.6761 - mae: 9.5326 - val_loss: 106.6421 - val_mse: 106.6421 - val_mae: 7.1517\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 186.5890 - mse: 186.5890 - mae: 9.6156 - val_loss: 102.7747 - val_mse: 102.7747 - val_mae: 6.7114\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 187.6568 - mse: 187.6568 - mae: 9.6400 - val_loss: 107.9142 - val_mse: 107.9142 - val_mae: 7.2767\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 187.2946 - mse: 187.2946 - mae: 9.5172 - val_loss: 103.4514 - val_mse: 103.4514 - val_mae: 6.8021\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 187.1723 - mse: 187.1723 - mae: 9.7303 - val_loss: 104.1556 - val_mse: 104.1556 - val_mae: 6.8867\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 188.0560 - mse: 188.0560 - mae: 9.3675 - val_loss: 104.6630 - val_mse: 104.6630 - val_mae: 6.9467\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 186.5521 - mse: 186.5521 - mae: 9.7590 - val_loss: 104.0322 - val_mse: 104.0322 - val_mae: 6.8696\n",
            "Epoch 73: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 27ms/step - loss: 441.2950 - mse: 441.2950 - mae: 17.7444 - val_loss: 353.8916 - val_mse: 353.8916 - val_mae: 16.0043\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 350.0389 - mse: 350.0389 - mae: 15.6865 - val_loss: 306.6656 - val_mse: 306.6656 - val_mae: 14.4164\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 303.2200 - mse: 303.2200 - mae: 14.1091 - val_loss: 277.7919 - val_mse: 277.7919 - val_mae: 13.3449\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 278.6060 - mse: 278.6060 - mae: 13.1286 - val_loss: 257.5732 - val_mse: 257.5732 - val_mae: 12.5077\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.1983 - mse: 262.1983 - mae: 12.4723 - val_loss: 246.5800 - val_mse: 246.5800 - val_mae: 12.0974\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 252.6582 - mse: 252.6582 - mae: 12.0649 - val_loss: 238.8203 - val_mse: 238.8203 - val_mae: 11.7681\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 245.5621 - mse: 245.5621 - mae: 11.7416 - val_loss: 233.0293 - val_mse: 233.0293 - val_mae: 11.5567\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 241.6241 - mse: 241.6241 - mae: 11.7467 - val_loss: 228.3978 - val_mse: 228.3978 - val_mae: 11.4278\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 237.1422 - mse: 237.1422 - mae: 11.3949 - val_loss: 225.3609 - val_mse: 225.3609 - val_mae: 11.2352\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 234.1230 - mse: 234.1230 - mae: 11.2781 - val_loss: 222.9866 - val_mse: 222.9866 - val_mae: 11.2168\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 231.6787 - mse: 231.6787 - mae: 11.2984 - val_loss: 219.1215 - val_mse: 219.1215 - val_mae: 11.0691\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 228.1028 - mse: 228.1028 - mae: 11.1149 - val_loss: 215.6868 - val_mse: 215.6868 - val_mae: 10.9425\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 225.3761 - mse: 225.3761 - mae: 11.0168 - val_loss: 212.8837 - val_mse: 212.8837 - val_mae: 10.8332\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 223.4526 - mse: 223.4526 - mae: 10.9381 - val_loss: 210.9525 - val_mse: 210.9525 - val_mae: 10.7739\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 220.5401 - mse: 220.5401 - mae: 10.9044 - val_loss: 206.7108 - val_mse: 206.7108 - val_mae: 10.6569\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 214.8216 - mse: 214.8216 - mae: 10.6999 - val_loss: 201.2637 - val_mse: 201.2637 - val_mae: 10.4488\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 211.4502 - mse: 211.4502 - mae: 10.6115 - val_loss: 198.9290 - val_mse: 198.9290 - val_mae: 10.3544\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 208.6737 - mse: 208.6737 - mae: 10.4289 - val_loss: 197.3349 - val_mse: 197.3349 - val_mae: 10.2507\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 208.4850 - mse: 208.4850 - mae: 10.3421 - val_loss: 196.0000 - val_mse: 196.0000 - val_mae: 10.2412\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 207.4582 - mse: 207.4582 - mae: 10.4526 - val_loss: 195.3081 - val_mse: 195.3081 - val_mae: 10.1980\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 206.2079 - mse: 206.2079 - mae: 10.3052 - val_loss: 193.4702 - val_mse: 193.4702 - val_mae: 10.1235\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 204.1612 - mse: 204.1612 - mae: 10.2369 - val_loss: 191.6933 - val_mse: 191.6933 - val_mae: 10.0755\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 203.2990 - mse: 203.2990 - mae: 10.2004 - val_loss: 190.8301 - val_mse: 190.8301 - val_mae: 10.0625\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 202.4124 - mse: 202.4124 - mae: 10.2718 - val_loss: 190.0553 - val_mse: 190.0553 - val_mae: 10.0105\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 201.3268 - mse: 201.3268 - mae: 10.1785 - val_loss: 187.9029 - val_mse: 187.9029 - val_mae: 9.9430\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 198.1780 - mse: 198.1780 - mae: 10.0551 - val_loss: 183.9722 - val_mse: 183.9722 - val_mae: 9.8085\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 195.5433 - mse: 195.5433 - mae: 10.0762 - val_loss: 182.4639 - val_mse: 182.4639 - val_mae: 9.7656\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 193.8011 - mse: 193.8011 - mae: 9.8965 - val_loss: 181.5796 - val_mse: 181.5796 - val_mae: 9.6609\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 193.4525 - mse: 193.4525 - mae: 9.8050 - val_loss: 180.9620 - val_mse: 180.9620 - val_mae: 9.6734\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 193.2025 - mse: 193.2025 - mae: 9.9403 - val_loss: 180.4806 - val_mse: 180.4806 - val_mae: 9.6551\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 192.8122 - mse: 192.8122 - mae: 9.8437 - val_loss: 180.3162 - val_mse: 180.3162 - val_mae: 9.6177\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 192.7084 - mse: 192.7084 - mae: 9.7651 - val_loss: 179.9894 - val_mse: 179.9894 - val_mae: 9.6007\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 192.5375 - mse: 192.5375 - mae: 9.8972 - val_loss: 179.9312 - val_mse: 179.9312 - val_mae: 9.6479\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 192.0003 - mse: 192.0003 - mae: 9.8356 - val_loss: 179.6434 - val_mse: 179.6434 - val_mae: 9.5631\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 192.1324 - mse: 192.1324 - mae: 9.7055 - val_loss: 179.1738 - val_mse: 179.1738 - val_mae: 9.5739\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 191.8332 - mse: 191.8332 - mae: 9.8513 - val_loss: 178.9147 - val_mse: 178.9147 - val_mae: 9.5693\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 191.6439 - mse: 191.6439 - mae: 9.8451 - val_loss: 178.6405 - val_mse: 178.6405 - val_mae: 9.5305\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 191.5444 - mse: 191.5444 - mae: 9.6682 - val_loss: 178.3764 - val_mse: 178.3764 - val_mae: 9.5041\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 191.2753 - mse: 191.2753 - mae: 9.7472 - val_loss: 177.3505 - val_mse: 177.3505 - val_mae: 9.4995\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 189.1494 - mse: 189.1494 - mae: 9.7071 - val_loss: 174.8938 - val_mse: 174.8938 - val_mae: 9.4240\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 186.8640 - mse: 186.8640 - mae: 9.5750 - val_loss: 172.2376 - val_mse: 172.2376 - val_mae: 9.3382\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 184.4306 - mse: 184.4306 - mae: 9.5561 - val_loss: 170.0363 - val_mse: 170.0363 - val_mae: 9.2480\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.1518 - mse: 182.1518 - mae: 9.5761 - val_loss: 168.6402 - val_mse: 168.6402 - val_mae: 9.2384\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 180.6379 - mse: 180.6379 - mae: 9.4526 - val_loss: 167.3810 - val_mse: 167.3810 - val_mae: 9.1418\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 179.9705 - mse: 179.9705 - mae: 9.3266 - val_loss: 166.8049 - val_mse: 166.8049 - val_mae: 9.1299\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 179.6037 - mse: 179.6037 - mae: 9.4114 - val_loss: 166.4202 - val_mse: 166.4202 - val_mae: 9.1291\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 179.8342 - mse: 179.8342 - mae: 9.4969 - val_loss: 166.4480 - val_mse: 166.4480 - val_mae: 9.1073\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 180.1003 - mse: 180.1003 - mae: 9.2597 - val_loss: 166.3170 - val_mse: 166.3170 - val_mae: 9.0748\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 180.3554 - mse: 180.3554 - mae: 9.4721 - val_loss: 165.9691 - val_mse: 165.9691 - val_mae: 9.0888\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 178.9924 - mse: 178.9924 - mae: 9.2922 - val_loss: 166.0320 - val_mse: 166.0320 - val_mae: 9.0621\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 179.0388 - mse: 179.0388 - mae: 9.3549 - val_loss: 165.1572 - val_mse: 165.1572 - val_mae: 9.0581\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 177.0628 - mse: 177.0628 - mae: 9.2778 - val_loss: 162.9334 - val_mse: 162.9334 - val_mae: 8.9774\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 175.3253 - mse: 175.3253 - mae: 9.1896 - val_loss: 161.8395 - val_mse: 161.8395 - val_mae: 8.9460\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 175.1589 - mse: 175.1589 - mae: 9.3266 - val_loss: 161.2070 - val_mse: 161.2070 - val_mae: 8.9142\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174.7242 - mse: 174.7242 - mae: 9.1218 - val_loss: 160.9965 - val_mse: 160.9965 - val_mae: 8.9070\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174.6141 - mse: 174.6141 - mae: 9.2560 - val_loss: 161.3238 - val_mse: 161.3238 - val_mae: 8.9398\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174.5652 - mse: 174.5652 - mae: 9.1474 - val_loss: 161.0840 - val_mse: 161.0840 - val_mae: 8.8763\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174.3370 - mse: 174.3370 - mae: 9.1688 - val_loss: 160.7729 - val_mse: 160.7729 - val_mae: 8.8859\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174.1828 - mse: 174.1828 - mae: 9.1117 - val_loss: 160.6412 - val_mse: 160.6412 - val_mae: 8.8683\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 173.9628 - mse: 173.9628 - mae: 9.2259 - val_loss: 160.6531 - val_mse: 160.6531 - val_mae: 8.8978\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 173.9571 - mse: 173.9571 - mae: 9.0955 - val_loss: 160.7362 - val_mse: 160.7362 - val_mae: 8.8476\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174.4254 - mse: 174.4254 - mae: 9.2234 - val_loss: 160.5628 - val_mse: 160.5628 - val_mae: 8.8798\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 173.4072 - mse: 173.4072 - mae: 9.1083 - val_loss: 159.8101 - val_mse: 159.8101 - val_mae: 8.8268\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 172.3129 - mse: 172.3129 - mae: 9.0137 - val_loss: 158.0161 - val_mse: 158.0161 - val_mae: 8.7785\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 171.2862 - mse: 171.2862 - mae: 9.1689 - val_loss: 156.8434 - val_mse: 156.8434 - val_mae: 8.7712\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.9790 - mse: 169.9790 - mae: 8.9645 - val_loss: 156.5641 - val_mse: 156.5641 - val_mae: 8.7015\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.6369 - mse: 169.6368 - mae: 8.9557 - val_loss: 156.5024 - val_mse: 156.5024 - val_mae: 8.7278\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.7385 - mse: 169.7385 - mae: 9.1311 - val_loss: 156.2114 - val_mse: 156.2114 - val_mae: 8.6904\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 171.3468 - mse: 171.3468 - mae: 8.8943 - val_loss: 156.2034 - val_mse: 156.2034 - val_mae: 8.6616\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.7376 - mse: 169.7376 - mae: 9.1310 - val_loss: 156.1559 - val_mse: 156.1559 - val_mae: 8.7207\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.5187 - mse: 169.5187 - mae: 8.9029 - val_loss: 156.0916 - val_mse: 156.0916 - val_mae: 8.6714\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.2628 - mse: 169.2628 - mae: 9.0049 - val_loss: 156.2277 - val_mse: 156.2277 - val_mae: 8.6988\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.7948 - mse: 169.7948 - mae: 8.9324 - val_loss: 155.8623 - val_mse: 155.8623 - val_mae: 8.6502\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 168.6994 - mse: 168.6994 - mae: 8.9544 - val_loss: 155.8250 - val_mse: 155.8250 - val_mae: 8.6899\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 168.7510 - mse: 168.7510 - mae: 8.9901 - val_loss: 155.4254 - val_mse: 155.4254 - val_mae: 8.6325\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 168.7782 - mse: 168.7782 - mae: 8.8679 - val_loss: 155.0405 - val_mse: 155.0405 - val_mae: 8.6058\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 167.3345 - mse: 167.3345 - mae: 8.9948 - val_loss: 153.1422 - val_mse: 153.1422 - val_mae: 8.6129\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 165.6175 - mse: 165.6175 - mae: 8.8475 - val_loss: 151.5549 - val_mse: 151.5549 - val_mae: 8.5163\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 165.7397 - mse: 165.7397 - mae: 8.9791 - val_loss: 150.1253 - val_mse: 150.1253 - val_mae: 8.4732\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 163.1668 - mse: 163.1668 - mae: 8.6672 - val_loss: 148.7688 - val_mse: 148.7688 - val_mae: 8.4262\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 161.6400 - mse: 161.6400 - mae: 8.7201 - val_loss: 147.7690 - val_mse: 147.7690 - val_mae: 8.3951\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 160.8038 - mse: 160.8038 - mae: 8.7513 - val_loss: 147.2289 - val_mse: 147.2289 - val_mae: 8.3601\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 160.6654 - mse: 160.6654 - mae: 8.5734 - val_loss: 146.9447 - val_mse: 146.9447 - val_mae: 8.3503\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 160.5458 - mse: 160.5458 - mae: 8.7571 - val_loss: 146.9249 - val_mse: 146.9249 - val_mae: 8.3712\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 160.0145 - mse: 160.0145 - mae: 8.6810 - val_loss: 147.1650 - val_mse: 147.1650 - val_mae: 8.3408\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 160.9623 - mse: 160.9623 - mae: 8.5524 - val_loss: 147.0189 - val_mse: 147.0189 - val_mae: 8.3239\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 160.1596 - mse: 160.1596 - mae: 8.6344 - val_loss: 146.7138 - val_mse: 146.7138 - val_mae: 8.3301\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 159.8962 - mse: 159.8962 - mae: 8.6090 - val_loss: 146.6532 - val_mse: 146.6532 - val_mae: 8.3161\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 159.8227 - mse: 159.8227 - mae: 8.5702 - val_loss: 146.5035 - val_mse: 146.5035 - val_mae: 8.3132\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 159.8499 - mse: 159.8499 - mae: 8.6336 - val_loss: 146.3934 - val_mse: 146.3934 - val_mae: 8.3208\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 159.7672 - mse: 159.7672 - mae: 8.6063 - val_loss: 146.5665 - val_mse: 146.5665 - val_mae: 8.3125\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 159.7748 - mse: 159.7748 - mae: 8.6564 - val_loss: 146.4758 - val_mse: 146.4758 - val_mae: 8.2961\n",
            "Epoch 92: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 630.9357 - mse: 630.9357 - mae: 20.8159 - val_loss: 429.3110 - val_mse: 429.3110 - val_mae: 17.5300\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 452.3919 - mse: 452.3919 - mae: 17.9315 - val_loss: 344.3930 - val_mse: 344.3930 - val_mae: 15.7466\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 400.2293 - mse: 400.2293 - mae: 16.6984 - val_loss: 308.2043 - val_mse: 308.2043 - val_mae: 14.8295\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 376.0267 - mse: 376.0267 - mae: 16.0140 - val_loss: 288.6479 - val_mse: 288.6479 - val_mae: 14.2671\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 360.6859 - mse: 360.6859 - mae: 15.5511 - val_loss: 275.7111 - val_mse: 275.7111 - val_mae: 13.8904\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 349.6405 - mse: 349.6405 - mae: 15.2170 - val_loss: 265.1390 - val_mse: 265.1390 - val_mae: 13.5864\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 340.5947 - mse: 340.5947 - mae: 14.9450 - val_loss: 257.0031 - val_mse: 257.0031 - val_mae: 13.3406\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 333.5687 - mse: 333.5687 - mae: 14.7244 - val_loss: 250.8817 - val_mse: 250.8817 - val_mae: 13.1477\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 327.1968 - mse: 327.1968 - mae: 14.5266 - val_loss: 244.8504 - val_mse: 244.8504 - val_mae: 12.9523\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 321.3151 - mse: 321.3151 - mae: 14.3428 - val_loss: 239.5398 - val_mse: 239.5398 - val_mae: 12.7747\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 317.0227 - mse: 317.0227 - mae: 14.2014 - val_loss: 235.4925 - val_mse: 235.4925 - val_mae: 12.6206\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 312.6274 - mse: 312.6274 - mae: 14.0478 - val_loss: 230.7731 - val_mse: 230.7731 - val_mae: 12.4530\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 308.5419 - mse: 308.5419 - mae: 13.9123 - val_loss: 227.7701 - val_mse: 227.7701 - val_mae: 12.3381\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 305.7786 - mse: 305.7786 - mae: 13.8290 - val_loss: 224.8196 - val_mse: 224.8196 - val_mae: 12.2227\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 303.1785 - mse: 303.1785 - mae: 13.7304 - val_loss: 222.1322 - val_mse: 222.1322 - val_mae: 12.1266\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 301.1845 - mse: 301.1845 - mae: 13.6585 - val_loss: 219.5643 - val_mse: 219.5643 - val_mae: 12.0372\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 299.2528 - mse: 299.2528 - mae: 13.5925 - val_loss: 217.8222 - val_mse: 217.8222 - val_mae: 11.9673\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 297.4037 - mse: 297.4037 - mae: 13.5298 - val_loss: 216.0349 - val_mse: 216.0349 - val_mae: 11.8902\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 295.5135 - mse: 295.5135 - mae: 13.4581 - val_loss: 214.2070 - val_mse: 214.2070 - val_mae: 11.8188\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 293.3836 - mse: 293.3836 - mae: 13.3957 - val_loss: 212.3994 - val_mse: 212.3994 - val_mae: 11.7474\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 290.6798 - mse: 290.6798 - mae: 13.3269 - val_loss: 209.6315 - val_mse: 209.6315 - val_mae: 11.6467\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 287.6169 - mse: 287.6169 - mae: 13.2229 - val_loss: 206.8572 - val_mse: 206.8572 - val_mae: 11.5556\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 285.0733 - mse: 285.0733 - mae: 13.1308 - val_loss: 205.1245 - val_mse: 205.1245 - val_mae: 11.4959\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 283.1110 - mse: 283.1110 - mae: 13.0732 - val_loss: 203.5042 - val_mse: 203.5042 - val_mae: 11.4309\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 281.4916 - mse: 281.4916 - mae: 13.0293 - val_loss: 201.8371 - val_mse: 201.8371 - val_mae: 11.3631\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 280.0241 - mse: 280.0241 - mae: 12.9655 - val_loss: 200.4767 - val_mse: 200.4767 - val_mae: 11.3146\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 278.7317 - mse: 278.7317 - mae: 12.9249 - val_loss: 199.0046 - val_mse: 199.0046 - val_mae: 11.2539\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 277.4858 - mse: 277.4858 - mae: 12.8822 - val_loss: 197.5103 - val_mse: 197.5103 - val_mae: 11.1934\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 276.1765 - mse: 276.1765 - mae: 12.8304 - val_loss: 196.4747 - val_mse: 196.4747 - val_mae: 11.1514\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 274.9948 - mse: 274.9948 - mae: 12.8041 - val_loss: 194.8655 - val_mse: 194.8655 - val_mae: 11.0854\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 273.9502 - mse: 273.9502 - mae: 12.7543 - val_loss: 193.5411 - val_mse: 193.5411 - val_mae: 11.0253\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 272.9479 - mse: 272.9479 - mae: 12.7022 - val_loss: 192.8653 - val_mse: 192.8653 - val_mae: 10.9955\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 271.9692 - mse: 271.9692 - mae: 12.6852 - val_loss: 191.9280 - val_mse: 191.9280 - val_mae: 10.9497\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 270.9472 - mse: 270.9472 - mae: 12.6453 - val_loss: 191.0552 - val_mse: 191.0552 - val_mae: 10.9083\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 270.2392 - mse: 270.2392 - mae: 12.6157 - val_loss: 190.1593 - val_mse: 190.1593 - val_mae: 10.8656\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 269.4543 - mse: 269.4543 - mae: 12.5960 - val_loss: 189.0475 - val_mse: 189.0475 - val_mae: 10.8177\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 268.6927 - mse: 268.6927 - mae: 12.5652 - val_loss: 188.1339 - val_mse: 188.1339 - val_mae: 10.7728\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 267.9127 - mse: 267.9127 - mae: 12.5358 - val_loss: 186.8678 - val_mse: 186.8678 - val_mae: 10.7246\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 266.6541 - mse: 266.6541 - mae: 12.4902 - val_loss: 186.1917 - val_mse: 186.1917 - val_mae: 10.7002\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 265.8336 - mse: 265.8336 - mae: 12.4651 - val_loss: 185.7244 - val_mse: 185.7244 - val_mae: 10.6802\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.9898 - mse: 264.9898 - mae: 12.4628 - val_loss: 184.8080 - val_mse: 184.8080 - val_mae: 10.6337\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.3240 - mse: 264.3240 - mae: 12.4234 - val_loss: 184.0780 - val_mse: 184.0780 - val_mae: 10.5996\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 263.8277 - mse: 263.8277 - mae: 12.4021 - val_loss: 183.2916 - val_mse: 183.2916 - val_mae: 10.5638\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.2554 - mse: 263.2554 - mae: 12.3797 - val_loss: 182.7480 - val_mse: 182.7480 - val_mae: 10.5348\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.8353 - mse: 262.8353 - mae: 12.3691 - val_loss: 182.1984 - val_mse: 182.1984 - val_mae: 10.5067\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.2587 - mse: 262.2587 - mae: 12.3368 - val_loss: 181.6022 - val_mse: 181.6022 - val_mae: 10.4759\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.0778 - mse: 262.0778 - mae: 12.3192 - val_loss: 181.1878 - val_mse: 181.1878 - val_mae: 10.4562\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 261.4081 - mse: 261.4081 - mae: 12.2987 - val_loss: 180.9472 - val_mse: 180.9472 - val_mae: 10.4480\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 260.9222 - mse: 260.9222 - mae: 12.2814 - val_loss: 180.8399 - val_mse: 180.8399 - val_mae: 10.4471\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 260.5013 - mse: 260.5013 - mae: 12.2792 - val_loss: 180.2272 - val_mse: 180.2272 - val_mae: 10.4153\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 259.9547 - mse: 259.9547 - mae: 12.2533 - val_loss: 179.8175 - val_mse: 179.8175 - val_mae: 10.3984\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 259.4969 - mse: 259.4969 - mae: 12.2490 - val_loss: 179.1342 - val_mse: 179.1342 - val_mae: 10.3667\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 258.8095 - mse: 258.8095 - mae: 12.2191 - val_loss: 178.6669 - val_mse: 178.6669 - val_mae: 10.3478\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 258.2115 - mse: 258.2115 - mae: 12.2289 - val_loss: 177.4988 - val_mse: 177.4988 - val_mae: 10.2819\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 257.5735 - mse: 257.5735 - mae: 12.1590 - val_loss: 177.3320 - val_mse: 177.3320 - val_mae: 10.2814\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.9525 - mse: 256.9525 - mae: 12.1567 - val_loss: 176.8080 - val_mse: 176.8080 - val_mae: 10.2538\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.4899 - mse: 256.4899 - mae: 12.1315 - val_loss: 176.2948 - val_mse: 176.2948 - val_mae: 10.2299\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.1325 - mse: 256.1325 - mae: 12.1173 - val_loss: 176.1697 - val_mse: 176.1697 - val_mae: 10.2274\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 255.7537 - mse: 255.7537 - mae: 12.1251 - val_loss: 175.2743 - val_mse: 175.2743 - val_mae: 10.1769\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 255.2929 - mse: 255.2929 - mae: 12.0843 - val_loss: 175.0069 - val_mse: 175.0069 - val_mae: 10.1649\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 255.0879 - mse: 255.0879 - mae: 12.0716 - val_loss: 174.5892 - val_mse: 174.5892 - val_mae: 10.1443\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 254.4678 - mse: 254.4678 - mae: 12.0705 - val_loss: 173.8850 - val_mse: 173.8850 - val_mae: 10.1048\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 254.1344 - mse: 254.1344 - mae: 12.0388 - val_loss: 173.5328 - val_mse: 173.5328 - val_mae: 10.0911\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 253.7642 - mse: 253.7642 - mae: 12.0238 - val_loss: 173.3947 - val_mse: 173.3947 - val_mae: 10.0927\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 253.3616 - mse: 253.3616 - mae: 12.0152 - val_loss: 172.8750 - val_mse: 172.8750 - val_mae: 10.0640\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 252.8802 - mse: 252.8802 - mae: 12.0007 - val_loss: 172.4018 - val_mse: 172.4018 - val_mae: 10.0408\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 252.4705 - mse: 252.4705 - mae: 11.9814 - val_loss: 172.0876 - val_mse: 172.0876 - val_mae: 10.0285\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 251.9293 - mse: 251.9293 - mae: 11.9768 - val_loss: 171.7672 - val_mse: 171.7672 - val_mae: 10.0144\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 251.5163 - mse: 251.5163 - mae: 11.9759 - val_loss: 171.1840 - val_mse: 171.1840 - val_mae: 9.9850\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 250.7626 - mse: 250.7626 - mae: 11.9333 - val_loss: 170.5428 - val_mse: 170.5428 - val_mae: 9.9557\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 250.1693 - mse: 250.1693 - mae: 11.9298 - val_loss: 169.6076 - val_mse: 169.6076 - val_mae: 9.9081\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.4758 - mse: 249.4758 - mae: 11.8693 - val_loss: 169.3471 - val_mse: 169.3471 - val_mae: 9.9043\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 248.6541 - mse: 248.6541 - mae: 11.8757 - val_loss: 168.5934 - val_mse: 168.5934 - val_mae: 9.8693\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 248.0478 - mse: 248.0478 - mae: 11.8354 - val_loss: 168.6119 - val_mse: 168.6119 - val_mae: 9.8841\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 247.4974 - mse: 247.4974 - mae: 11.8386 - val_loss: 168.0127 - val_mse: 168.0127 - val_mae: 9.8542\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 247.0389 - mse: 247.0389 - mae: 11.8114 - val_loss: 167.3931 - val_mse: 167.3931 - val_mae: 9.8266\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 246.4843 - mse: 246.4843 - mae: 11.8054 - val_loss: 167.0158 - val_mse: 167.0158 - val_mae: 9.8106\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 245.9594 - mse: 245.9594 - mae: 11.7907 - val_loss: 166.5426 - val_mse: 166.5426 - val_mae: 9.7900\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 245.5739 - mse: 245.5739 - mae: 11.7703 - val_loss: 166.0615 - val_mse: 166.0615 - val_mae: 9.7686\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 245.3564 - mse: 245.3564 - mae: 11.7636 - val_loss: 165.8959 - val_mse: 165.8959 - val_mae: 9.7680\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 244.9463 - mse: 244.9463 - mae: 11.7690 - val_loss: 165.1285 - val_mse: 165.1285 - val_mae: 9.7251\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 244.4579 - mse: 244.4579 - mae: 11.7176 - val_loss: 165.0371 - val_mse: 165.0371 - val_mae: 9.7276\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 244.1080 - mse: 244.1080 - mae: 11.7240 - val_loss: 164.6656 - val_mse: 164.6656 - val_mae: 9.7090\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 243.7720 - mse: 243.7720 - mae: 11.7057 - val_loss: 164.4288 - val_mse: 164.4288 - val_mae: 9.7018\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 243.5326 - mse: 243.5326 - mae: 11.6888 - val_loss: 164.1794 - val_mse: 164.1794 - val_mae: 9.6944\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 243.1276 - mse: 243.1276 - mae: 11.6856 - val_loss: 163.9511 - val_mse: 163.9511 - val_mae: 9.6964\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 242.0102 - mse: 242.0102 - mae: 11.6895 - val_loss: 162.3334 - val_mse: 162.3334 - val_mae: 9.6347\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 240.9516 - mse: 240.9516 - mae: 11.6369 - val_loss: 161.9635 - val_mse: 161.9635 - val_mae: 9.6293\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 240.3873 - mse: 240.3873 - mae: 11.6259 - val_loss: 161.3643 - val_mse: 161.3643 - val_mae: 9.5958\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 239.7628 - mse: 239.7628 - mae: 11.6280 - val_loss: 160.5942 - val_mse: 160.5942 - val_mae: 9.5503\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 239.3307 - mse: 239.3307 - mae: 11.5768 - val_loss: 160.3894 - val_mse: 160.3894 - val_mae: 9.5437\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 239.0495 - mse: 239.0495 - mae: 11.5840 - val_loss: 159.7785 - val_mse: 159.7785 - val_mae: 9.5104\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 238.6104 - mse: 238.6104 - mae: 11.5452 - val_loss: 159.4654 - val_mse: 159.4654 - val_mae: 9.4952\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 238.1516 - mse: 238.1516 - mae: 11.5252 - val_loss: 159.3978 - val_mse: 159.3978 - val_mae: 9.4985\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 237.7676 - mse: 237.7676 - mae: 11.5339 - val_loss: 159.0779 - val_mse: 159.0779 - val_mae: 9.4803\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 237.4409 - mse: 237.4409 - mae: 11.5174 - val_loss: 158.6105 - val_mse: 158.6105 - val_mae: 9.4529\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 237.1448 - mse: 237.1448 - mae: 11.4912 - val_loss: 158.7199 - val_mse: 158.7199 - val_mae: 9.4628\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 236.8072 - mse: 236.8072 - mae: 11.5185 - val_loss: 158.2418 - val_mse: 158.2418 - val_mae: 9.4314\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 236.5399 - mse: 236.5399 - mae: 11.4912 - val_loss: 157.9926 - val_mse: 157.9926 - val_mae: 9.4166\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 236.3281 - mse: 236.3281 - mae: 11.4554 - val_loss: 157.9847 - val_mse: 157.9847 - val_mae: 9.4193\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 560.0522 - mse: 560.0522 - mae: 20.0409 - val_loss: 398.4152 - val_mse: 398.4152 - val_mae: 17.2265\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 492.9070 - mse: 492.9070 - mae: 18.7217 - val_loss: 351.0987 - val_mse: 351.0987 - val_mae: 16.1310\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 458.6072 - mse: 458.6072 - mae: 17.8974 - val_loss: 324.1171 - val_mse: 324.1171 - val_mae: 15.4163\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 436.6924 - mse: 436.6924 - mae: 17.3214 - val_loss: 305.1883 - val_mse: 305.1883 - val_mae: 14.8764\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 420.1118 - mse: 420.1118 - mae: 16.8715 - val_loss: 290.6103 - val_mse: 290.6103 - val_mae: 14.4318\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 406.8102 - mse: 406.8102 - mae: 16.4986 - val_loss: 279.0197 - val_mse: 279.0197 - val_mae: 14.0531\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 395.4292 - mse: 395.4292 - mae: 16.1745 - val_loss: 269.4447 - val_mse: 269.4447 - val_mae: 13.7373\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 386.2561 - mse: 386.2561 - mae: 15.9069 - val_loss: 262.2704 - val_mse: 262.2704 - val_mae: 13.4863\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 378.4172 - mse: 378.4172 - mae: 15.6818 - val_loss: 256.4591 - val_mse: 256.4591 - val_mae: 13.2859\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 372.1948 - mse: 372.1948 - mae: 15.5032 - val_loss: 251.7635 - val_mse: 251.7635 - val_mae: 13.1263\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 367.2818 - mse: 367.2818 - mae: 15.3658 - val_loss: 248.0584 - val_mse: 248.0584 - val_mae: 12.9906\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 363.4475 - mse: 363.4475 - mae: 15.2474 - val_loss: 245.2233 - val_mse: 245.2233 - val_mae: 12.8850\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 360.1787 - mse: 360.1787 - mae: 15.1577 - val_loss: 242.7760 - val_mse: 242.7760 - val_mae: 12.7941\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 357.3674 - mse: 357.3674 - mae: 15.0804 - val_loss: 240.2370 - val_mse: 240.2370 - val_mae: 12.7027\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 354.4051 - mse: 354.4051 - mae: 14.9895 - val_loss: 238.0527 - val_mse: 238.0527 - val_mae: 12.6255\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 351.3657 - mse: 351.3657 - mae: 14.9154 - val_loss: 235.0812 - val_mse: 235.0812 - val_mae: 12.5270\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 347.6280 - mse: 347.6280 - mae: 14.8045 - val_loss: 231.9787 - val_mse: 231.9787 - val_mae: 12.4264\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 344.1104 - mse: 344.1104 - mae: 14.7054 - val_loss: 229.4248 - val_mse: 229.4248 - val_mae: 12.3301\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 341.4389 - mse: 341.4389 - mae: 14.6130 - val_loss: 227.4924 - val_mse: 227.4924 - val_mae: 12.2507\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 339.2405 - mse: 339.2405 - mae: 14.5467 - val_loss: 225.7079 - val_mse: 225.7079 - val_mae: 12.1740\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 337.3445 - mse: 337.3445 - mae: 14.4940 - val_loss: 223.8041 - val_mse: 223.8041 - val_mae: 12.0905\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 335.7135 - mse: 335.7135 - mae: 14.4181 - val_loss: 222.3403 - val_mse: 222.3403 - val_mae: 12.0263\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 334.2096 - mse: 334.2096 - mae: 14.3636 - val_loss: 221.2909 - val_mse: 221.2909 - val_mae: 11.9819\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 332.9738 - mse: 332.9738 - mae: 14.3331 - val_loss: 220.3380 - val_mse: 220.3380 - val_mae: 11.9373\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 331.7464 - mse: 331.7464 - mae: 14.2960 - val_loss: 219.1712 - val_mse: 219.1712 - val_mae: 11.8860\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 330.6572 - mse: 330.6572 - mae: 14.2502 - val_loss: 218.3039 - val_mse: 218.3039 - val_mae: 11.8508\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 329.5311 - mse: 329.5311 - mae: 14.2155 - val_loss: 216.8527 - val_mse: 216.8527 - val_mae: 11.7909\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 328.4408 - mse: 328.4408 - mae: 14.1748 - val_loss: 215.7692 - val_mse: 215.7692 - val_mae: 11.7454\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 327.0984 - mse: 327.0984 - mae: 14.1318 - val_loss: 214.7150 - val_mse: 214.7150 - val_mae: 11.7026\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 325.8867 - mse: 325.8867 - mae: 14.0918 - val_loss: 213.7266 - val_mse: 213.7266 - val_mae: 11.6631\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 324.6888 - mse: 324.6888 - mae: 14.0659 - val_loss: 212.2208 - val_mse: 212.2208 - val_mae: 11.6008\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 323.4185 - mse: 323.4185 - mae: 14.0093 - val_loss: 211.4970 - val_mse: 211.4970 - val_mae: 11.5783\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 321.9464 - mse: 321.9464 - mae: 13.9894 - val_loss: 210.2744 - val_mse: 210.2744 - val_mae: 11.5299\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 320.4570 - mse: 320.4570 - mae: 13.9417 - val_loss: 209.0838 - val_mse: 209.0838 - val_mae: 11.4849\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 319.0272 - mse: 319.0272 - mae: 13.8997 - val_loss: 208.0803 - val_mse: 208.0803 - val_mae: 11.4375\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 317.8517 - mse: 317.8517 - mae: 13.8770 - val_loss: 206.9245 - val_mse: 206.9245 - val_mae: 11.3790\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 316.8528 - mse: 316.8528 - mae: 13.8270 - val_loss: 205.9235 - val_mse: 205.9235 - val_mae: 11.3256\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 315.9156 - mse: 315.9156 - mae: 13.7871 - val_loss: 204.9778 - val_mse: 204.9778 - val_mae: 11.2748\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 314.9775 - mse: 314.9775 - mae: 13.7593 - val_loss: 204.1410 - val_mse: 204.1410 - val_mae: 11.2345\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 314.0657 - mse: 314.0657 - mae: 13.7068 - val_loss: 203.6480 - val_mse: 203.6480 - val_mae: 11.2187\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 313.1736 - mse: 313.1736 - mae: 13.7133 - val_loss: 202.5033 - val_mse: 202.5033 - val_mae: 11.1726\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 311.9152 - mse: 311.9152 - mae: 13.6719 - val_loss: 201.0507 - val_mse: 201.0507 - val_mae: 11.1248\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 310.1278 - mse: 310.1278 - mae: 13.6119 - val_loss: 199.4325 - val_mse: 199.4325 - val_mae: 11.0786\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 307.9788 - mse: 307.9788 - mae: 13.5594 - val_loss: 197.9918 - val_mse: 197.9918 - val_mae: 11.0213\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 306.4473 - mse: 306.4473 - mae: 13.5421 - val_loss: 196.4938 - val_mse: 196.4938 - val_mae: 10.9467\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 304.8984 - mse: 304.8984 - mae: 13.4464 - val_loss: 195.4924 - val_mse: 195.4924 - val_mae: 10.8973\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 303.6920 - mse: 303.6920 - mae: 13.4038 - val_loss: 194.6674 - val_mse: 194.6674 - val_mae: 10.8581\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 302.6565 - mse: 302.6565 - mae: 13.3765 - val_loss: 194.0781 - val_mse: 194.0781 - val_mae: 10.8339\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 301.5813 - mse: 301.5813 - mae: 13.3336 - val_loss: 193.9462 - val_mse: 193.9462 - val_mae: 10.8360\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 300.6187 - mse: 300.6187 - mae: 13.3424 - val_loss: 192.7487 - val_mse: 192.7487 - val_mae: 10.7737\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 299.6108 - mse: 299.6108 - mae: 13.3040 - val_loss: 191.7302 - val_mse: 191.7302 - val_mae: 10.7342\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 298.6717 - mse: 298.6717 - mae: 13.2772 - val_loss: 190.6944 - val_mse: 190.6944 - val_mae: 10.6997\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 297.1375 - mse: 297.1375 - mae: 13.2335 - val_loss: 189.5216 - val_mse: 189.5216 - val_mae: 10.6571\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 295.9294 - mse: 295.9294 - mae: 13.2095 - val_loss: 188.2176 - val_mse: 188.2176 - val_mae: 10.6012\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 294.6063 - mse: 294.6063 - mae: 13.1492 - val_loss: 187.5827 - val_mse: 187.5827 - val_mae: 10.5839\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 293.5938 - mse: 293.5938 - mae: 13.1362 - val_loss: 186.6318 - val_mse: 186.6318 - val_mae: 10.5464\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 292.4719 - mse: 292.4719 - mae: 13.0921 - val_loss: 185.7947 - val_mse: 185.7947 - val_mae: 10.5179\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 291.4359 - mse: 291.4359 - mae: 13.0700 - val_loss: 184.9433 - val_mse: 184.9433 - val_mae: 10.4848\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 290.6016 - mse: 290.6016 - mae: 13.0316 - val_loss: 184.4382 - val_mse: 184.4382 - val_mae: 10.4677\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 289.8155 - mse: 289.8155 - mae: 13.0175 - val_loss: 183.9346 - val_mse: 183.9346 - val_mae: 10.4463\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 289.2104 - mse: 289.2104 - mae: 13.0078 - val_loss: 183.1531 - val_mse: 183.1531 - val_mae: 10.4031\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 288.4383 - mse: 288.4383 - mae: 12.9759 - val_loss: 182.4400 - val_mse: 182.4400 - val_mae: 10.3639\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 287.8912 - mse: 287.8912 - mae: 12.9504 - val_loss: 181.9385 - val_mse: 181.9385 - val_mae: 10.3372\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 287.3933 - mse: 287.3933 - mae: 12.9274 - val_loss: 181.5644 - val_mse: 181.5644 - val_mae: 10.3205\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 287.0746 - mse: 287.0746 - mae: 12.8901 - val_loss: 181.3976 - val_mse: 181.3976 - val_mae: 10.3154\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 286.4762 - mse: 286.4762 - mae: 12.8886 - val_loss: 181.3022 - val_mse: 181.3022 - val_mae: 10.3139\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 286.0624 - mse: 286.0624 - mae: 12.8936 - val_loss: 180.6545 - val_mse: 180.6545 - val_mae: 10.2772\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 285.7016 - mse: 285.7016 - mae: 12.8710 - val_loss: 180.4473 - val_mse: 180.4473 - val_mae: 10.2693\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 285.3709 - mse: 285.3709 - mae: 12.8667 - val_loss: 180.0978 - val_mse: 180.0978 - val_mae: 10.2523\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 284.7379 - mse: 284.7379 - mae: 12.8324 - val_loss: 179.4863 - val_mse: 179.4863 - val_mae: 10.2295\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 283.5809 - mse: 283.5809 - mae: 12.8276 - val_loss: 177.6058 - val_mse: 177.6058 - val_mae: 10.1584\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 281.7456 - mse: 281.7456 - mae: 12.7393 - val_loss: 176.4274 - val_mse: 176.4274 - val_mae: 10.1174\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 280.0204 - mse: 280.0204 - mae: 12.6948 - val_loss: 175.4777 - val_mse: 175.4777 - val_mae: 10.0780\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 278.5939 - mse: 278.5939 - mae: 12.6461 - val_loss: 174.9774 - val_mse: 174.9774 - val_mae: 10.0645\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 277.5291 - mse: 277.5291 - mae: 12.6294 - val_loss: 174.0878 - val_mse: 174.0878 - val_mae: 10.0245\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 276.6568 - mse: 276.6568 - mae: 12.5756 - val_loss: 173.3280 - val_mse: 173.3280 - val_mae: 9.9923\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 275.5648 - mse: 275.5648 - mae: 12.5627 - val_loss: 172.9728 - val_mse: 172.9728 - val_mae: 9.9873\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 274.7941 - mse: 274.7941 - mae: 12.5735 - val_loss: 171.7223 - val_mse: 171.7223 - val_mae: 9.9202\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 274.1250 - mse: 274.1250 - mae: 12.5117 - val_loss: 171.3587 - val_mse: 171.3587 - val_mae: 9.9074\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 273.6339 - mse: 273.6339 - mae: 12.5033 - val_loss: 171.0735 - val_mse: 171.0735 - val_mae: 9.8975\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 273.0981 - mse: 273.0981 - mae: 12.4954 - val_loss: 170.3428 - val_mse: 170.3428 - val_mae: 9.8583\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 272.5149 - mse: 272.5149 - mae: 12.4484 - val_loss: 170.2811 - val_mse: 170.2811 - val_mae: 9.8659\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 272.0458 - mse: 272.0458 - mae: 12.4468 - val_loss: 170.2062 - val_mse: 170.2062 - val_mae: 9.8713\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 271.5056 - mse: 271.5056 - mae: 12.4486 - val_loss: 169.7378 - val_mse: 169.7378 - val_mae: 9.8500\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 271.0225 - mse: 271.0225 - mae: 12.4189 - val_loss: 169.5088 - val_mse: 169.5088 - val_mae: 9.8424\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 270.5298 - mse: 270.5298 - mae: 12.4181 - val_loss: 169.0353 - val_mse: 169.0353 - val_mae: 9.8219\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 270.0882 - mse: 270.0882 - mae: 12.4002 - val_loss: 168.7705 - val_mse: 168.7705 - val_mae: 9.8130\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 269.7621 - mse: 269.7621 - mae: 12.3899 - val_loss: 168.8141 - val_mse: 168.8141 - val_mae: 9.8233\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 269.3905 - mse: 269.3905 - mae: 12.4071 - val_loss: 168.1333 - val_mse: 168.1333 - val_mae: 9.7828\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 268.9680 - mse: 268.9680 - mae: 12.3919 - val_loss: 167.3592 - val_mse: 167.3592 - val_mae: 9.7373\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 268.7256 - mse: 268.7256 - mae: 12.3541 - val_loss: 167.2812 - val_mse: 167.2812 - val_mae: 9.7371\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 268.6403 - mse: 268.6403 - mae: 12.3533 - val_loss: 166.9749 - val_mse: 166.9749 - val_mae: 9.7227\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 268.0988 - mse: 268.0988 - mae: 12.3233 - val_loss: 167.1428 - val_mse: 167.1428 - val_mae: 9.7409\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 267.9371 - mse: 267.9371 - mae: 12.3294 - val_loss: 167.0099 - val_mse: 167.0099 - val_mae: 9.7371\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 267.5586 - mse: 267.5586 - mae: 12.3452 - val_loss: 166.4643 - val_mse: 166.4643 - val_mae: 9.7029\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 267.3421 - mse: 267.3421 - mae: 12.3141 - val_loss: 166.0989 - val_mse: 166.0989 - val_mae: 9.6834\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 267.0962 - mse: 267.0962 - mae: 12.2975 - val_loss: 166.3036 - val_mse: 166.3036 - val_mae: 9.7034\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 266.9119 - mse: 266.9119 - mae: 12.3254 - val_loss: 165.8931 - val_mse: 165.8931 - val_mae: 9.6782\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 266.7419 - mse: 266.7419 - mae: 12.3059 - val_loss: 165.5088 - val_mse: 165.5088 - val_mae: 9.6544\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 266.4638 - mse: 266.4638 - mae: 12.2701 - val_loss: 165.3050 - val_mse: 165.3050 - val_mae: 9.6447\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: 770.1128 - mse: 770.1128 - mae: 21.8594 - val_loss: 561.3215 - val_mse: 561.3215 - val_mae: 19.4590\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 501.9758 - mse: 501.9758 - mae: 18.5489 - val_loss: 425.1478 - val_mse: 425.1478 - val_mae: 17.3198\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 412.6693 - mse: 412.6693 - mae: 16.9674 - val_loss: 371.8601 - val_mse: 371.8601 - val_mae: 16.1943\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 373.0560 - mse: 373.0560 - mae: 16.0892 - val_loss: 344.4371 - val_mse: 344.4371 - val_mae: 15.4862\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 350.4424 - mse: 350.4424 - mae: 15.4920 - val_loss: 327.4753 - val_mse: 327.4753 - val_mae: 14.9805\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 335.0915 - mse: 335.0915 - mae: 15.0390 - val_loss: 315.4324 - val_mse: 315.4324 - val_mae: 14.5838\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 323.9915 - mse: 323.9915 - mae: 14.6794 - val_loss: 306.5780 - val_mse: 306.5780 - val_mae: 14.2702\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 315.2558 - mse: 315.2558 - mae: 14.3783 - val_loss: 299.4098 - val_mse: 299.4098 - val_mae: 13.9919\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 307.9034 - mse: 307.9034 - mae: 14.1131 - val_loss: 293.2981 - val_mse: 293.2981 - val_mae: 13.7615\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 301.2775 - mse: 301.2775 - mae: 13.8847 - val_loss: 287.6913 - val_mse: 287.6913 - val_mae: 13.5599\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 295.6082 - mse: 295.6082 - mae: 13.6887 - val_loss: 283.0695 - val_mse: 283.0695 - val_mae: 13.4000\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 290.8787 - mse: 290.8787 - mae: 13.5308 - val_loss: 279.2424 - val_mse: 279.2424 - val_mae: 13.2613\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 286.9590 - mse: 286.9590 - mae: 13.3886 - val_loss: 276.1505 - val_mse: 276.1505 - val_mae: 13.1548\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 283.7707 - mse: 283.7707 - mae: 13.2779 - val_loss: 273.4192 - val_mse: 273.4192 - val_mae: 13.0602\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 280.9120 - mse: 280.9120 - mae: 13.1845 - val_loss: 270.9192 - val_mse: 270.9192 - val_mae: 12.9708\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 278.2997 - mse: 278.2997 - mae: 13.0924 - val_loss: 268.4959 - val_mse: 268.4959 - val_mae: 12.8899\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 275.7711 - mse: 275.7711 - mae: 13.0144 - val_loss: 265.9664 - val_mse: 265.9664 - val_mae: 12.7964\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 273.2529 - mse: 273.2529 - mae: 12.9220 - val_loss: 263.5848 - val_mse: 263.5848 - val_mae: 12.7196\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 270.7935 - mse: 270.7935 - mae: 12.8398 - val_loss: 261.4116 - val_mse: 261.4116 - val_mae: 12.6530\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 268.4604 - mse: 268.4604 - mae: 12.7668 - val_loss: 259.2688 - val_mse: 259.2688 - val_mae: 12.5861\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 266.3532 - mse: 266.3532 - mae: 12.6991 - val_loss: 257.2166 - val_mse: 257.2166 - val_mae: 12.5216\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.0852 - mse: 264.0852 - mae: 12.6209 - val_loss: 255.1761 - val_mse: 255.1761 - val_mae: 12.4585\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 261.9737 - mse: 261.9737 - mae: 12.5516 - val_loss: 253.0376 - val_mse: 253.0376 - val_mae: 12.3928\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 259.4926 - mse: 259.4926 - mae: 12.4794 - val_loss: 250.4998 - val_mse: 250.4998 - val_mae: 12.3188\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.9084 - mse: 256.9084 - mae: 12.4029 - val_loss: 248.1785 - val_mse: 248.1785 - val_mae: 12.2381\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 254.6692 - mse: 254.6692 - mae: 12.3214 - val_loss: 246.3025 - val_mse: 246.3025 - val_mae: 12.1729\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 252.7695 - mse: 252.7695 - mae: 12.2611 - val_loss: 244.4801 - val_mse: 244.4801 - val_mae: 12.1082\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 251.0075 - mse: 251.0075 - mae: 12.1943 - val_loss: 242.7998 - val_mse: 242.7998 - val_mae: 12.0523\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.2890 - mse: 249.2890 - mae: 12.1280 - val_loss: 241.1242 - val_mse: 241.1242 - val_mae: 11.9898\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 247.8120 - mse: 247.8120 - mae: 12.0756 - val_loss: 239.6730 - val_mse: 239.6730 - val_mae: 11.9363\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 246.4864 - mse: 246.4864 - mae: 12.0263 - val_loss: 238.3667 - val_mse: 238.3667 - val_mae: 11.8874\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 245.2156 - mse: 245.2156 - mae: 11.9660 - val_loss: 237.1637 - val_mse: 237.1637 - val_mae: 11.8480\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 244.0966 - mse: 244.0966 - mae: 11.9367 - val_loss: 235.9534 - val_mse: 235.9534 - val_mae: 11.7967\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 243.0915 - mse: 243.0915 - mae: 11.8885 - val_loss: 234.8707 - val_mse: 234.8707 - val_mae: 11.7552\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 242.1009 - mse: 242.1009 - mae: 11.8402 - val_loss: 233.8605 - val_mse: 233.8605 - val_mae: 11.7198\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 241.3033 - mse: 241.3033 - mae: 11.8143 - val_loss: 232.8777 - val_mse: 232.8777 - val_mae: 11.6782\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 240.4079 - mse: 240.4079 - mae: 11.7877 - val_loss: 231.7147 - val_mse: 231.7147 - val_mae: 11.6318\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 239.0832 - mse: 239.0832 - mae: 11.7317 - val_loss: 229.7635 - val_mse: 229.7635 - val_mae: 11.5705\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 237.1562 - mse: 237.1562 - mae: 11.6699 - val_loss: 227.8611 - val_mse: 227.8611 - val_mae: 11.5128\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 235.2107 - mse: 235.2107 - mae: 11.6045 - val_loss: 226.3708 - val_mse: 226.3708 - val_mae: 11.4623\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 233.9149 - mse: 233.9149 - mae: 11.5581 - val_loss: 225.1945 - val_mse: 225.1945 - val_mae: 11.4164\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 232.8305 - mse: 232.8305 - mae: 11.5071 - val_loss: 224.2441 - val_mse: 224.2441 - val_mae: 11.3925\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 231.8680 - mse: 231.8680 - mae: 11.4954 - val_loss: 223.2579 - val_mse: 223.2579 - val_mae: 11.3525\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 231.0760 - mse: 231.0760 - mae: 11.4605 - val_loss: 222.3872 - val_mse: 222.3872 - val_mae: 11.3165\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 230.2031 - mse: 230.2031 - mae: 11.4214 - val_loss: 221.5754 - val_mse: 221.5754 - val_mae: 11.2918\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 229.3345 - mse: 229.3345 - mae: 11.3946 - val_loss: 220.7270 - val_mse: 220.7270 - val_mae: 11.2629\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 228.5786 - mse: 228.5786 - mae: 11.3848 - val_loss: 219.8896 - val_mse: 219.8896 - val_mae: 11.2243\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 227.8329 - mse: 227.8329 - mae: 11.3327 - val_loss: 219.1885 - val_mse: 219.1885 - val_mae: 11.1990\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 227.2244 - mse: 227.2244 - mae: 11.3191 - val_loss: 218.6227 - val_mse: 218.6227 - val_mae: 11.1783\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 226.6501 - mse: 226.6501 - mae: 11.2915 - val_loss: 218.0939 - val_mse: 218.0939 - val_mae: 11.1584\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 226.1044 - mse: 226.1044 - mae: 11.2754 - val_loss: 217.5023 - val_mse: 217.5023 - val_mae: 11.1300\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 225.6588 - mse: 225.6588 - mae: 11.2485 - val_loss: 216.9477 - val_mse: 216.9477 - val_mae: 11.1089\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 225.0810 - mse: 225.0810 - mae: 11.2228 - val_loss: 216.5153 - val_mse: 216.5153 - val_mae: 11.0987\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 224.5980 - mse: 224.5980 - mae: 11.2174 - val_loss: 216.0479 - val_mse: 216.0479 - val_mae: 11.0822\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 224.2031 - mse: 224.2031 - mae: 11.1965 - val_loss: 215.5350 - val_mse: 215.5350 - val_mae: 11.0593\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 223.7886 - mse: 223.7886 - mae: 11.1858 - val_loss: 215.0014 - val_mse: 215.0014 - val_mae: 11.0328\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 223.3540 - mse: 223.3540 - mae: 11.1579 - val_loss: 214.5588 - val_mse: 214.5588 - val_mae: 11.0177\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 223.0144 - mse: 223.0144 - mae: 11.1513 - val_loss: 214.1766 - val_mse: 214.1766 - val_mae: 11.0023\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 222.6331 - mse: 222.6331 - mae: 11.1252 - val_loss: 213.7341 - val_mse: 213.7341 - val_mae: 10.9917\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 222.2526 - mse: 222.2526 - mae: 11.1270 - val_loss: 213.2930 - val_mse: 213.2930 - val_mae: 10.9678\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 221.8637 - mse: 221.8637 - mae: 11.1014 - val_loss: 212.9238 - val_mse: 212.9238 - val_mae: 10.9557\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 221.4773 - mse: 221.4773 - mae: 11.0979 - val_loss: 212.4969 - val_mse: 212.4969 - val_mae: 10.9375\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 221.1277 - mse: 221.1277 - mae: 11.0800 - val_loss: 212.0318 - val_mse: 212.0318 - val_mae: 10.9149\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 220.7150 - mse: 220.7150 - mae: 11.0513 - val_loss: 211.6680 - val_mse: 211.6680 - val_mae: 10.9049\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 220.3110 - mse: 220.3110 - mae: 11.0539 - val_loss: 211.2543 - val_mse: 211.2543 - val_mae: 10.8861\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 219.8424 - mse: 219.8424 - mae: 11.0266 - val_loss: 210.7139 - val_mse: 210.7139 - val_mae: 10.8723\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 219.1430 - mse: 219.1430 - mae: 11.0100 - val_loss: 210.0989 - val_mse: 210.0989 - val_mae: 10.8568\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 218.4658 - mse: 218.4658 - mae: 11.0032 - val_loss: 209.3398 - val_mse: 209.3398 - val_mae: 10.8244\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 217.7612 - mse: 217.7612 - mae: 10.9612 - val_loss: 208.7467 - val_mse: 208.7467 - val_mae: 10.8124\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 217.0911 - mse: 217.0911 - mae: 10.9560 - val_loss: 208.2263 - val_mse: 208.2263 - val_mae: 10.7880\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 216.5409 - mse: 216.5409 - mae: 10.9220 - val_loss: 207.8003 - val_mse: 207.8003 - val_mae: 10.7762\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 216.0428 - mse: 216.0428 - mae: 10.9155 - val_loss: 207.3304 - val_mse: 207.3304 - val_mae: 10.7567\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.5912 - mse: 215.5912 - mae: 10.8820 - val_loss: 206.8920 - val_mse: 206.8920 - val_mae: 10.7469\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.0793 - mse: 215.0793 - mae: 10.8807 - val_loss: 206.4319 - val_mse: 206.4319 - val_mae: 10.7286\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 214.5580 - mse: 214.5580 - mae: 10.8623 - val_loss: 205.9185 - val_mse: 205.9185 - val_mae: 10.7083\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 214.0717 - mse: 214.0717 - mae: 10.8383 - val_loss: 205.4331 - val_mse: 205.4331 - val_mae: 10.6980\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 213.4930 - mse: 213.4930 - mae: 10.8343 - val_loss: 204.9130 - val_mse: 204.9130 - val_mae: 10.6783\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.9902 - mse: 212.9902 - mae: 10.8142 - val_loss: 204.5211 - val_mse: 204.5211 - val_mae: 10.6686\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 212.4285 - mse: 212.4285 - mae: 10.8157 - val_loss: 203.9973 - val_mse: 203.9973 - val_mae: 10.6397\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 211.8981 - mse: 211.8981 - mae: 10.7686 - val_loss: 203.5051 - val_mse: 203.5051 - val_mae: 10.6228\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 211.3430 - mse: 211.3430 - mae: 10.7448 - val_loss: 202.9701 - val_mse: 202.9701 - val_mae: 10.6174\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 210.6430 - mse: 210.6430 - mae: 10.7407 - val_loss: 202.3729 - val_mse: 202.3729 - val_mae: 10.6043\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.9892 - mse: 209.9892 - mae: 10.7116 - val_loss: 201.8524 - val_mse: 201.8524 - val_mae: 10.5960\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 209.3246 - mse: 209.3246 - mae: 10.7008 - val_loss: 201.2716 - val_mse: 201.2716 - val_mae: 10.5781\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 208.8140 - mse: 208.8140 - mae: 10.6964 - val_loss: 200.6933 - val_mse: 200.6933 - val_mae: 10.5510\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 208.3812 - mse: 208.3812 - mae: 10.6658 - val_loss: 200.1705 - val_mse: 200.1705 - val_mae: 10.5294\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 207.9019 - mse: 207.9019 - mae: 10.6380 - val_loss: 199.8028 - val_mse: 199.8028 - val_mae: 10.5188\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 207.4383 - mse: 207.4383 - mae: 10.6220 - val_loss: 199.3653 - val_mse: 199.3653 - val_mae: 10.5053\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 207.0845 - mse: 207.0845 - mae: 10.6098 - val_loss: 198.9706 - val_mse: 198.9706 - val_mae: 10.4963\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 206.8844 - mse: 206.8844 - mae: 10.6063 - val_loss: 198.6745 - val_mse: 198.6745 - val_mae: 10.4906\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 206.4416 - mse: 206.4416 - mae: 10.5936 - val_loss: 198.3551 - val_mse: 198.3551 - val_mae: 10.4825\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 206.1819 - mse: 206.1819 - mae: 10.5877 - val_loss: 198.0690 - val_mse: 198.0690 - val_mae: 10.4802\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 205.8669 - mse: 205.8669 - mae: 10.5974 - val_loss: 197.6780 - val_mse: 197.6780 - val_mae: 10.4612\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 205.6314 - mse: 205.6314 - mae: 10.5885 - val_loss: 197.2505 - val_mse: 197.2505 - val_mae: 10.4372\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 205.3525 - mse: 205.3525 - mae: 10.5590 - val_loss: 196.9508 - val_mse: 196.9508 - val_mae: 10.4298\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 205.0436 - mse: 205.0436 - mae: 10.5515 - val_loss: 196.6492 - val_mse: 196.6492 - val_mae: 10.4174\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 204.8079 - mse: 204.8079 - mae: 10.5426 - val_loss: 196.3094 - val_mse: 196.3094 - val_mae: 10.4034\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 204.5495 - mse: 204.5495 - mae: 10.5283 - val_loss: 195.9740 - val_mse: 195.9740 - val_mae: 10.3920\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 204.2969 - mse: 204.2969 - mae: 10.5162 - val_loss: 195.6759 - val_mse: 195.6759 - val_mae: 10.3826\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 204.0932 - mse: 204.0932 - mae: 10.5196 - val_loss: 195.3176 - val_mse: 195.3176 - val_mae: 10.3660\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 533.2402 - mse: 533.2402 - mae: 19.0503 - val_loss: 315.8628 - val_mse: 315.8628 - val_mae: 15.2749\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 386.7249 - mse: 386.7249 - mae: 16.4419 - val_loss: 449.2648 - val_mse: 449.2648 - val_mae: 20.2769\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 596.1201 - mse: 596.1201 - mae: 22.5917 - val_loss: 368.0098 - val_mse: 368.0098 - val_mae: 17.4799\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 542.7561 - mse: 542.7561 - mae: 20.9627 - val_loss: 448.9587 - val_mse: 448.9587 - val_mae: 20.2693\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 598.2086 - mse: 598.2086 - mae: 22.6921 - val_loss: 448.6761 - val_mse: 448.6761 - val_mae: 20.2624\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 597.8662 - mse: 597.8662 - mae: 22.6845 - val_loss: 448.3388 - val_mse: 448.3388 - val_mae: 20.2540\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 597.4756 - mse: 597.4756 - mae: 22.6759 - val_loss: 447.9824 - val_mse: 447.9824 - val_mae: 20.2452\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 597.0682 - mse: 597.0682 - mae: 22.6671 - val_loss: 447.6180 - val_mse: 447.6180 - val_mae: 20.2362\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 596.6584 - mse: 596.6584 - mae: 22.6579 - val_loss: 447.2463 - val_mse: 447.2463 - val_mae: 20.2270\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 596.2418 - mse: 596.2418 - mae: 22.6487 - val_loss: 446.8711 - val_mse: 446.8711 - val_mae: 20.2177\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 595.8204 - mse: 595.8204 - mae: 22.6395 - val_loss: 446.4991 - val_mse: 446.4991 - val_mae: 20.2085\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 532.9894 - mse: 532.9894 - mae: 19.7110 - val_loss: 283.2144 - val_mse: 283.2144 - val_mae: 14.4403\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 330.1006 - mse: 330.1006 - mae: 14.4727 - val_loss: 184.4566 - val_mse: 184.4566 - val_mae: 11.0951\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 288.8507 - mse: 288.8507 - mae: 13.2531 - val_loss: 153.7384 - val_mse: 153.7384 - val_mae: 9.3158\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 255.0211 - mse: 255.0211 - mae: 11.9505 - val_loss: 171.6995 - val_mse: 171.6995 - val_mae: 10.3675\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 250.0921 - mse: 250.0921 - mae: 11.9321 - val_loss: 138.2652 - val_mse: 138.2652 - val_mae: 8.7799\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 232.7910 - mse: 232.7910 - mae: 11.3853 - val_loss: 135.8458 - val_mse: 135.8458 - val_mae: 8.4870\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 226.4014 - mse: 226.4014 - mae: 10.8466 - val_loss: 121.8107 - val_mse: 121.8107 - val_mae: 7.5561\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 225.9877 - mse: 225.9877 - mae: 10.7838 - val_loss: 170.0737 - val_mse: 170.0737 - val_mae: 10.8664\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 223.5864 - mse: 223.5864 - mae: 11.0239 - val_loss: 119.5201 - val_mse: 119.5201 - val_mae: 7.3781\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 214.2499 - mse: 214.2499 - mae: 10.5508 - val_loss: 164.8065 - val_mse: 164.8065 - val_mae: 10.6581\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 228.7394 - mse: 228.7394 - mae: 11.1443 - val_loss: 119.5400 - val_mse: 119.5400 - val_mae: 7.3611\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.7640 - mse: 209.7640 - mae: 10.4018 - val_loss: 121.3213 - val_mse: 121.3213 - val_mae: 7.5438\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.1559 - mse: 209.1559 - mae: 10.2645 - val_loss: 120.4879 - val_mse: 120.4879 - val_mae: 7.4697\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 212.1100 - mse: 212.1100 - mae: 10.5269 - val_loss: 119.3216 - val_mse: 119.3216 - val_mae: 7.4092\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 208.5036 - mse: 208.5036 - mae: 10.2904 - val_loss: 117.8133 - val_mse: 117.8133 - val_mae: 7.1501\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.2116 - mse: 215.2116 - mae: 10.6053 - val_loss: 120.8650 - val_mse: 120.8650 - val_mae: 7.4020\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 216.0697 - mse: 216.0697 - mae: 10.4985 - val_loss: 146.4796 - val_mse: 146.4796 - val_mae: 9.3352\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.9921 - mse: 212.9921 - mae: 10.5463 - val_loss: 119.1314 - val_mse: 119.1314 - val_mae: 7.2161\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 210.6122 - mse: 210.6122 - mae: 10.2198 - val_loss: 125.7187 - val_mse: 125.7187 - val_mae: 7.8480\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 210.5779 - mse: 210.5779 - mae: 10.5230 - val_loss: 117.2337 - val_mse: 117.2337 - val_mae: 7.0820\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.7732 - mse: 209.7732 - mae: 10.2404 - val_loss: 120.4866 - val_mse: 120.4866 - val_mae: 7.3712\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 207.2748 - mse: 207.2748 - mae: 10.2328 - val_loss: 117.6964 - val_mse: 117.6964 - val_mae: 7.0660\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 205.7906 - mse: 205.7906 - mae: 10.0803 - val_loss: 123.9348 - val_mse: 123.9348 - val_mae: 7.6371\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 210.7894 - mse: 210.7894 - mae: 10.1548 - val_loss: 191.6298 - val_mse: 191.6298 - val_mae: 11.9844\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 220.5432 - mse: 220.5432 - mae: 10.9776 - val_loss: 116.5129 - val_mse: 116.5129 - val_mae: 6.9823\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 212.5437 - mse: 212.5437 - mae: 10.4861 - val_loss: 122.6471 - val_mse: 122.6471 - val_mae: 7.5872\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 217.5293 - mse: 217.5293 - mae: 10.8333 - val_loss: 140.8367 - val_mse: 140.8367 - val_mae: 9.1808\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 250.7486 - mse: 250.7486 - mae: 12.0023 - val_loss: 128.2863 - val_mse: 128.2863 - val_mae: 8.1293\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 218.9210 - mse: 218.9210 - mae: 10.7490 - val_loss: 131.1693 - val_mse: 131.1693 - val_mae: 8.3494\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 209.8975 - mse: 209.8975 - mae: 10.4049 - val_loss: 120.3773 - val_mse: 120.3773 - val_mae: 7.3117\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.6060 - mse: 209.6060 - mae: 10.3480 - val_loss: 121.7019 - val_mse: 121.7019 - val_mae: 7.4987\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 210.6722 - mse: 210.6722 - mae: 10.0634 - val_loss: 138.3819 - val_mse: 138.3819 - val_mae: 8.8739\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 207.5342 - mse: 207.5342 - mae: 10.4168 - val_loss: 117.4689 - val_mse: 117.4689 - val_mae: 7.0597\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.1310 - mse: 201.1310 - mae: 9.9492 - val_loss: 131.8141 - val_mse: 131.8141 - val_mae: 8.5535\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 203.8669 - mse: 203.8669 - mae: 10.2168 - val_loss: 115.1853 - val_mse: 115.1853 - val_mae: 6.8383\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 202.8262 - mse: 202.8262 - mae: 9.8445 - val_loss: 134.6731 - val_mse: 134.6731 - val_mae: 8.5944\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 206.1548 - mse: 206.1548 - mae: 10.4316 - val_loss: 114.9555 - val_mse: 114.9555 - val_mae: 6.8256\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 218.2897 - mse: 218.2897 - mae: 10.6325 - val_loss: 146.1877 - val_mse: 146.1877 - val_mae: 9.4828\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 203.4870 - mse: 203.4870 - mae: 10.1697 - val_loss: 119.6317 - val_mse: 119.6317 - val_mae: 7.2892\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 206.2485 - mse: 206.2485 - mae: 10.2156 - val_loss: 133.7429 - val_mse: 133.7429 - val_mae: 8.7253\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 217.1804 - mse: 217.1804 - mae: 10.7536 - val_loss: 142.6296 - val_mse: 142.6296 - val_mae: 9.3500\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 225.9254 - mse: 225.9254 - mae: 10.9076 - val_loss: 127.1394 - val_mse: 127.1394 - val_mae: 7.9487\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 216.1702 - mse: 216.1702 - mae: 10.8349 - val_loss: 117.1506 - val_mse: 117.1506 - val_mae: 7.1039\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 211.6827 - mse: 211.6827 - mae: 10.2100 - val_loss: 122.1882 - val_mse: 122.1882 - val_mae: 7.5968\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 208.4001 - mse: 208.4001 - mae: 10.5037 - val_loss: 122.2416 - val_mse: 122.2416 - val_mae: 7.6396\n",
            "Epoch 45: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 608.7359 - mse: 608.7359 - mae: 18.4852 - val_loss: 254.3233 - val_mse: 254.3233 - val_mae: 12.4738\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 267.5490 - mse: 267.5490 - mae: 12.7787 - val_loss: 205.6440 - val_mse: 205.6440 - val_mae: 10.8917\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 211.4346 - mse: 211.4346 - mae: 11.0050 - val_loss: 186.0982 - val_mse: 186.0982 - val_mae: 10.1416\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.8019 - mse: 201.8019 - mae: 10.5875 - val_loss: 200.1668 - val_mse: 200.1668 - val_mae: 11.3227\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 242.3042 - mse: 242.3042 - mae: 12.3545 - val_loss: 160.7283 - val_mse: 160.7283 - val_mae: 8.9296\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 203.6282 - mse: 203.6282 - mae: 10.7116 - val_loss: 151.9635 - val_mse: 151.9635 - val_mae: 8.7819\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 196.0241 - mse: 196.0241 - mae: 10.7542 - val_loss: 146.8484 - val_mse: 146.8484 - val_mae: 8.6503\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 162.1683 - mse: 162.1683 - mae: 8.9564 - val_loss: 177.8284 - val_mse: 177.8284 - val_mae: 10.0865\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169.2146 - mse: 169.2146 - mae: 9.1863 - val_loss: 144.9742 - val_mse: 144.9742 - val_mae: 8.5476\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 165.0256 - mse: 165.0256 - mae: 9.2869 - val_loss: 147.6961 - val_mse: 147.6961 - val_mae: 8.7665\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.8481 - mse: 155.8481 - mae: 8.7384 - val_loss: 150.8890 - val_mse: 150.8890 - val_mae: 8.9600\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.8290 - mse: 155.8290 - mae: 8.6741 - val_loss: 139.1976 - val_mse: 139.1976 - val_mae: 8.0432\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 155.1027 - mse: 155.1027 - mae: 8.4565 - val_loss: 144.4024 - val_mse: 144.4024 - val_mae: 8.2887\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.0938 - mse: 155.0938 - mae: 8.4341 - val_loss: 151.3445 - val_mse: 151.3445 - val_mae: 8.6687\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 155.8731 - mse: 155.8731 - mae: 8.6871 - val_loss: 166.8204 - val_mse: 166.8204 - val_mae: 9.7494\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 169.3052 - mse: 169.3052 - mae: 9.2962 - val_loss: 140.6010 - val_mse: 140.6010 - val_mae: 8.0105\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 167.5838 - mse: 167.5838 - mae: 9.2841 - val_loss: 143.9857 - val_mse: 143.9857 - val_mae: 8.4658\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 158.5677 - mse: 158.5677 - mae: 8.8284 - val_loss: 138.5578 - val_mse: 138.5578 - val_mae: 8.2975\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.5964 - mse: 154.5964 - mae: 8.6746 - val_loss: 145.8995 - val_mse: 145.8995 - val_mae: 8.2957\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 156.0522 - mse: 156.0522 - mae: 8.3564 - val_loss: 138.5304 - val_mse: 138.5304 - val_mae: 8.0877\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 155.9834 - mse: 155.9834 - mae: 8.6772 - val_loss: 141.1606 - val_mse: 141.1606 - val_mae: 8.4455\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 152.4980 - mse: 152.4980 - mae: 8.3978 - val_loss: 149.3185 - val_mse: 149.3185 - val_mae: 8.6937\n",
            "Epoch 22: early stopping\n",
            "5/5 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 25ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 24ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 23ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 1598.6593 - mse: 1598.6593 - mae: 27.5648 - val_loss: 449.2593 - val_mse: 449.2593 - val_mae: 20.2767\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 598.6407 - mse: 598.6407 - mae: 22.7017 - val_loss: 449.2006 - val_mse: 449.2006 - val_mae: 20.2753\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 598.5750 - mse: 598.5750 - mae: 22.7002 - val_loss: 449.1422 - val_mse: 449.1422 - val_mae: 20.2739\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 598.5095 - mse: 598.5095 - mae: 22.6988 - val_loss: 449.0828 - val_mse: 449.0828 - val_mae: 20.2724\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 598.4431 - mse: 598.4431 - mae: 22.6973 - val_loss: 449.0241 - val_mse: 449.0241 - val_mae: 20.2709\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 598.3774 - mse: 598.3774 - mae: 22.6959 - val_loss: 448.9658 - val_mse: 448.9658 - val_mae: 20.2695\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 598.3121 - mse: 598.3121 - mae: 22.6944 - val_loss: 448.9073 - val_mse: 448.9073 - val_mae: 20.2681\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 598.2466 - mse: 598.2466 - mae: 22.6930 - val_loss: 448.8486 - val_mse: 448.8486 - val_mae: 20.2666\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 598.1808 - mse: 598.1808 - mae: 22.6915 - val_loss: 448.7895 - val_mse: 448.7895 - val_mae: 20.2651\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 598.1147 - mse: 598.1147 - mae: 22.6901 - val_loss: 448.7312 - val_mse: 448.7312 - val_mae: 20.2637\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 598.0494 - mse: 598.0494 - mae: 22.6886 - val_loss: 448.6726 - val_mse: 448.6726 - val_mae: 20.2623\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 381.6987 - mse: 381.6987 - mae: 16.1111 - val_loss: 225.7763 - val_mse: 225.7763 - val_mae: 12.5975\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 314.5922 - mse: 314.5922 - mae: 13.6606 - val_loss: 190.9583 - val_mse: 190.9583 - val_mae: 10.2418\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 318.9151 - mse: 318.9151 - mae: 13.7768 - val_loss: 197.1329 - val_mse: 197.1329 - val_mae: 10.7576\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 306.7042 - mse: 306.7042 - mae: 13.2601 - val_loss: 216.4000 - val_mse: 216.4000 - val_mae: 12.2219\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 311.7825 - mse: 311.7825 - mae: 13.5515 - val_loss: 188.0779 - val_mse: 188.0779 - val_mae: 9.8908\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 318.8997 - mse: 318.8997 - mae: 13.7973 - val_loss: 189.5882 - val_mse: 189.5882 - val_mae: 9.9817\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 312.7106 - mse: 312.7106 - mae: 13.4664 - val_loss: 188.7065 - val_mse: 188.7065 - val_mae: 10.0389\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 322.9647 - mse: 322.9647 - mae: 13.9134 - val_loss: 196.6956 - val_mse: 196.6956 - val_mae: 10.8163\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 301.2527 - mse: 301.2527 - mae: 12.9606 - val_loss: 237.8981 - val_mse: 237.8981 - val_mae: 13.5665\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 310.8460 - mse: 310.8460 - mae: 13.3437 - val_loss: 204.8585 - val_mse: 204.8585 - val_mae: 11.4878\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 307.7694 - mse: 307.7694 - mae: 13.3514 - val_loss: 190.8751 - val_mse: 190.8751 - val_mae: 10.1182\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 305.3466 - mse: 305.3466 - mae: 13.0908 - val_loss: 201.7753 - val_mse: 201.7753 - val_mae: 11.1893\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 306.2849 - mse: 306.2849 - mae: 13.2214 - val_loss: 192.6568 - val_mse: 192.6568 - val_mae: 10.5042\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 312.2993 - mse: 312.2993 - mae: 13.6280 - val_loss: 189.1359 - val_mse: 189.1359 - val_mae: 9.9289\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 302.6241 - mse: 302.6241 - mae: 13.0559 - val_loss: 192.2130 - val_mse: 192.2130 - val_mae: 10.4646\n",
            "Epoch 15: early stopping\n",
            "5/5 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 29ms/step - loss: 769.7183 - mse: 769.7183 - mae: 20.2990 - val_loss: 191.6077 - val_mse: 191.6077 - val_mae: 10.8476\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 181.5785 - mse: 181.5785 - mae: 10.1507 - val_loss: 153.2952 - val_mse: 153.2952 - val_mae: 8.8512\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 171.0882 - mse: 171.0882 - mae: 9.6417 - val_loss: 151.4996 - val_mse: 151.4996 - val_mae: 8.8623\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 159.4514 - mse: 159.4514 - mae: 8.9251 - val_loss: 148.8629 - val_mse: 148.8629 - val_mae: 8.7823\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 158.9560 - mse: 158.9560 - mae: 8.8866 - val_loss: 153.0349 - val_mse: 153.0349 - val_mae: 8.7627\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 160.7018 - mse: 160.7018 - mae: 9.0037 - val_loss: 148.7216 - val_mse: 148.7216 - val_mae: 8.7101\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 160.5814 - mse: 160.5814 - mae: 8.9904 - val_loss: 155.5895 - val_mse: 155.5895 - val_mae: 9.2580\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 162.4281 - mse: 162.4281 - mae: 9.0521 - val_loss: 146.1745 - val_mse: 146.1745 - val_mae: 8.6326\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.7055 - mse: 155.7055 - mae: 8.7418 - val_loss: 144.3508 - val_mse: 144.3508 - val_mae: 8.5501\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.8735 - mse: 155.8735 - mae: 8.7015 - val_loss: 144.4896 - val_mse: 144.4896 - val_mae: 8.5694\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 155.3419 - mse: 155.3419 - mae: 8.6789 - val_loss: 148.5198 - val_mse: 148.5198 - val_mae: 8.6959\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 157.8972 - mse: 157.8972 - mae: 8.7306 - val_loss: 172.0785 - val_mse: 172.0785 - val_mae: 9.8454\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 162.8178 - mse: 162.8178 - mae: 8.9622 - val_loss: 143.2167 - val_mse: 143.2167 - val_mae: 8.4806\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 158.6120 - mse: 158.6120 - mae: 8.7463 - val_loss: 142.1811 - val_mse: 142.1811 - val_mae: 8.4909\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.6865 - mse: 154.6865 - mae: 8.6990 - val_loss: 142.9025 - val_mse: 142.9025 - val_mae: 8.4314\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 155.7752 - mse: 155.7752 - mae: 8.5575 - val_loss: 146.4776 - val_mse: 146.4776 - val_mae: 8.6877\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 158.2023 - mse: 158.2023 - mae: 8.8233 - val_loss: 142.5062 - val_mse: 142.5062 - val_mae: 8.4198\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.7876 - mse: 154.7876 - mae: 8.6615 - val_loss: 140.8092 - val_mse: 140.8092 - val_mae: 8.4082\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 153.9754 - mse: 153.9754 - mae: 8.6437 - val_loss: 143.4646 - val_mse: 143.4646 - val_mae: 8.4323\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 153.9168 - mse: 153.9168 - mae: 8.5477 - val_loss: 140.9160 - val_mse: 140.9160 - val_mae: 8.3758\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 156.0674 - mse: 156.0674 - mae: 8.7080 - val_loss: 141.1971 - val_mse: 141.1971 - val_mae: 8.3595\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 153.1685 - mse: 153.1685 - mae: 8.5062 - val_loss: 143.2013 - val_mse: 143.2013 - val_mae: 8.4128\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 151.7790 - mse: 151.7790 - mae: 8.5506 - val_loss: 150.1551 - val_mse: 150.1551 - val_mae: 8.7644\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 157.4361 - mse: 157.4361 - mae: 8.5595 - val_loss: 153.6808 - val_mse: 153.6808 - val_mae: 9.0176\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 159.9449 - mse: 159.9449 - mae: 8.9455 - val_loss: 140.8750 - val_mse: 140.8750 - val_mae: 8.3352\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 152.8466 - mse: 152.8466 - mae: 8.4884 - val_loss: 152.2129 - val_mse: 152.2129 - val_mae: 8.9579\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 157.2587 - mse: 157.2587 - mae: 8.8736 - val_loss: 139.6767 - val_mse: 139.6767 - val_mae: 8.3719\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.2218 - mse: 154.2218 - mae: 8.6321 - val_loss: 139.5679 - val_mse: 139.5679 - val_mae: 8.3054\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 152.5300 - mse: 152.5300 - mae: 8.4847 - val_loss: 140.6127 - val_mse: 140.6127 - val_mae: 8.3047\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 152.4383 - mse: 152.4383 - mae: 8.4488 - val_loss: 141.8233 - val_mse: 141.8233 - val_mae: 8.3408\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 153.0494 - mse: 153.0494 - mae: 8.5502 - val_loss: 141.1727 - val_mse: 141.1727 - val_mae: 8.3163\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 152.9981 - mse: 152.9981 - mae: 8.4564 - val_loss: 146.7682 - val_mse: 146.7682 - val_mae: 8.6288\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 157.7236 - mse: 157.7236 - mae: 8.8077 - val_loss: 156.5919 - val_mse: 156.5919 - val_mae: 9.2795\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 157.3433 - mse: 157.3433 - mae: 8.6750 - val_loss: 140.3598 - val_mse: 140.3598 - val_mae: 8.2969\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 151.6182 - mse: 151.6182 - mae: 8.4329 - val_loss: 138.6290 - val_mse: 138.6290 - val_mae: 8.1124\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 156.9257 - mse: 156.9257 - mae: 8.6609 - val_loss: 159.7558 - val_mse: 159.7558 - val_mae: 9.4707\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 162.0341 - mse: 162.0341 - mae: 8.9346 - val_loss: 160.2657 - val_mse: 160.2657 - val_mae: 9.4847\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 161.7246 - mse: 161.7246 - mae: 8.9865 - val_loss: 138.8627 - val_mse: 138.8627 - val_mae: 8.1448\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 155.8302 - mse: 155.8303 - mae: 8.6438 - val_loss: 139.3564 - val_mse: 139.3564 - val_mae: 8.2877\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 150.5318 - mse: 150.5318 - mae: 8.4381 - val_loss: 138.5240 - val_mse: 138.5240 - val_mae: 8.0711\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 156.6517 - mse: 156.6517 - mae: 8.7733 - val_loss: 138.3759 - val_mse: 138.3759 - val_mae: 8.0721\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 149.9457 - mse: 149.9457 - mae: 8.2261 - val_loss: 213.9239 - val_mse: 213.9239 - val_mae: 12.5238\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 176.4453 - mse: 176.4453 - mae: 9.7050 - val_loss: 144.7367 - val_mse: 144.7367 - val_mae: 8.5009\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.9486 - mse: 154.9486 - mae: 8.5360 - val_loss: 140.6176 - val_mse: 140.6176 - val_mae: 8.1753\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 156.4442 - mse: 156.4442 - mae: 8.5814 - val_loss: 145.7913 - val_mse: 145.7913 - val_mae: 8.5495\n",
            "Epoch 45: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 577.2953 - mse: 577.2953 - mae: 19.8289 - val_loss: 468.7786 - val_mse: 468.7786 - val_mae: 17.7266\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 509.9995 - mse: 509.9995 - mae: 18.6716 - val_loss: 405.1576 - val_mse: 405.1576 - val_mae: 16.6623\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 458.6025 - mse: 458.6025 - mae: 17.7207 - val_loss: 359.3557 - val_mse: 359.3557 - val_mae: 15.8166\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 421.4363 - mse: 421.4363 - mae: 16.9552 - val_loss: 325.2114 - val_mse: 325.2114 - val_mae: 15.1246\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 393.2693 - mse: 393.2693 - mae: 16.3311 - val_loss: 298.7758 - val_mse: 298.7758 - val_mae: 14.5201\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 369.0819 - mse: 369.0819 - mae: 15.7361 - val_loss: 276.8308 - val_mse: 276.8308 - val_mae: 13.9334\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 348.3734 - mse: 348.3734 - mae: 15.1778 - val_loss: 258.6392 - val_mse: 258.6392 - val_mae: 13.4002\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 331.9118 - mse: 331.9118 - mae: 14.6869 - val_loss: 244.2251 - val_mse: 244.2251 - val_mae: 12.9490\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 318.2311 - mse: 318.2311 - mae: 14.2690 - val_loss: 232.1788 - val_mse: 232.1788 - val_mae: 12.5438\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 305.6844 - mse: 305.6844 - mae: 13.8759 - val_loss: 221.4694 - val_mse: 221.4694 - val_mae: 12.1823\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 293.4850 - mse: 293.4850 - mae: 13.4984 - val_loss: 211.4189 - val_mse: 211.4189 - val_mae: 11.8457\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 282.7490 - mse: 282.7490 - mae: 13.1597 - val_loss: 203.3912 - val_mse: 203.3912 - val_mae: 11.5528\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 274.2551 - mse: 274.2551 - mae: 12.8817 - val_loss: 196.9017 - val_mse: 196.9017 - val_mae: 11.2935\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 267.0438 - mse: 267.0438 - mae: 12.6260 - val_loss: 190.8793 - val_mse: 190.8793 - val_mae: 11.0610\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 260.4846 - mse: 260.4846 - mae: 12.3969 - val_loss: 185.6848 - val_mse: 185.6848 - val_mae: 10.8419\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 255.1786 - mse: 255.1786 - mae: 12.1913 - val_loss: 180.8346 - val_mse: 180.8346 - val_mae: 10.6246\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 250.4444 - mse: 250.4444 - mae: 12.0085 - val_loss: 175.6829 - val_mse: 175.6829 - val_mae: 10.4127\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 246.0472 - mse: 246.0472 - mae: 11.8409 - val_loss: 170.7355 - val_mse: 170.7355 - val_mae: 10.2100\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 241.6451 - mse: 241.6451 - mae: 11.6792 - val_loss: 166.8454 - val_mse: 166.8454 - val_mae: 10.0335\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 237.7422 - mse: 237.7422 - mae: 11.5526 - val_loss: 163.8251 - val_mse: 163.8251 - val_mae: 9.8921\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 233.6792 - mse: 233.6792 - mae: 11.4343 - val_loss: 160.2948 - val_mse: 160.2948 - val_mae: 9.7397\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 229.4421 - mse: 229.4421 - mae: 11.2945 - val_loss: 156.0798 - val_mse: 156.0798 - val_mae: 9.5367\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 226.0732 - mse: 226.0732 - mae: 11.1293 - val_loss: 152.0909 - val_mse: 152.0909 - val_mae: 9.3368\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 223.0547 - mse: 223.0547 - mae: 11.0042 - val_loss: 149.5129 - val_mse: 149.5129 - val_mae: 9.2098\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 220.6694 - mse: 220.6694 - mae: 10.9441 - val_loss: 147.5339 - val_mse: 147.5339 - val_mae: 9.1096\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 218.5210 - mse: 218.5210 - mae: 10.8567 - val_loss: 145.0648 - val_mse: 145.0648 - val_mae: 8.9745\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 216.8010 - mse: 216.8010 - mae: 10.7656 - val_loss: 142.9244 - val_mse: 142.9244 - val_mae: 8.8625\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.1045 - mse: 215.1045 - mae: 10.7039 - val_loss: 141.3911 - val_mse: 141.3911 - val_mae: 8.7770\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 213.7035 - mse: 213.7035 - mae: 10.6390 - val_loss: 139.7637 - val_mse: 139.7637 - val_mae: 8.6825\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 212.4485 - mse: 212.4485 - mae: 10.6122 - val_loss: 139.1193 - val_mse: 139.1193 - val_mae: 8.6451\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 211.2939 - mse: 211.2939 - mae: 10.5652 - val_loss: 137.6107 - val_mse: 137.6107 - val_mae: 8.5531\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 210.3536 - mse: 210.3536 - mae: 10.4905 - val_loss: 136.0006 - val_mse: 136.0006 - val_mae: 8.4479\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 209.4552 - mse: 209.4552 - mae: 10.4441 - val_loss: 135.5931 - val_mse: 135.5931 - val_mae: 8.4335\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 208.5907 - mse: 208.5907 - mae: 10.4493 - val_loss: 135.0332 - val_mse: 135.0332 - val_mae: 8.3967\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 207.9802 - mse: 207.9802 - mae: 10.4269 - val_loss: 134.2211 - val_mse: 134.2211 - val_mae: 8.3416\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 207.2347 - mse: 207.2347 - mae: 10.3907 - val_loss: 133.6321 - val_mse: 133.6321 - val_mae: 8.3041\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 206.6456 - mse: 206.6456 - mae: 10.3573 - val_loss: 132.8190 - val_mse: 132.8190 - val_mae: 8.2509\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 206.1200 - mse: 206.1200 - mae: 10.3140 - val_loss: 132.1924 - val_mse: 132.1924 - val_mae: 8.2058\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 205.5255 - mse: 205.5255 - mae: 10.2812 - val_loss: 131.4736 - val_mse: 131.4736 - val_mae: 8.1600\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 204.9440 - mse: 204.9440 - mae: 10.2488 - val_loss: 130.9629 - val_mse: 130.9629 - val_mae: 8.1211\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 204.3450 - mse: 204.3450 - mae: 10.2667 - val_loss: 131.4400 - val_mse: 131.4400 - val_mae: 8.1723\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 203.6740 - mse: 203.6740 - mae: 10.2702 - val_loss: 130.5598 - val_mse: 130.5598 - val_mae: 8.1114\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 202.6608 - mse: 202.6608 - mae: 10.2034 - val_loss: 129.3694 - val_mse: 129.3694 - val_mae: 8.0384\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 201.4679 - mse: 201.4679 - mae: 10.1459 - val_loss: 128.6027 - val_mse: 128.6027 - val_mae: 8.0008\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 200.8467 - mse: 200.8467 - mae: 10.1234 - val_loss: 128.1407 - val_mse: 128.1407 - val_mae: 7.9849\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 200.1315 - mse: 200.1315 - mae: 10.1065 - val_loss: 127.4437 - val_mse: 127.4437 - val_mae: 7.9510\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 199.7699 - mse: 199.7699 - mae: 10.0736 - val_loss: 126.5404 - val_mse: 126.5404 - val_mae: 7.8831\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 199.2811 - mse: 199.2811 - mae: 10.0391 - val_loss: 126.4744 - val_mse: 126.4744 - val_mae: 7.8961\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 198.8076 - mse: 198.8076 - mae: 10.0451 - val_loss: 126.3158 - val_mse: 126.3158 - val_mae: 7.8964\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 198.4763 - mse: 198.4763 - mae: 10.0560 - val_loss: 126.4606 - val_mse: 126.4606 - val_mae: 7.9226\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 198.2469 - mse: 198.2469 - mae: 10.0442 - val_loss: 125.8001 - val_mse: 125.8001 - val_mae: 7.8661\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 197.9290 - mse: 197.9290 - mae: 10.0062 - val_loss: 125.4558 - val_mse: 125.4558 - val_mae: 7.8441\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 197.7340 - mse: 197.7340 - mae: 9.9888 - val_loss: 124.8793 - val_mse: 124.8793 - val_mae: 7.7914\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 197.8087 - mse: 197.8087 - mae: 10.0154 - val_loss: 125.7926 - val_mse: 125.7926 - val_mae: 7.8923\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 197.2672 - mse: 197.2672 - mae: 9.9759 - val_loss: 124.1497 - val_mse: 124.1497 - val_mae: 7.7280\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 196.9109 - mse: 196.9109 - mae: 9.9119 - val_loss: 124.1768 - val_mse: 124.1768 - val_mae: 7.7412\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 196.7324 - mse: 196.7324 - mae: 9.9357 - val_loss: 124.2303 - val_mse: 124.2303 - val_mae: 7.7554\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 196.5294 - mse: 196.5294 - mae: 9.9420 - val_loss: 124.0361 - val_mse: 124.0361 - val_mae: 7.7382\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 196.4065 - mse: 196.4065 - mae: 9.9572 - val_loss: 124.5766 - val_mse: 124.5766 - val_mae: 7.7976\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 196.1841 - mse: 196.1841 - mae: 9.9301 - val_loss: 123.6482 - val_mse: 123.6482 - val_mae: 7.7041\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 196.0540 - mse: 196.0540 - mae: 9.8775 - val_loss: 123.2203 - val_mse: 123.2203 - val_mae: 7.6593\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 196.0142 - mse: 196.0142 - mae: 9.8892 - val_loss: 123.8350 - val_mse: 123.8350 - val_mae: 7.7294\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 195.6556 - mse: 195.6556 - mae: 9.8902 - val_loss: 123.3171 - val_mse: 123.3171 - val_mae: 7.6768\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.5776 - mse: 195.5776 - mae: 9.8504 - val_loss: 123.0069 - val_mse: 123.0069 - val_mae: 7.6427\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.3994 - mse: 195.3994 - mae: 9.8330 - val_loss: 123.3069 - val_mse: 123.3069 - val_mae: 7.6832\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.2816 - mse: 195.2816 - mae: 9.8855 - val_loss: 123.4385 - val_mse: 123.4385 - val_mae: 7.7016\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.1654 - mse: 195.1654 - mae: 9.8636 - val_loss: 122.8540 - val_mse: 122.8540 - val_mae: 7.6376\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.0530 - mse: 195.0530 - mae: 9.8497 - val_loss: 122.9074 - val_mse: 122.9074 - val_mae: 7.6469\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.1538 - mse: 195.1538 - mae: 9.8848 - val_loss: 123.4214 - val_mse: 123.4214 - val_mae: 7.7078\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 194.8116 - mse: 194.8116 - mae: 9.8487 - val_loss: 122.7012 - val_mse: 122.7012 - val_mae: 7.6285\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 194.6882 - mse: 194.6882 - mae: 9.8212 - val_loss: 122.5861 - val_mse: 122.5861 - val_mae: 7.6145\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 194.7143 - mse: 194.7143 - mae: 9.7761 - val_loss: 121.9284 - val_mse: 121.9284 - val_mae: 7.5393\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 194.6174 - mse: 194.6174 - mae: 9.7856 - val_loss: 122.6265 - val_mse: 122.6265 - val_mae: 7.6259\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 194.3323 - mse: 194.3323 - mae: 9.8052 - val_loss: 122.2602 - val_mse: 122.2602 - val_mae: 7.5880\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 194.1593 - mse: 194.1593 - mae: 9.8016 - val_loss: 122.6906 - val_mse: 122.6906 - val_mae: 7.6393\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 194.1401 - mse: 194.1401 - mae: 9.7887 - val_loss: 122.1356 - val_mse: 122.1356 - val_mae: 7.5834\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 193.6782 - mse: 193.6782 - mae: 9.7873 - val_loss: 122.2833 - val_mse: 122.2833 - val_mae: 7.5967\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 193.5059 - mse: 193.5059 - mae: 9.7952 - val_loss: 122.4917 - val_mse: 122.4917 - val_mae: 7.6130\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 193.2946 - mse: 193.2946 - mae: 9.7556 - val_loss: 122.0009 - val_mse: 122.0009 - val_mae: 7.5633\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 193.2261 - mse: 193.2261 - mae: 9.7392 - val_loss: 121.8470 - val_mse: 121.8470 - val_mae: 7.5544\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 193.2397 - mse: 193.2397 - mae: 9.7766 - val_loss: 122.5721 - val_mse: 122.5721 - val_mae: 7.6431\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 192.9347 - mse: 192.9347 - mae: 9.7524 - val_loss: 121.3193 - val_mse: 121.3193 - val_mae: 7.5098\n",
            "Epoch 82: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 28ms/step - loss: 669.2255 - mse: 669.2255 - mae: 21.3322 - val_loss: 531.0915 - val_mse: 531.0915 - val_mae: 18.9958\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 595.4772 - mse: 595.4772 - mae: 20.2299 - val_loss: 460.4991 - val_mse: 460.4991 - val_mae: 17.9011\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 538.7148 - mse: 538.7148 - mae: 19.2914 - val_loss: 405.5406 - val_mse: 405.5406 - val_mae: 16.9621\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 494.0520 - mse: 494.0520 - mae: 18.4652 - val_loss: 364.0434 - val_mse: 364.0434 - val_mae: 16.1288\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 458.7926 - mse: 458.7926 - mae: 17.7470 - val_loss: 331.9077 - val_mse: 331.9077 - val_mae: 15.3844\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 430.4435 - mse: 430.4435 - mae: 17.0946 - val_loss: 306.7921 - val_mse: 306.7921 - val_mae: 14.7417\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 407.3165 - mse: 407.3165 - mae: 16.5200 - val_loss: 285.8896 - val_mse: 285.8896 - val_mae: 14.1617\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 387.4430 - mse: 387.4430 - mae: 15.9946 - val_loss: 267.2570 - val_mse: 267.2570 - val_mae: 13.6242\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 369.3250 - mse: 369.3250 - mae: 15.5101 - val_loss: 251.9397 - val_mse: 251.9397 - val_mae: 13.1443\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 353.8755 - mse: 353.8755 - mae: 15.0669 - val_loss: 240.3466 - val_mse: 240.3466 - val_mae: 12.7470\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 341.8524 - mse: 341.8524 - mae: 14.7274 - val_loss: 230.3082 - val_mse: 230.3082 - val_mae: 12.4090\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 331.5229 - mse: 331.5229 - mae: 14.4382 - val_loss: 221.1079 - val_mse: 221.1079 - val_mae: 12.1037\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 322.5375 - mse: 322.5375 - mae: 14.1780 - val_loss: 213.0259 - val_mse: 213.0259 - val_mae: 11.8256\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 313.2276 - mse: 313.2276 - mae: 13.9079 - val_loss: 204.5773 - val_mse: 204.5773 - val_mae: 11.5405\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 303.9551 - mse: 303.9551 - mae: 13.6292 - val_loss: 198.2949 - val_mse: 198.2949 - val_mae: 11.2823\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 297.1914 - mse: 297.1914 - mae: 13.4106 - val_loss: 193.8161 - val_mse: 193.8161 - val_mae: 11.0825\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 291.3379 - mse: 291.3379 - mae: 13.2152 - val_loss: 188.2501 - val_mse: 188.2501 - val_mae: 10.8577\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 286.2152 - mse: 286.2152 - mae: 13.0383 - val_loss: 183.5518 - val_mse: 183.5518 - val_mae: 10.6578\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 281.8715 - mse: 281.8715 - mae: 12.8825 - val_loss: 179.6480 - val_mse: 179.6480 - val_mae: 10.4679\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 278.2934 - mse: 278.2934 - mae: 12.7641 - val_loss: 177.2945 - val_mse: 177.2945 - val_mae: 10.3415\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 275.3177 - mse: 275.3177 - mae: 12.6874 - val_loss: 174.5209 - val_mse: 174.5209 - val_mae: 10.2232\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 271.2284 - mse: 271.2284 - mae: 12.5470 - val_loss: 169.5207 - val_mse: 169.5207 - val_mae: 9.9974\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 266.7988 - mse: 266.7988 - mae: 12.3587 - val_loss: 165.7511 - val_mse: 165.7511 - val_mae: 9.7987\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 263.1946 - mse: 263.1946 - mae: 12.2439 - val_loss: 163.7035 - val_mse: 163.7035 - val_mae: 9.7117\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 259.9355 - mse: 259.9355 - mae: 12.1775 - val_loss: 161.6850 - val_mse: 161.6850 - val_mae: 9.6306\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 256.8970 - mse: 256.8970 - mae: 12.0713 - val_loss: 158.5918 - val_mse: 158.5918 - val_mae: 9.4748\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 254.2084 - mse: 254.2084 - mae: 11.9507 - val_loss: 156.5641 - val_mse: 156.5641 - val_mae: 9.3575\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 252.6052 - mse: 252.6052 - mae: 11.8695 - val_loss: 154.9136 - val_mse: 154.9136 - val_mae: 9.2657\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 251.3101 - mse: 251.3101 - mae: 11.8111 - val_loss: 153.9200 - val_mse: 153.9200 - val_mae: 9.2104\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 250.3775 - mse: 250.3775 - mae: 11.7695 - val_loss: 153.2370 - val_mse: 153.2370 - val_mae: 9.1736\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 249.5767 - mse: 249.5767 - mae: 11.7551 - val_loss: 152.8800 - val_mse: 152.8800 - val_mae: 9.1536\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 248.8391 - mse: 248.8391 - mae: 11.7209 - val_loss: 151.3788 - val_mse: 151.3788 - val_mae: 9.0581\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 247.0574 - mse: 247.0574 - mae: 11.6632 - val_loss: 149.9542 - val_mse: 149.9542 - val_mae: 9.0427\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 243.3943 - mse: 243.3943 - mae: 11.5937 - val_loss: 146.4459 - val_mse: 146.4459 - val_mae: 8.9013\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 239.8304 - mse: 239.8304 - mae: 11.4678 - val_loss: 144.9158 - val_mse: 144.9158 - val_mae: 8.8205\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 238.3698 - mse: 238.3698 - mae: 11.4339 - val_loss: 144.6849 - val_mse: 144.6849 - val_mae: 8.8100\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 237.3119 - mse: 237.3119 - mae: 11.3693 - val_loss: 143.2867 - val_mse: 143.2867 - val_mae: 8.7106\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 236.3080 - mse: 236.3080 - mae: 11.2926 - val_loss: 141.8607 - val_mse: 141.8607 - val_mae: 8.6324\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 234.7800 - mse: 234.7800 - mae: 11.2528 - val_loss: 140.6810 - val_mse: 140.6810 - val_mae: 8.6006\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 232.9660 - mse: 232.9660 - mae: 11.1730 - val_loss: 138.5478 - val_mse: 138.5478 - val_mae: 8.4716\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 231.3530 - mse: 231.3530 - mae: 11.1480 - val_loss: 139.2184 - val_mse: 139.2184 - val_mae: 8.5434\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 230.2260 - mse: 230.2260 - mae: 11.1451 - val_loss: 137.9998 - val_mse: 137.9998 - val_mae: 8.4678\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 229.1895 - mse: 229.1895 - mae: 11.0362 - val_loss: 135.8330 - val_mse: 135.8330 - val_mae: 8.3235\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 227.2885 - mse: 227.2885 - mae: 10.9586 - val_loss: 134.9317 - val_mse: 134.9317 - val_mae: 8.3146\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 226.6882 - mse: 226.6882 - mae: 11.0167 - val_loss: 135.9605 - val_mse: 135.9605 - val_mae: 8.4108\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 225.1173 - mse: 225.1173 - mae: 10.9281 - val_loss: 133.7438 - val_mse: 133.7438 - val_mae: 8.2475\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 224.1404 - mse: 224.1404 - mae: 10.8332 - val_loss: 132.4462 - val_mse: 132.4462 - val_mae: 8.1780\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 223.0888 - mse: 223.0888 - mae: 10.8103 - val_loss: 132.4097 - val_mse: 132.4097 - val_mae: 8.2199\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 221.9829 - mse: 221.9829 - mae: 10.8021 - val_loss: 131.6444 - val_mse: 131.6444 - val_mae: 8.1888\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 220.8471 - mse: 220.8471 - mae: 10.8076 - val_loss: 132.1397 - val_mse: 132.1397 - val_mae: 8.2878\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 219.0876 - mse: 219.0876 - mae: 10.8137 - val_loss: 129.9883 - val_mse: 129.9883 - val_mae: 8.1714\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 217.5678 - mse: 217.5678 - mae: 10.6989 - val_loss: 128.5252 - val_mse: 128.5252 - val_mae: 8.0661\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 216.6736 - mse: 216.6736 - mae: 10.6401 - val_loss: 127.9557 - val_mse: 127.9557 - val_mae: 8.0401\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 216.2251 - mse: 216.2251 - mae: 10.6417 - val_loss: 128.3327 - val_mse: 128.3327 - val_mae: 8.0966\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 215.5036 - mse: 215.5036 - mae: 10.6232 - val_loss: 127.1049 - val_mse: 127.1049 - val_mae: 8.0050\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 215.0759 - mse: 215.0759 - mae: 10.5878 - val_loss: 127.0841 - val_mse: 127.0841 - val_mae: 8.0124\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 214.8191 - mse: 214.8191 - mae: 10.5672 - val_loss: 126.4594 - val_mse: 126.4594 - val_mae: 7.9637\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 214.3508 - mse: 214.3508 - mae: 10.5640 - val_loss: 126.5811 - val_mse: 126.5811 - val_mae: 7.9800\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 214.1193 - mse: 214.1193 - mae: 10.5340 - val_loss: 126.0828 - val_mse: 126.0828 - val_mae: 7.9381\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 213.7903 - mse: 213.7903 - mae: 10.5305 - val_loss: 126.3686 - val_mse: 126.3686 - val_mae: 7.9681\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 213.6344 - mse: 213.6344 - mae: 10.5616 - val_loss: 126.3411 - val_mse: 126.3411 - val_mae: 7.9697\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 213.2750 - mse: 213.2750 - mae: 10.5164 - val_loss: 125.6014 - val_mse: 125.6014 - val_mae: 7.9011\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 213.0220 - mse: 213.0220 - mae: 10.4770 - val_loss: 125.0131 - val_mse: 125.0131 - val_mae: 7.8492\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 212.8282 - mse: 212.8282 - mae: 10.4357 - val_loss: 124.6836 - val_mse: 124.6836 - val_mae: 7.8203\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 213.0874 - mse: 213.0874 - mae: 10.3937 - val_loss: 124.3026 - val_mse: 124.3026 - val_mae: 7.7870\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 212.4476 - mse: 212.4476 - mae: 10.4219 - val_loss: 125.1234 - val_mse: 125.1234 - val_mae: 7.8708\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 212.2752 - mse: 212.2752 - mae: 10.4711 - val_loss: 125.7387 - val_mse: 125.7387 - val_mae: 7.9317\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 212.1367 - mse: 212.1367 - mae: 10.4761 - val_loss: 124.7698 - val_mse: 124.7698 - val_mae: 7.8369\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 212.0437 - mse: 212.0437 - mae: 10.4180 - val_loss: 124.6637 - val_mse: 124.6637 - val_mae: 7.8277\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 211.8268 - mse: 211.8268 - mae: 10.3919 - val_loss: 124.2248 - val_mse: 124.2248 - val_mae: 7.7869\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 211.8026 - mse: 211.8026 - mae: 10.4223 - val_loss: 124.9331 - val_mse: 124.9331 - val_mae: 7.8602\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 211.5621 - mse: 211.5621 - mae: 10.3745 - val_loss: 123.5386 - val_mse: 123.5386 - val_mae: 7.7183\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 211.3834 - mse: 211.3834 - mae: 10.3329 - val_loss: 123.8538 - val_mse: 123.8538 - val_mae: 7.7542\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 211.1752 - mse: 211.1752 - mae: 10.3528 - val_loss: 124.0493 - val_mse: 124.0493 - val_mae: 7.7754\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 211.0634 - mse: 211.0634 - mae: 10.3936 - val_loss: 125.0813 - val_mse: 125.0813 - val_mae: 7.8822\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 211.4803 - mse: 211.4803 - mae: 10.3841 - val_loss: 123.7329 - val_mse: 123.7329 - val_mae: 7.7440\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 210.8791 - mse: 210.8791 - mae: 10.3682 - val_loss: 124.2193 - val_mse: 124.2193 - val_mae: 7.7962\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 211.1900 - mse: 211.1900 - mae: 10.4506 - val_loss: 125.4915 - val_mse: 125.4915 - val_mae: 7.9281\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 210.5092 - mse: 210.5092 - mae: 10.3689 - val_loss: 123.2924 - val_mse: 123.2924 - val_mae: 7.7007\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 210.5306 - mse: 210.5306 - mae: 10.2907 - val_loss: 123.1594 - val_mse: 123.1594 - val_mae: 7.6913\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 210.4711 - mse: 210.4711 - mae: 10.3226 - val_loss: 124.0434 - val_mse: 124.0434 - val_mae: 7.7886\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 210.2426 - mse: 210.2426 - mae: 10.3041 - val_loss: 123.0284 - val_mse: 123.0284 - val_mae: 7.6810\n",
            "Epoch 82: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 44ms/step - loss: 459.3810 - mse: 459.3810 - mae: 18.0583 - val_loss: 430.1611 - val_mse: 430.1611 - val_mae: 17.5351\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 407.7469 - mse: 407.7469 - mae: 16.9621 - val_loss: 384.6225 - val_mse: 384.6225 - val_mae: 16.5167\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 369.9620 - mse: 369.9620 - mae: 16.0486 - val_loss: 350.2309 - val_mse: 350.2309 - val_mae: 15.6638\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 340.5186 - mse: 340.5186 - mae: 15.2709 - val_loss: 322.7532 - val_mse: 322.7532 - val_mae: 14.8630\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 316.7648 - mse: 316.7648 - mae: 14.5237 - val_loss: 300.0644 - val_mse: 300.0644 - val_mae: 14.0995\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 296.9627 - mse: 296.9627 - mae: 13.8431 - val_loss: 282.7625 - val_mse: 282.7625 - val_mae: 13.4778\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 281.8707 - mse: 281.8707 - mae: 13.3049 - val_loss: 268.9279 - val_mse: 268.9279 - val_mae: 12.9844\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 270.0936 - mse: 270.0936 - mae: 12.8829 - val_loss: 257.6367 - val_mse: 257.6367 - val_mae: 12.5802\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 259.6134 - mse: 259.6134 - mae: 12.5052 - val_loss: 248.0262 - val_mse: 248.0262 - val_mae: 12.2109\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 251.0035 - mse: 251.0035 - mae: 12.1742 - val_loss: 240.2324 - val_mse: 240.2324 - val_mae: 11.9216\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 244.0112 - mse: 244.0112 - mae: 11.9142 - val_loss: 234.0878 - val_mse: 234.0878 - val_mae: 11.6876\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 238.6803 - mse: 238.6803 - mae: 11.7143 - val_loss: 229.3842 - val_mse: 229.3842 - val_mae: 11.5144\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 234.6974 - mse: 234.6974 - mae: 11.5575 - val_loss: 225.4891 - val_mse: 225.4891 - val_mae: 11.3648\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 231.4789 - mse: 231.4789 - mae: 11.4234 - val_loss: 222.3682 - val_mse: 222.3682 - val_mae: 11.2482\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 228.9323 - mse: 228.9323 - mae: 11.3267 - val_loss: 219.8372 - val_mse: 219.8372 - val_mae: 11.1633\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 226.8798 - mse: 226.8798 - mae: 11.2473 - val_loss: 217.6417 - val_mse: 217.6417 - val_mae: 11.0712\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 225.1591 - mse: 225.1591 - mae: 11.1786 - val_loss: 215.7883 - val_mse: 215.7883 - val_mae: 10.9942\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 223.2572 - mse: 223.2572 - mae: 11.0890 - val_loss: 213.6862 - val_mse: 213.6862 - val_mae: 10.8926\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 221.5518 - mse: 221.5518 - mae: 10.9872 - val_loss: 211.5762 - val_mse: 211.5762 - val_mae: 10.8084\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 219.3004 - mse: 219.3004 - mae: 10.9157 - val_loss: 209.2045 - val_mse: 209.2045 - val_mae: 10.7347\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 216.6661 - mse: 216.6661 - mae: 10.8393 - val_loss: 206.2800 - val_mse: 206.2800 - val_mae: 10.6398\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 213.9337 - mse: 213.9337 - mae: 10.7515 - val_loss: 203.9344 - val_mse: 203.9344 - val_mae: 10.5562\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 211.8839 - mse: 211.8839 - mae: 10.6613 - val_loss: 202.0112 - val_mse: 202.0112 - val_mae: 10.4864\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 210.0176 - mse: 210.0176 - mae: 10.6116 - val_loss: 200.2783 - val_mse: 200.2783 - val_mae: 10.4315\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 208.5893 - mse: 208.5893 - mae: 10.5830 - val_loss: 198.9006 - val_mse: 198.9006 - val_mae: 10.3907\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 207.4940 - mse: 207.4940 - mae: 10.5343 - val_loss: 197.5252 - val_mse: 197.5252 - val_mae: 10.3265\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 206.4235 - mse: 206.4235 - mae: 10.4837 - val_loss: 196.4428 - val_mse: 196.4428 - val_mae: 10.2855\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 205.4895 - mse: 205.4895 - mae: 10.4392 - val_loss: 195.3484 - val_mse: 195.3484 - val_mae: 10.2289\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 204.6777 - mse: 204.6777 - mae: 10.3774 - val_loss: 194.4350 - val_mse: 194.4350 - val_mae: 10.1822\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 203.9996 - mse: 203.9996 - mae: 10.3521 - val_loss: 193.5516 - val_mse: 193.5516 - val_mae: 10.1511\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 203.3426 - mse: 203.3426 - mae: 10.3319 - val_loss: 192.6571 - val_mse: 192.6571 - val_mae: 10.1135\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 202.6101 - mse: 202.6101 - mae: 10.2796 - val_loss: 191.3825 - val_mse: 191.3825 - val_mae: 10.0583\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 201.2653 - mse: 201.2653 - mae: 10.2463 - val_loss: 189.3916 - val_mse: 189.3916 - val_mae: 10.0243\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 199.3195 - mse: 199.3195 - mae: 10.2198 - val_loss: 187.3865 - val_mse: 187.3865 - val_mae: 9.9559\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 197.5994 - mse: 197.5994 - mae: 10.1180 - val_loss: 185.4764 - val_mse: 185.4764 - val_mae: 9.8785\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.9334 - mse: 195.9334 - mae: 10.0479 - val_loss: 183.5504 - val_mse: 183.5504 - val_mae: 9.8191\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 194.6566 - mse: 194.6566 - mae: 10.0544 - val_loss: 182.2477 - val_mse: 182.2477 - val_mae: 9.7777\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 193.2226 - mse: 193.2226 - mae: 9.9586 - val_loss: 181.4621 - val_mse: 181.4621 - val_mae: 9.7174\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 192.7944 - mse: 192.7944 - mae: 9.8860 - val_loss: 180.9060 - val_mse: 180.9060 - val_mae: 9.6801\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 192.2744 - mse: 192.2744 - mae: 9.8756 - val_loss: 180.2373 - val_mse: 180.2373 - val_mae: 9.6606\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 191.8886 - mse: 191.8886 - mae: 9.8580 - val_loss: 179.8508 - val_mse: 179.8508 - val_mae: 9.6407\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 191.5716 - mse: 191.5716 - mae: 9.8351 - val_loss: 179.5295 - val_mse: 179.5295 - val_mae: 9.6169\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 191.1147 - mse: 191.1147 - mae: 9.8456 - val_loss: 179.1889 - val_mse: 179.1889 - val_mae: 9.6236\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 190.8844 - mse: 190.8844 - mae: 9.8820 - val_loss: 178.6978 - val_mse: 178.6978 - val_mae: 9.5959\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 190.3722 - mse: 190.3722 - mae: 9.7911 - val_loss: 178.4405 - val_mse: 178.4405 - val_mae: 9.5630\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 190.0515 - mse: 190.0515 - mae: 9.7465 - val_loss: 177.9822 - val_mse: 177.9822 - val_mae: 9.5451\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 189.9622 - mse: 189.9622 - mae: 9.7987 - val_loss: 177.7173 - val_mse: 177.7173 - val_mae: 9.5398\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 189.4991 - mse: 189.4991 - mae: 9.7375 - val_loss: 177.3923 - val_mse: 177.3923 - val_mae: 9.5086\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 189.2617 - mse: 189.2617 - mae: 9.7124 - val_loss: 176.9770 - val_mse: 176.9770 - val_mae: 9.4914\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 188.8546 - mse: 188.8546 - mae: 9.6843 - val_loss: 176.7042 - val_mse: 176.7042 - val_mae: 9.4770\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 188.6548 - mse: 188.6548 - mae: 9.6817 - val_loss: 176.4775 - val_mse: 176.4775 - val_mae: 9.4648\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 188.3770 - mse: 188.3770 - mae: 9.6642 - val_loss: 176.1531 - val_mse: 176.1531 - val_mae: 9.4466\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 188.1408 - mse: 188.1408 - mae: 9.6339 - val_loss: 175.8235 - val_mse: 175.8235 - val_mae: 9.4328\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 188.0035 - mse: 188.0035 - mae: 9.6772 - val_loss: 175.4737 - val_mse: 175.4737 - val_mae: 9.4313\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 187.7469 - mse: 187.7469 - mae: 9.6573 - val_loss: 175.1940 - val_mse: 175.1940 - val_mae: 9.4179\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 187.5361 - mse: 187.5361 - mae: 9.6285 - val_loss: 175.0152 - val_mse: 175.0152 - val_mae: 9.4056\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 187.3158 - mse: 187.3158 - mae: 9.6032 - val_loss: 174.8246 - val_mse: 174.8246 - val_mae: 9.3840\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 187.1340 - mse: 187.1340 - mae: 9.5957 - val_loss: 174.5315 - val_mse: 174.5315 - val_mae: 9.3745\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 186.9770 - mse: 186.9770 - mae: 9.5665 - val_loss: 174.3517 - val_mse: 174.3517 - val_mae: 9.3609\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 186.7732 - mse: 186.7732 - mae: 9.5927 - val_loss: 174.0767 - val_mse: 174.0767 - val_mae: 9.3618\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 186.6084 - mse: 186.6084 - mae: 9.5825 - val_loss: 174.0022 - val_mse: 174.0022 - val_mae: 9.3418\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 186.4589 - mse: 186.4589 - mae: 9.5599 - val_loss: 173.7269 - val_mse: 173.7269 - val_mae: 9.3303\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 186.2056 - mse: 186.2056 - mae: 9.5506 - val_loss: 173.4870 - val_mse: 173.4870 - val_mae: 9.3205\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 186.0888 - mse: 186.0888 - mae: 9.5122 - val_loss: 173.3647 - val_mse: 173.3647 - val_mae: 9.3034\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 185.9518 - mse: 185.9518 - mae: 9.5208 - val_loss: 172.9714 - val_mse: 172.9714 - val_mae: 9.3008\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 185.6536 - mse: 185.6536 - mae: 9.5217 - val_loss: 172.8206 - val_mse: 172.8206 - val_mae: 9.2808\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 185.4584 - mse: 185.4584 - mae: 9.4791 - val_loss: 172.6469 - val_mse: 172.6469 - val_mae: 9.2625\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 185.4889 - mse: 185.4889 - mae: 9.5314 - val_loss: 172.4610 - val_mse: 172.4610 - val_mae: 9.2540\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 185.4464 - mse: 185.4464 - mae: 9.4764 - val_loss: 172.4163 - val_mse: 172.4163 - val_mae: 9.2360\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 184.9915 - mse: 184.9915 - mae: 9.4576 - val_loss: 172.0376 - val_mse: 172.0376 - val_mae: 9.2327\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 184.8136 - mse: 184.8136 - mae: 9.4635 - val_loss: 171.9095 - val_mse: 171.9095 - val_mae: 9.2235\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 184.6635 - mse: 184.6635 - mae: 9.4536 - val_loss: 171.8275 - val_mse: 171.8275 - val_mae: 9.2081\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 184.7387 - mse: 184.7387 - mae: 9.4285 - val_loss: 171.7202 - val_mse: 171.7202 - val_mae: 9.2054\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 184.3832 - mse: 184.3832 - mae: 9.4104 - val_loss: 171.3973 - val_mse: 171.3973 - val_mae: 9.1961\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 184.2104 - mse: 184.2104 - mae: 9.4501 - val_loss: 171.3624 - val_mse: 171.3624 - val_mae: 9.1847\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 184.3110 - mse: 184.3110 - mae: 9.4298 - val_loss: 171.3858 - val_mse: 171.3858 - val_mae: 9.1713\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 184.0359 - mse: 184.0359 - mae: 9.4043 - val_loss: 171.0639 - val_mse: 171.0639 - val_mae: 9.1684\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 183.8797 - mse: 183.8797 - mae: 9.4377 - val_loss: 170.8938 - val_mse: 170.8938 - val_mae: 9.1666\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 184.0347 - mse: 184.0347 - mae: 9.4758 - val_loss: 170.7654 - val_mse: 170.7654 - val_mae: 9.1596\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 183.5420 - mse: 183.5420 - mae: 9.3740 - val_loss: 170.9370 - val_mse: 170.9370 - val_mae: 9.1651\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 183.5860 - mse: 183.5860 - mae: 9.3207 - val_loss: 170.5605 - val_mse: 170.5605 - val_mae: 9.1436\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 183.6322 - mse: 183.6322 - mae: 9.4030 - val_loss: 170.3900 - val_mse: 170.3900 - val_mae: 9.1335\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 183.3017 - mse: 183.3017 - mae: 9.3834 - val_loss: 170.2592 - val_mse: 170.2592 - val_mae: 9.1340\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 183.1427 - mse: 183.1427 - mae: 9.3506 - val_loss: 170.0269 - val_mse: 170.0269 - val_mae: 9.1250\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 183.1659 - mse: 183.1659 - mae: 9.4220 - val_loss: 169.7549 - val_mse: 169.7549 - val_mae: 9.0986\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 182.5238 - mse: 182.5238 - mae: 9.3156 - val_loss: 168.7469 - val_mse: 168.7469 - val_mae: 9.0698\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 180.4887 - mse: 180.4887 - mae: 9.2265 - val_loss: 165.3363 - val_mse: 165.3363 - val_mae: 8.9898\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 177.5909 - mse: 177.5909 - mae: 9.2093 - val_loss: 163.6129 - val_mse: 163.6129 - val_mae: 8.9183\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 176.1872 - mse: 176.1872 - mae: 9.1624 - val_loss: 162.8050 - val_mse: 162.8050 - val_mae: 8.8876\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 175.6974 - mse: 175.6974 - mae: 9.1062 - val_loss: 162.4154 - val_mse: 162.4154 - val_mae: 8.8716\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 174.8925 - mse: 174.8925 - mae: 9.0960 - val_loss: 161.2707 - val_mse: 161.2707 - val_mae: 8.8239\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 173.1574 - mse: 173.1574 - mae: 9.0880 - val_loss: 158.9012 - val_mse: 158.9012 - val_mae: 8.7424\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 171.4438 - mse: 171.4438 - mae: 9.0291 - val_loss: 157.8347 - val_mse: 157.8347 - val_mae: 8.7013\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 170.7005 - mse: 170.7005 - mae: 8.9822 - val_loss: 157.4686 - val_mse: 157.4686 - val_mae: 8.6807\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 170.4220 - mse: 170.4220 - mae: 8.9176 - val_loss: 157.3225 - val_mse: 157.3225 - val_mae: 8.6663\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 170.2839 - mse: 170.2839 - mae: 8.8828 - val_loss: 156.9272 - val_mse: 156.9272 - val_mae: 8.6516\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 170.1836 - mse: 170.1837 - mae: 8.9347 - val_loss: 156.8939 - val_mse: 156.8939 - val_mae: 8.6284\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 169.6751 - mse: 169.6751 - mae: 8.8659 - val_loss: 156.7928 - val_mse: 156.7928 - val_mae: 8.6365\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 169.6613 - mse: 169.6613 - mae: 8.8324 - val_loss: 156.5858 - val_mse: 156.5858 - val_mae: 8.6329\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 169.7310 - mse: 169.7310 - mae: 8.8888 - val_loss: 156.4127 - val_mse: 156.4127 - val_mae: 8.6134\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 3272.7617 - mse: 3272.7617 - mae: 31.2451 - val_loss: 393.9520 - val_mse: 393.9520 - val_mae: 18.2472\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 517.9700 - mse: 517.9700 - mae: 20.2997 - val_loss: 390.2607 - val_mse: 390.2607 - val_mae: 18.0607\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 517.2631 - mse: 517.2631 - mae: 20.3095 - val_loss: 388.7586 - val_mse: 388.7586 - val_mae: 17.9654\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 515.7912 - mse: 515.7912 - mae: 20.2154 - val_loss: 391.9187 - val_mse: 391.9187 - val_mae: 18.2817\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 514.3058 - mse: 514.3058 - mae: 20.2049 - val_loss: 388.0389 - val_mse: 388.0389 - val_mae: 17.9132\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 516.4542 - mse: 516.4542 - mae: 20.2502 - val_loss: 386.0322 - val_mse: 386.0322 - val_mae: 17.8182\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 513.0811 - mse: 513.0811 - mae: 20.1175 - val_loss: 385.2109 - val_mse: 385.2109 - val_mae: 17.7948\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 513.5750 - mse: 513.5750 - mae: 20.1534 - val_loss: 385.7234 - val_mse: 385.7234 - val_mae: 17.9525\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 512.5353 - mse: 512.5353 - mae: 20.1113 - val_loss: 386.6136 - val_mse: 386.6136 - val_mae: 18.0560\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 511.9429 - mse: 511.9429 - mae: 20.1020 - val_loss: 384.3393 - val_mse: 384.3393 - val_mae: 17.7439\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 511.9549 - mse: 511.9549 - mae: 20.0929 - val_loss: 384.0745 - val_mse: 384.0745 - val_mae: 17.8016\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 512.6909 - mse: 512.6909 - mae: 20.1238 - val_loss: 383.7631 - val_mse: 383.7631 - val_mae: 17.7820\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 511.6844 - mse: 511.6844 - mae: 20.0688 - val_loss: 391.1447 - val_mse: 391.1447 - val_mae: 18.3230\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 514.0403 - mse: 514.0403 - mae: 20.2190 - val_loss: 384.3156 - val_mse: 384.3156 - val_mae: 17.7680\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 511.4431 - mse: 511.4431 - mae: 20.0761 - val_loss: 384.3587 - val_mse: 384.3587 - val_mae: 17.9508\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 512.1711 - mse: 512.1711 - mae: 20.1123 - val_loss: 382.9887 - val_mse: 382.9887 - val_mae: 17.7876\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 511.4199 - mse: 511.4199 - mae: 20.0486 - val_loss: 384.2621 - val_mse: 384.2621 - val_mae: 17.9599\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 511.7031 - mse: 511.7031 - mae: 20.1100 - val_loss: 382.3183 - val_mse: 382.3183 - val_mae: 17.6966\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 510.6112 - mse: 510.6112 - mae: 20.0473 - val_loss: 383.0465 - val_mse: 383.0465 - val_mae: 17.8729\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 511.1003 - mse: 511.1003 - mae: 20.0824 - val_loss: 382.0389 - val_mse: 382.0389 - val_mae: 17.7597\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 510.4366 - mse: 510.4366 - mae: 20.0324 - val_loss: 381.5927 - val_mse: 381.5927 - val_mae: 17.6947\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 510.8664 - mse: 510.8664 - mae: 20.0650 - val_loss: 383.9104 - val_mse: 383.9104 - val_mae: 17.9779\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 511.0004 - mse: 511.0004 - mae: 20.0948 - val_loss: 381.4560 - val_mse: 381.4560 - val_mae: 17.7598\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 509.8756 - mse: 509.8756 - mae: 20.0339 - val_loss: 382.7582 - val_mse: 382.7582 - val_mae: 17.9125\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 510.5032 - mse: 510.5032 - mae: 20.0680 - val_loss: 381.7091 - val_mse: 381.7091 - val_mae: 17.8296\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 509.5390 - mse: 509.5390 - mae: 20.0554 - val_loss: 385.0254 - val_mse: 385.0254 - val_mae: 18.0684\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 509.5940 - mse: 509.5940 - mae: 20.0893 - val_loss: 379.9742 - val_mse: 379.9742 - val_mae: 17.6613\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 509.0033 - mse: 509.0033 - mae: 19.9919 - val_loss: 379.9625 - val_mse: 379.9625 - val_mae: 17.7093\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 509.3710 - mse: 509.3710 - mae: 20.0620 - val_loss: 380.1701 - val_mse: 380.1701 - val_mae: 17.7491\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 508.0587 - mse: 508.0587 - mae: 20.0248 - val_loss: 379.5588 - val_mse: 379.5588 - val_mae: 17.6483\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 507.3845 - mse: 507.3845 - mae: 19.9989 - val_loss: 380.6169 - val_mse: 380.6169 - val_mae: 17.6971\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 507.9377 - mse: 507.9377 - mae: 19.9760 - val_loss: 384.2955 - val_mse: 384.2955 - val_mae: 18.1187\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 508.2773 - mse: 508.2773 - mae: 20.0344 - val_loss: 378.4662 - val_mse: 378.4662 - val_mae: 17.6542\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 505.6107 - mse: 505.6107 - mae: 19.9443 - val_loss: 378.1511 - val_mse: 378.1511 - val_mae: 17.6854\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 504.3969 - mse: 504.3969 - mae: 19.8722 - val_loss: 384.9773 - val_mse: 384.9773 - val_mae: 18.1921\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 505.6448 - mse: 505.6448 - mae: 19.9659 - val_loss: 378.0000 - val_mse: 378.0000 - val_mae: 17.7408\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 504.1938 - mse: 504.1938 - mae: 19.9247 - val_loss: 376.8260 - val_mse: 376.8260 - val_mae: 17.5975\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 504.5743 - mse: 504.5743 - mae: 19.9250 - val_loss: 380.4780 - val_mse: 380.4780 - val_mae: 17.7140\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 505.6192 - mse: 505.6192 - mae: 19.9447 - val_loss: 380.1115 - val_mse: 380.1115 - val_mae: 17.9555\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 503.3988 - mse: 503.3988 - mae: 19.9009 - val_loss: 376.0184 - val_mse: 376.0184 - val_mae: 17.6453\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 503.4270 - mse: 503.4270 - mae: 19.8842 - val_loss: 375.8731 - val_mse: 375.8731 - val_mae: 17.6591\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 502.0713 - mse: 502.0713 - mae: 19.8354 - val_loss: 374.8301 - val_mse: 374.8301 - val_mae: 17.5337\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 502.3281 - mse: 502.3281 - mae: 19.8546 - val_loss: 374.4622 - val_mse: 374.4622 - val_mae: 17.5278\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 501.6234 - mse: 501.6234 - mae: 19.8108 - val_loss: 374.1516 - val_mse: 374.1516 - val_mae: 17.5274\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 500.9803 - mse: 500.9803 - mae: 19.8153 - val_loss: 374.8723 - val_mse: 374.8723 - val_mae: 17.6482\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 500.7824 - mse: 500.7824 - mae: 19.8083 - val_loss: 373.5331 - val_mse: 373.5331 - val_mae: 17.4787\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 502.2040 - mse: 502.2040 - mae: 19.8280 - val_loss: 373.3103 - val_mse: 373.3103 - val_mae: 17.4561\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 500.5030 - mse: 500.5030 - mae: 19.7602 - val_loss: 374.2283 - val_mse: 374.2283 - val_mae: 17.6470\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 500.3040 - mse: 500.3040 - mae: 19.7728 - val_loss: 378.6277 - val_mse: 378.6277 - val_mae: 17.9494\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 500.4983 - mse: 500.4983 - mae: 19.7903 - val_loss: 372.2218 - val_mse: 372.2218 - val_mae: 17.4299\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 500.0791 - mse: 500.0791 - mae: 19.7470 - val_loss: 374.5338 - val_mse: 374.5338 - val_mae: 17.7161\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 500.1665 - mse: 500.1665 - mae: 19.7680 - val_loss: 373.8997 - val_mse: 373.8997 - val_mae: 17.6782\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 498.6982 - mse: 498.6982 - mae: 19.7240 - val_loss: 372.3533 - val_mse: 372.3533 - val_mae: 17.5370\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 499.5163 - mse: 499.5163 - mae: 19.7578 - val_loss: 371.8981 - val_mse: 371.8981 - val_mae: 17.4041\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 499.9356 - mse: 499.9355 - mae: 19.7537 - val_loss: 372.6828 - val_mse: 372.6828 - val_mae: 17.6042\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 498.0177 - mse: 498.0177 - mae: 19.6738 - val_loss: 371.3766 - val_mse: 371.3766 - val_mae: 17.4754\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 498.1478 - mse: 498.1478 - mae: 19.6587 - val_loss: 371.3292 - val_mse: 371.3292 - val_mae: 17.4908\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 498.6315 - mse: 498.6315 - mae: 19.7055 - val_loss: 370.8912 - val_mse: 370.8912 - val_mae: 17.4469\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 497.7984 - mse: 497.7984 - mae: 19.6590 - val_loss: 370.7755 - val_mse: 370.7755 - val_mae: 17.4514\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 498.1638 - mse: 498.1638 - mae: 19.6874 - val_loss: 370.8603 - val_mse: 370.8603 - val_mae: 17.4788\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 500.6711 - mse: 500.6711 - mae: 19.7588 - val_loss: 370.1863 - val_mse: 370.1863 - val_mae: 17.3279\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 497.1937 - mse: 497.1937 - mae: 19.6294 - val_loss: 369.9575 - val_mse: 369.9575 - val_mae: 17.3218\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 497.1041 - mse: 497.1041 - mae: 19.6246 - val_loss: 370.1670 - val_mse: 370.1670 - val_mae: 17.4341\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 497.3112 - mse: 497.3112 - mae: 19.6247 - val_loss: 372.1947 - val_mse: 372.1947 - val_mae: 17.6317\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 497.5922 - mse: 497.5922 - mae: 19.6801 - val_loss: 370.7823 - val_mse: 370.7823 - val_mae: 17.3491\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 497.6300 - mse: 497.6300 - mae: 19.6027 - val_loss: 369.2842 - val_mse: 369.2842 - val_mae: 17.3574\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 496.5199 - mse: 496.5199 - mae: 19.5803 - val_loss: 370.5316 - val_mse: 370.5316 - val_mae: 17.5060\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 496.4852 - mse: 496.4852 - mae: 19.6131 - val_loss: 369.5528 - val_mse: 369.5528 - val_mae: 17.4164\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 497.4586 - mse: 497.4586 - mae: 19.6426 - val_loss: 369.0079 - val_mse: 369.0079 - val_mae: 17.3540\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 495.9522 - mse: 495.9522 - mae: 19.5695 - val_loss: 368.8700 - val_mse: 368.8700 - val_mae: 17.3520\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 496.4095 - mse: 496.4095 - mae: 19.6135 - val_loss: 369.0573 - val_mse: 369.0573 - val_mae: 17.2784\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 496.4186 - mse: 496.4186 - mae: 19.5649 - val_loss: 368.8835 - val_mse: 368.8835 - val_mae: 17.3714\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 495.8406 - mse: 495.8406 - mae: 19.5707 - val_loss: 368.3188 - val_mse: 368.3188 - val_mae: 17.2587\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 495.7744 - mse: 495.7744 - mae: 19.5369 - val_loss: 373.9969 - val_mse: 373.9969 - val_mae: 17.7521\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 497.2468 - mse: 497.2468 - mae: 19.6460 - val_loss: 368.0097 - val_mse: 368.0097 - val_mae: 17.2624\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 495.6777 - mse: 495.6777 - mae: 19.5335 - val_loss: 368.4169 - val_mse: 368.4169 - val_mae: 17.2544\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 498.1933 - mse: 498.1933 - mae: 19.6202 - val_loss: 370.5728 - val_mse: 370.5728 - val_mae: 17.5489\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 495.1608 - mse: 495.1608 - mae: 19.5314 - val_loss: 368.4127 - val_mse: 368.4127 - val_mae: 17.3611\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 495.5092 - mse: 495.5092 - mae: 19.5617 - val_loss: 367.8892 - val_mse: 367.8892 - val_mae: 17.3012\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 496.3319 - mse: 496.3319 - mae: 19.5814 - val_loss: 370.7772 - val_mse: 370.7772 - val_mae: 17.5681\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 494.9095 - mse: 494.9095 - mae: 19.5597 - val_loss: 367.4551 - val_mse: 367.4551 - val_mae: 17.2254\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 494.9697 - mse: 494.9697 - mae: 19.4845 - val_loss: 368.0503 - val_mse: 368.0503 - val_mae: 17.3366\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 494.8633 - mse: 494.8633 - mae: 19.5040 - val_loss: 367.4520 - val_mse: 367.4520 - val_mae: 17.2718\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 495.0258 - mse: 495.0258 - mae: 19.5030 - val_loss: 368.3763 - val_mse: 368.3763 - val_mae: 17.3769\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 494.9225 - mse: 494.9225 - mae: 19.5016 - val_loss: 367.0165 - val_mse: 367.0165 - val_mae: 17.2204\n",
            "Epoch 85: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 2564.4250 - mse: 2564.4250 - mae: 29.0280 - val_loss: 342.8925 - val_mse: 342.8925 - val_mae: 16.7069\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 464.6601 - mse: 464.6601 - mae: 18.5392 - val_loss: 289.5444 - val_mse: 289.5444 - val_mae: 14.8590\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 437.7137 - mse: 437.7137 - mae: 17.7252 - val_loss: 281.4173 - val_mse: 281.4173 - val_mae: 14.5219\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 427.7780 - mse: 427.7780 - mae: 17.4765 - val_loss: 295.1811 - val_mse: 295.1811 - val_mae: 15.1197\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 422.9738 - mse: 422.9738 - mae: 17.3799 - val_loss: 282.3945 - val_mse: 282.3945 - val_mae: 14.5199\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 420.6380 - mse: 420.6380 - mae: 17.3196 - val_loss: 288.5798 - val_mse: 288.5798 - val_mae: 14.8963\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 417.1804 - mse: 417.1804 - mae: 17.1571 - val_loss: 286.1387 - val_mse: 286.1387 - val_mae: 14.7181\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 422.8406 - mse: 422.8406 - mae: 17.2691 - val_loss: 278.8290 - val_mse: 278.8290 - val_mae: 14.4911\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 407.3288 - mse: 407.3288 - mae: 16.9071 - val_loss: 292.5406 - val_mse: 292.5406 - val_mae: 15.0901\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 403.6344 - mse: 403.6344 - mae: 16.7839 - val_loss: 299.5504 - val_mse: 299.5504 - val_mae: 15.4160\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 407.5983 - mse: 407.5983 - mae: 16.9000 - val_loss: 281.4904 - val_mse: 281.4904 - val_mae: 14.7743\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 401.0747 - mse: 401.0747 - mae: 16.6928 - val_loss: 276.3664 - val_mse: 276.3664 - val_mae: 14.5159\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 397.6418 - mse: 397.6418 - mae: 16.6470 - val_loss: 289.4681 - val_mse: 289.4681 - val_mae: 15.0779\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 400.4554 - mse: 400.4554 - mae: 16.6689 - val_loss: 264.3900 - val_mse: 264.3900 - val_mae: 13.9325\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 391.7332 - mse: 391.7332 - mae: 16.4531 - val_loss: 262.4798 - val_mse: 262.4798 - val_mae: 13.9163\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 391.7318 - mse: 391.7318 - mae: 16.4401 - val_loss: 260.7599 - val_mse: 260.7599 - val_mae: 13.8949\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 390.2789 - mse: 390.2789 - mae: 16.3580 - val_loss: 284.4422 - val_mse: 284.4422 - val_mae: 14.9094\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 392.1767 - mse: 392.1767 - mae: 16.4273 - val_loss: 264.3052 - val_mse: 264.3052 - val_mae: 14.1424\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 381.5418 - mse: 381.5418 - mae: 16.0731 - val_loss: 275.2841 - val_mse: 275.2841 - val_mae: 14.5676\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 383.1495 - mse: 383.1495 - mae: 16.1548 - val_loss: 282.1537 - val_mse: 282.1537 - val_mae: 14.7952\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.8865 - mse: 376.8865 - mae: 16.0060 - val_loss: 252.1720 - val_mse: 252.1720 - val_mae: 13.5025\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 377.9235 - mse: 377.9235 - mae: 15.9933 - val_loss: 252.9904 - val_mse: 252.9904 - val_mae: 13.6962\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 373.2659 - mse: 373.2659 - mae: 15.8531 - val_loss: 266.5762 - val_mse: 266.5762 - val_mae: 14.3783\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 373.5336 - mse: 373.5336 - mae: 15.8929 - val_loss: 271.9949 - val_mse: 271.9949 - val_mae: 14.1985\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 372.0672 - mse: 372.0672 - mae: 15.8659 - val_loss: 256.5115 - val_mse: 256.5115 - val_mae: 13.9505\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 369.6815 - mse: 369.6815 - mae: 15.7485 - val_loss: 260.5982 - val_mse: 260.5982 - val_mae: 14.0714\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 368.0435 - mse: 368.0435 - mae: 15.7251 - val_loss: 247.9716 - val_mse: 247.9716 - val_mae: 13.5657\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 371.2410 - mse: 371.2410 - mae: 15.8060 - val_loss: 247.3368 - val_mse: 247.3368 - val_mae: 13.4181\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 364.5470 - mse: 364.5470 - mae: 15.5874 - val_loss: 246.9472 - val_mse: 246.9472 - val_mae: 13.5503\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 362.3750 - mse: 362.3750 - mae: 15.5190 - val_loss: 247.6461 - val_mse: 247.6461 - val_mae: 13.5523\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 361.6979 - mse: 361.6979 - mae: 15.5138 - val_loss: 247.3976 - val_mse: 247.3976 - val_mae: 13.5202\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 365.8502 - mse: 365.8502 - mae: 15.5647 - val_loss: 263.3130 - val_mse: 263.3130 - val_mae: 14.1437\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 359.2724 - mse: 359.2724 - mae: 15.4611 - val_loss: 248.0842 - val_mse: 248.0842 - val_mae: 13.5398\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 354.1785 - mse: 354.1785 - mae: 15.2811 - val_loss: 249.1666 - val_mse: 249.1666 - val_mae: 13.6518\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 352.9944 - mse: 352.9944 - mae: 15.2538 - val_loss: 242.7237 - val_mse: 242.7237 - val_mae: 13.4737\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 350.0549 - mse: 350.0549 - mae: 15.1396 - val_loss: 239.5520 - val_mse: 239.5520 - val_mae: 12.9424\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 346.7238 - mse: 346.7238 - mae: 15.0179 - val_loss: 223.4933 - val_mse: 223.4933 - val_mae: 12.4698\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 343.6350 - mse: 343.6350 - mae: 14.9394 - val_loss: 222.5507 - val_mse: 222.5507 - val_mae: 12.3392\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 337.7102 - mse: 337.7102 - mae: 14.7233 - val_loss: 222.0546 - val_mse: 222.0546 - val_mae: 12.3758\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 333.8645 - mse: 333.8645 - mae: 14.6467 - val_loss: 230.9591 - val_mse: 230.9591 - val_mae: 12.7572\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 335.9007 - mse: 335.9007 - mae: 14.6564 - val_loss: 215.8977 - val_mse: 215.8977 - val_mae: 11.8953\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 331.9838 - mse: 331.9838 - mae: 14.5692 - val_loss: 215.6053 - val_mse: 215.6053 - val_mae: 11.9956\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 331.5617 - mse: 331.5617 - mae: 14.5169 - val_loss: 215.8473 - val_mse: 215.8473 - val_mae: 11.8980\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 332.2919 - mse: 332.2919 - mae: 14.5114 - val_loss: 220.3890 - val_mse: 220.3890 - val_mae: 12.2525\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 328.2327 - mse: 328.2327 - mae: 14.4547 - val_loss: 212.0953 - val_mse: 212.0953 - val_mae: 11.6393\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 330.8233 - mse: 330.8233 - mae: 14.4890 - val_loss: 212.4731 - val_mse: 212.4731 - val_mae: 11.7982\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 332.7867 - mse: 332.7867 - mae: 14.5378 - val_loss: 211.1459 - val_mse: 211.1459 - val_mae: 11.7309\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 329.9249 - mse: 329.9249 - mae: 14.4131 - val_loss: 218.5813 - val_mse: 218.5813 - val_mae: 12.2065\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 328.2285 - mse: 328.2285 - mae: 14.3533 - val_loss: 256.4807 - val_mse: 256.4807 - val_mae: 13.7511\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 335.1516 - mse: 335.1516 - mae: 14.7019 - val_loss: 213.2691 - val_mse: 213.2691 - val_mae: 11.9614\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 329.9189 - mse: 329.9189 - mae: 14.4302 - val_loss: 215.9406 - val_mse: 215.9406 - val_mae: 11.9369\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 333.8104 - mse: 333.8104 - mae: 14.5828 - val_loss: 216.7236 - val_mse: 216.7236 - val_mae: 11.9762\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 326.3205 - mse: 326.3205 - mae: 14.3120 - val_loss: 211.4944 - val_mse: 211.4944 - val_mae: 11.7890\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 325.9135 - mse: 325.9135 - mae: 14.3304 - val_loss: 211.9056 - val_mse: 211.9056 - val_mae: 11.8393\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 324.5034 - mse: 324.5034 - mae: 14.2284 - val_loss: 228.5257 - val_mse: 228.5257 - val_mae: 12.6048\n",
            "Epoch 55: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: 1510.8193 - mse: 1510.8193 - mae: 27.0464 - val_loss: 457.0741 - val_mse: 457.0741 - val_mae: 19.2930\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 506.4329 - mse: 506.4329 - mae: 20.1528 - val_loss: 458.0275 - val_mse: 458.0275 - val_mae: 19.5622\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 483.0490 - mse: 483.0490 - mae: 19.7583 - val_loss: 459.2547 - val_mse: 459.2547 - val_mae: 19.6535\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 480.1266 - mse: 480.1266 - mae: 19.7424 - val_loss: 453.9941 - val_mse: 453.9941 - val_mae: 19.4063\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 477.6070 - mse: 477.6070 - mae: 19.6574 - val_loss: 454.9915 - val_mse: 454.9915 - val_mae: 19.4455\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 476.7382 - mse: 476.7382 - mae: 19.6199 - val_loss: 454.8471 - val_mse: 454.8471 - val_mae: 19.4314\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 476.3586 - mse: 476.3586 - mae: 19.6160 - val_loss: 453.0150 - val_mse: 453.0150 - val_mae: 19.3681\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 475.8638 - mse: 475.8638 - mae: 19.5703 - val_loss: 452.7219 - val_mse: 452.7219 - val_mae: 19.3673\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 474.2128 - mse: 474.2128 - mae: 19.5705 - val_loss: 452.8839 - val_mse: 452.8839 - val_mae: 19.3420\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 474.7433 - mse: 474.7433 - mae: 19.5871 - val_loss: 453.7096 - val_mse: 453.7096 - val_mae: 19.2823\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 473.5570 - mse: 473.5570 - mae: 19.4815 - val_loss: 451.0242 - val_mse: 451.0242 - val_mae: 19.2016\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 472.5000 - mse: 472.5000 - mae: 19.4877 - val_loss: 447.9248 - val_mse: 447.9248 - val_mae: 19.1128\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 471.0023 - mse: 471.0023 - mae: 19.4277 - val_loss: 449.7743 - val_mse: 449.7743 - val_mae: 19.2920\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 470.2169 - mse: 470.2169 - mae: 19.4409 - val_loss: 450.6156 - val_mse: 450.6156 - val_mae: 19.2392\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 470.0326 - mse: 470.0326 - mae: 19.4081 - val_loss: 449.1246 - val_mse: 449.1246 - val_mae: 19.2797\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 468.5095 - mse: 468.5095 - mae: 19.3665 - val_loss: 449.3785 - val_mse: 449.3785 - val_mae: 19.3485\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 468.0522 - mse: 468.0522 - mae: 19.3665 - val_loss: 448.3967 - val_mse: 448.3967 - val_mae: 19.1428\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 467.6455 - mse: 467.6455 - mae: 19.3294 - val_loss: 447.9146 - val_mse: 447.9146 - val_mae: 19.2869\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 466.6156 - mse: 466.6156 - mae: 19.3315 - val_loss: 447.1221 - val_mse: 447.1221 - val_mae: 19.2571\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 465.8326 - mse: 465.8326 - mae: 19.3074 - val_loss: 445.7602 - val_mse: 445.7602 - val_mae: 19.1414\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 465.4669 - mse: 465.4669 - mae: 19.2859 - val_loss: 445.4490 - val_mse: 445.4490 - val_mae: 19.0626\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 463.6734 - mse: 463.6734 - mae: 19.2126 - val_loss: 443.3315 - val_mse: 443.3315 - val_mae: 19.0109\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 462.5212 - mse: 462.5212 - mae: 19.1934 - val_loss: 444.2836 - val_mse: 444.2836 - val_mae: 19.1377\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 462.8597 - mse: 462.8597 - mae: 19.2086 - val_loss: 442.6744 - val_mse: 442.6744 - val_mae: 19.0188\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 461.0882 - mse: 461.0882 - mae: 19.1522 - val_loss: 439.8997 - val_mse: 439.8997 - val_mae: 18.8951\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 460.1737 - mse: 460.1737 - mae: 19.1409 - val_loss: 440.2050 - val_mse: 440.2050 - val_mae: 18.8710\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 459.7878 - mse: 459.7878 - mae: 19.1165 - val_loss: 439.2342 - val_mse: 439.2342 - val_mae: 18.8224\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 458.8898 - mse: 458.8898 - mae: 19.1292 - val_loss: 441.9342 - val_mse: 441.9342 - val_mae: 18.9188\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 457.1272 - mse: 457.1272 - mae: 19.0694 - val_loss: 437.4396 - val_mse: 437.4396 - val_mae: 18.8481\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 456.2949 - mse: 456.2949 - mae: 19.0567 - val_loss: 438.2848 - val_mse: 438.2848 - val_mae: 18.8860\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 455.3289 - mse: 455.3289 - mae: 19.0219 - val_loss: 437.2388 - val_mse: 437.2388 - val_mae: 18.8476\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 453.8756 - mse: 453.8756 - mae: 18.9922 - val_loss: 436.9316 - val_mse: 436.9316 - val_mae: 18.7963\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 452.1901 - mse: 452.1901 - mae: 18.9421 - val_loss: 436.4756 - val_mse: 436.4756 - val_mae: 18.7937\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 451.7607 - mse: 451.7607 - mae: 18.9315 - val_loss: 436.6408 - val_mse: 436.6408 - val_mae: 18.7407\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 451.6671 - mse: 451.6671 - mae: 18.9437 - val_loss: 436.5598 - val_mse: 436.5598 - val_mae: 18.7469\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 449.4182 - mse: 449.4182 - mae: 18.8573 - val_loss: 433.8884 - val_mse: 433.8884 - val_mae: 18.6636\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 447.3030 - mse: 447.3030 - mae: 18.8369 - val_loss: 433.4537 - val_mse: 433.4537 - val_mae: 18.6627\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 446.3865 - mse: 446.3865 - mae: 18.7775 - val_loss: 433.0287 - val_mse: 433.0287 - val_mae: 18.6331\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 444.8241 - mse: 444.8241 - mae: 18.7563 - val_loss: 432.1169 - val_mse: 432.1169 - val_mae: 18.6317\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 444.3956 - mse: 444.3956 - mae: 18.7361 - val_loss: 436.3342 - val_mse: 436.3342 - val_mae: 18.7020\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 444.1976 - mse: 444.1976 - mae: 18.7282 - val_loss: 434.1715 - val_mse: 434.1715 - val_mae: 18.6753\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 442.2185 - mse: 442.2185 - mae: 18.6488 - val_loss: 430.8811 - val_mse: 430.8811 - val_mae: 18.5860\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 440.6310 - mse: 440.6310 - mae: 18.6067 - val_loss: 429.4291 - val_mse: 429.4291 - val_mae: 18.5513\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 438.6550 - mse: 438.6550 - mae: 18.5579 - val_loss: 428.8746 - val_mse: 428.8746 - val_mae: 18.5341\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 437.9734 - mse: 437.9734 - mae: 18.5402 - val_loss: 429.7064 - val_mse: 429.7064 - val_mae: 18.5240\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 435.7205 - mse: 435.7205 - mae: 18.4836 - val_loss: 428.0529 - val_mse: 428.0529 - val_mae: 18.4779\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 434.6367 - mse: 434.6367 - mae: 18.4266 - val_loss: 426.4798 - val_mse: 426.4798 - val_mae: 18.4468\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 432.4527 - mse: 432.4527 - mae: 18.3183 - val_loss: 425.3923 - val_mse: 425.3923 - val_mae: 18.4072\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 431.5182 - mse: 431.5182 - mae: 18.3273 - val_loss: 424.1934 - val_mse: 424.1934 - val_mae: 18.3670\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 429.9233 - mse: 429.9233 - mae: 18.2539 - val_loss: 425.9156 - val_mse: 425.9156 - val_mae: 18.3908\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 429.1234 - mse: 429.1234 - mae: 18.2414 - val_loss: 420.1308 - val_mse: 420.1308 - val_mae: 18.2593\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 426.4277 - mse: 426.4277 - mae: 18.1508 - val_loss: 421.8863 - val_mse: 421.8863 - val_mae: 18.2949\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 425.2453 - mse: 425.2453 - mae: 18.0927 - val_loss: 420.0043 - val_mse: 420.0043 - val_mae: 18.2644\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 423.8840 - mse: 423.8840 - mae: 18.0660 - val_loss: 417.8414 - val_mse: 417.8414 - val_mae: 18.1837\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 421.9782 - mse: 421.9782 - mae: 17.9897 - val_loss: 418.4558 - val_mse: 418.4558 - val_mae: 18.1944\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 420.7520 - mse: 420.7520 - mae: 17.9312 - val_loss: 414.3819 - val_mse: 414.3819 - val_mae: 18.0263\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 418.5115 - mse: 418.5115 - mae: 17.8362 - val_loss: 409.8342 - val_mse: 409.8342 - val_mae: 17.9509\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 417.2808 - mse: 417.2808 - mae: 17.8207 - val_loss: 417.1695 - val_mse: 417.1695 - val_mae: 18.1169\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 415.4600 - mse: 415.4600 - mae: 17.7153 - val_loss: 406.3406 - val_mse: 406.3406 - val_mae: 17.9712\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 416.1025 - mse: 416.1025 - mae: 17.8137 - val_loss: 418.6373 - val_mse: 418.6373 - val_mae: 18.1309\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 414.3492 - mse: 414.3492 - mae: 17.6506 - val_loss: 413.3375 - val_mse: 413.3375 - val_mae: 18.1259\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 413.2983 - mse: 413.2983 - mae: 17.6962 - val_loss: 406.2578 - val_mse: 406.2578 - val_mae: 17.7174\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 410.7376 - mse: 410.7376 - mae: 17.5719 - val_loss: 404.4945 - val_mse: 404.4945 - val_mae: 17.6952\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 409.6071 - mse: 409.6071 - mae: 17.5250 - val_loss: 400.9485 - val_mse: 400.9485 - val_mae: 17.6978\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 408.7014 - mse: 408.7014 - mae: 17.5728 - val_loss: 404.2151 - val_mse: 404.2151 - val_mae: 17.6248\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.0408 - mse: 406.0408 - mae: 17.4611 - val_loss: 396.4516 - val_mse: 396.4516 - val_mae: 17.4180\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 399.2237 - mse: 399.2237 - mae: 17.2298 - val_loss: 389.1810 - val_mse: 389.1810 - val_mae: 17.2682\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 398.6346 - mse: 398.6346 - mae: 17.2009 - val_loss: 394.6751 - val_mse: 394.6751 - val_mae: 17.2869\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 393.0586 - mse: 393.0586 - mae: 16.9275 - val_loss: 386.4873 - val_mse: 386.4873 - val_mae: 17.1935\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 391.6037 - mse: 391.6037 - mae: 16.9115 - val_loss: 392.0476 - val_mse: 392.0476 - val_mae: 17.1101\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 391.1912 - mse: 391.1912 - mae: 16.8491 - val_loss: 389.0808 - val_mse: 389.0808 - val_mae: 17.1217\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 389.6729 - mse: 389.6729 - mae: 16.7921 - val_loss: 387.9623 - val_mse: 387.9623 - val_mae: 17.0421\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 388.3049 - mse: 388.3049 - mae: 16.6528 - val_loss: 381.6858 - val_mse: 381.6858 - val_mae: 16.8968\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 389.2899 - mse: 389.2899 - mae: 16.8078 - val_loss: 381.0469 - val_mse: 381.0469 - val_mae: 16.7859\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 386.3684 - mse: 386.3684 - mae: 16.6689 - val_loss: 386.8034 - val_mse: 386.8034 - val_mae: 16.9423\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 386.2817 - mse: 386.2817 - mae: 16.6235 - val_loss: 373.2823 - val_mse: 373.2823 - val_mae: 16.6222\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 381.0262 - mse: 381.0262 - mae: 16.4669 - val_loss: 371.8023 - val_mse: 371.8023 - val_mae: 16.4106\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 377.7657 - mse: 377.7657 - mae: 16.2389 - val_loss: 369.3617 - val_mse: 369.3617 - val_mae: 16.4019\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 374.5609 - mse: 374.5609 - mae: 16.1953 - val_loss: 372.3796 - val_mse: 372.3796 - val_mae: 16.3137\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.1648 - mse: 376.1648 - mae: 16.1679 - val_loss: 368.3101 - val_mse: 368.3101 - val_mae: 16.1494\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 375.9570 - mse: 375.9570 - mae: 16.1464 - val_loss: 364.1030 - val_mse: 364.1030 - val_mae: 16.0884\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 374.0596 - mse: 374.0596 - mae: 16.1527 - val_loss: 362.7499 - val_mse: 362.7499 - val_mae: 15.9230\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 374.1046 - mse: 374.1046 - mae: 16.0963 - val_loss: 367.1980 - val_mse: 367.1980 - val_mae: 16.2693\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 374.9071 - mse: 374.9071 - mae: 16.1844 - val_loss: 361.8381 - val_mse: 361.8381 - val_mae: 15.8635\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 373.5518 - mse: 373.5518 - mae: 16.0750 - val_loss: 367.6863 - val_mse: 367.6863 - val_mae: 16.1157\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 373.3143 - mse: 373.3143 - mae: 16.0533 - val_loss: 364.4949 - val_mse: 364.4949 - val_mae: 15.9868\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 373.7514 - mse: 373.7514 - mae: 16.0953 - val_loss: 364.7291 - val_mse: 364.7291 - val_mae: 15.9812\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 371.5393 - mse: 371.5393 - mae: 15.9783 - val_loss: 365.8359 - val_mse: 365.8359 - val_mae: 16.0334\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 371.3748 - mse: 371.3748 - mae: 15.9531 - val_loss: 361.5023 - val_mse: 361.5023 - val_mae: 15.8910\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 372.5594 - mse: 372.5594 - mae: 15.9984 - val_loss: 362.0172 - val_mse: 362.0172 - val_mae: 16.0031\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 372.7180 - mse: 372.7180 - mae: 16.0479 - val_loss: 360.4264 - val_mse: 360.4264 - val_mae: 15.7868\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 371.5890 - mse: 371.5890 - mae: 15.9673 - val_loss: 368.9687 - val_mse: 368.9687 - val_mae: 16.4437\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 372.8486 - mse: 372.8486 - mae: 16.0885 - val_loss: 359.2305 - val_mse: 359.2305 - val_mae: 15.7946\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 371.1939 - mse: 371.1939 - mae: 16.0232 - val_loss: 361.1520 - val_mse: 361.1520 - val_mae: 15.8475\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 370.5375 - mse: 370.5375 - mae: 15.9108 - val_loss: 360.4592 - val_mse: 360.4592 - val_mae: 15.8697\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 371.1237 - mse: 371.1237 - mae: 16.0172 - val_loss: 359.6783 - val_mse: 359.6783 - val_mae: 15.7797\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 369.8794 - mse: 369.8794 - mae: 15.8795 - val_loss: 359.9583 - val_mse: 359.9583 - val_mae: 15.8041\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 369.4198 - mse: 369.4198 - mae: 15.8839 - val_loss: 357.1918 - val_mse: 357.1918 - val_mae: 15.6856\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 369.7104 - mse: 369.7104 - mae: 15.9197 - val_loss: 357.5217 - val_mse: 357.5217 - val_mae: 15.6730\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 368.7158 - mse: 368.7158 - mae: 15.8638 - val_loss: 357.8280 - val_mse: 357.8280 - val_mae: 15.6905\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 23ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 27ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23399.2207 - mse: 23399.2207 - mae: 72.8679 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 23ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23112.9785 - mse: 23112.9785 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5352 - val_mse: 24142.5352 - val_mae: 71.7670\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 23ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 23041.8281 - mse: 23041.8281 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0898 - val_mse: 23629.0898 - val_mae: 73.5344\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 1964.0986 - mse: 1964.0986 - mae: 28.3690 - val_loss: 365.3475 - val_mse: 365.3475 - val_mae: 17.4808\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 478.2564 - mse: 478.2564 - mae: 19.1289 - val_loss: 341.2709 - val_mse: 341.2709 - val_mae: 16.4786\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 461.5773 - mse: 461.5773 - mae: 18.4810 - val_loss: 334.6707 - val_mse: 334.6707 - val_mae: 16.0478\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 457.9267 - mse: 457.9267 - mae: 18.2839 - val_loss: 335.7841 - val_mse: 335.7841 - val_mae: 16.0635\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 456.0497 - mse: 456.0497 - mae: 18.2630 - val_loss: 331.6158 - val_mse: 331.6158 - val_mae: 15.8525\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 455.4641 - mse: 455.4641 - mae: 18.2276 - val_loss: 332.5201 - val_mse: 332.5201 - val_mae: 15.8950\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 454.5847 - mse: 454.5847 - mae: 18.1606 - val_loss: 330.9434 - val_mse: 330.9434 - val_mae: 15.7940\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 455.7329 - mse: 455.7329 - mae: 18.2557 - val_loss: 333.1579 - val_mse: 333.1579 - val_mae: 15.9304\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 453.4375 - mse: 453.4375 - mae: 18.1393 - val_loss: 338.5118 - val_mse: 338.5118 - val_mae: 16.2452\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 453.5831 - mse: 453.5831 - mae: 18.1830 - val_loss: 333.3284 - val_mse: 333.3284 - val_mae: 15.9355\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 453.1157 - mse: 453.1157 - mae: 18.1456 - val_loss: 331.2141 - val_mse: 331.2141 - val_mae: 15.8101\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 454.3202 - mse: 454.3202 - mae: 18.2159 - val_loss: 331.0801 - val_mse: 331.0801 - val_mae: 15.8012\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 452.4957 - mse: 452.4957 - mae: 18.1171 - val_loss: 343.0656 - val_mse: 343.0656 - val_mae: 16.5051\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 457.6678 - mse: 457.6678 - mae: 18.4105 - val_loss: 330.5830 - val_mse: 330.5830 - val_mae: 15.7557\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 451.7039 - mse: 451.7039 - mae: 18.0902 - val_loss: 330.7888 - val_mse: 330.7888 - val_mae: 15.7776\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 453.5576 - mse: 453.5576 - mae: 18.1889 - val_loss: 330.3538 - val_mse: 330.3538 - val_mae: 15.7279\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 453.3079 - mse: 453.3079 - mae: 18.1104 - val_loss: 332.9909 - val_mse: 332.9909 - val_mae: 15.9354\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 453.1537 - mse: 453.1537 - mae: 18.1555 - val_loss: 330.8076 - val_mse: 330.8076 - val_mae: 15.7864\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 451.2082 - mse: 451.2082 - mae: 18.0512 - val_loss: 331.5498 - val_mse: 331.5498 - val_mae: 15.8370\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 452.9588 - mse: 452.9588 - mae: 18.1103 - val_loss: 335.4513 - val_mse: 335.4513 - val_mae: 16.0826\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 451.6189 - mse: 451.6189 - mae: 18.1228 - val_loss: 330.3396 - val_mse: 330.3396 - val_mae: 15.7529\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 453.6641 - mse: 453.6641 - mae: 18.1426 - val_loss: 331.2977 - val_mse: 331.2977 - val_mae: 15.8015\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 451.5460 - mse: 451.5460 - mae: 18.0422 - val_loss: 333.1535 - val_mse: 333.1535 - val_mae: 15.9465\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 451.9437 - mse: 451.9437 - mae: 18.0703 - val_loss: 330.3831 - val_mse: 330.3831 - val_mae: 15.7482\n",
            "Epoch 24: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 803.5110 - mse: 803.5110 - mae: 21.9114 - val_loss: 295.8392 - val_mse: 295.8392 - val_mae: 15.1714\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 373.9917 - mse: 373.9917 - mae: 15.7340 - val_loss: 191.2499 - val_mse: 191.2499 - val_mae: 10.6977\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 314.6246 - mse: 314.6246 - mae: 13.9026 - val_loss: 186.0582 - val_mse: 186.0582 - val_mae: 10.5298\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 297.3889 - mse: 297.3889 - mae: 13.3024 - val_loss: 211.3500 - val_mse: 211.3500 - val_mae: 12.0520\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 302.3692 - mse: 302.3692 - mae: 13.5787 - val_loss: 182.9641 - val_mse: 182.9641 - val_mae: 10.2299\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 311.9193 - mse: 311.9193 - mae: 13.8264 - val_loss: 185.8861 - val_mse: 185.8861 - val_mae: 10.4019\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 308.6758 - mse: 308.6758 - mae: 13.6494 - val_loss: 182.6830 - val_mse: 182.6830 - val_mae: 10.3312\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 331.2849 - mse: 331.2849 - mae: 14.3079 - val_loss: 183.7694 - val_mse: 183.7694 - val_mae: 10.4259\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 289.0391 - mse: 289.0391 - mae: 12.9156 - val_loss: 208.3746 - val_mse: 208.3746 - val_mae: 11.9942\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 290.0652 - mse: 290.0652 - mae: 12.9903 - val_loss: 213.1554 - val_mse: 213.1554 - val_mae: 12.2620\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 293.6415 - mse: 293.6415 - mae: 13.1991 - val_loss: 178.3445 - val_mse: 178.3445 - val_mae: 9.9718\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 289.4246 - mse: 289.4246 - mae: 12.8425 - val_loss: 195.2355 - val_mse: 195.2355 - val_mae: 11.2989\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 288.2818 - mse: 288.2818 - mae: 12.9537 - val_loss: 195.7930 - val_mse: 195.7930 - val_mae: 11.3220\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 292.4938 - mse: 292.4938 - mae: 13.1473 - val_loss: 177.1928 - val_mse: 177.1928 - val_mae: 9.9080\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 284.9965 - mse: 284.9965 - mae: 12.7433 - val_loss: 180.5825 - val_mse: 180.5825 - val_mae: 10.2681\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 285.4223 - mse: 285.4223 - mae: 12.7431 - val_loss: 178.1293 - val_mse: 178.1293 - val_mae: 10.0438\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 288.1713 - mse: 288.1713 - mae: 12.7946 - val_loss: 199.1311 - val_mse: 199.1311 - val_mae: 11.5298\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 291.9210 - mse: 291.9210 - mae: 13.0156 - val_loss: 178.1232 - val_mse: 178.1232 - val_mae: 10.0655\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 283.4474 - mse: 283.4474 - mae: 12.6649 - val_loss: 182.0225 - val_mse: 182.0225 - val_mae: 10.3987\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 285.9321 - mse: 285.9321 - mae: 12.7831 - val_loss: 188.7758 - val_mse: 188.7758 - val_mae: 10.8590\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 281.7324 - mse: 281.7324 - mae: 12.7446 - val_loss: 176.6251 - val_mse: 176.6251 - val_mae: 9.9014\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 284.7893 - mse: 284.7893 - mae: 12.6786 - val_loss: 177.7724 - val_mse: 177.7724 - val_mae: 9.8288\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 278.7857 - mse: 278.7857 - mae: 12.4899 - val_loss: 170.4291 - val_mse: 170.4291 - val_mae: 10.1124\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 263.6428 - mse: 263.6428 - mae: 12.0763 - val_loss: 166.2970 - val_mse: 166.2970 - val_mae: 9.8406\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 261.4459 - mse: 261.4459 - mae: 12.0754 - val_loss: 164.6075 - val_mse: 164.6075 - val_mae: 9.6958\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 267.5110 - mse: 267.5110 - mae: 12.2603 - val_loss: 176.0462 - val_mse: 176.0462 - val_mae: 10.5002\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 263.4286 - mse: 263.4286 - mae: 12.1460 - val_loss: 164.1951 - val_mse: 164.1951 - val_mae: 9.3537\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 271.2164 - mse: 271.2164 - mae: 12.4041 - val_loss: 159.9189 - val_mse: 159.9189 - val_mae: 9.1902\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 265.0211 - mse: 265.0211 - mae: 12.1683 - val_loss: 157.8411 - val_mse: 157.8411 - val_mae: 9.0263\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 261.7188 - mse: 261.7188 - mae: 11.9901 - val_loss: 158.7967 - val_mse: 158.7967 - val_mae: 9.1817\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 263.5137 - mse: 263.5137 - mae: 12.1876 - val_loss: 166.3537 - val_mse: 166.3537 - val_mae: 9.5404\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 271.5914 - mse: 271.5914 - mae: 12.1651 - val_loss: 186.3005 - val_mse: 186.3005 - val_mae: 11.1251\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.5986 - mse: 263.5986 - mae: 12.1437 - val_loss: 165.7630 - val_mse: 165.7630 - val_mae: 9.8223\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 258.6711 - mse: 258.6711 - mae: 11.9510 - val_loss: 167.6943 - val_mse: 167.6943 - val_mae: 9.9628\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 258.6002 - mse: 258.6002 - mae: 11.9495 - val_loss: 162.6257 - val_mse: 162.6257 - val_mae: 9.5750\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 260.4720 - mse: 260.4720 - mae: 11.9681 - val_loss: 164.2977 - val_mse: 164.2977 - val_mae: 9.6877\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 262.8201 - mse: 262.8201 - mae: 12.1239 - val_loss: 156.9626 - val_mse: 156.9626 - val_mae: 8.9552\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.1192 - mse: 264.1192 - mae: 12.0764 - val_loss: 157.4863 - val_mse: 157.4863 - val_mae: 8.9491\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 259.6543 - mse: 259.6543 - mae: 11.9422 - val_loss: 159.0278 - val_mse: 159.0278 - val_mae: 9.2097\n",
            "Epoch 39: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 974.5135 - mse: 974.5135 - mae: 20.7269 - val_loss: 361.4440 - val_mse: 361.4440 - val_mae: 15.7753\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 364.5479 - mse: 364.5479 - mae: 15.6282 - val_loss: 354.6092 - val_mse: 354.6092 - val_mae: 15.4751\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 362.0251 - mse: 362.0251 - mae: 15.5083 - val_loss: 352.8695 - val_mse: 352.8695 - val_mae: 15.4166\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 359.8590 - mse: 359.8590 - mae: 15.3853 - val_loss: 352.1737 - val_mse: 352.1737 - val_mae: 15.4252\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 359.7662 - mse: 359.7662 - mae: 15.4148 - val_loss: 352.4578 - val_mse: 352.4578 - val_mae: 15.3483\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 359.6253 - mse: 359.6253 - mae: 15.3857 - val_loss: 351.3232 - val_mse: 351.3232 - val_mae: 15.3500\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 360.2801 - mse: 360.2801 - mae: 15.4578 - val_loss: 350.7445 - val_mse: 350.7445 - val_mae: 15.3833\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 359.6530 - mse: 359.6530 - mae: 15.4665 - val_loss: 353.1984 - val_mse: 353.1984 - val_mae: 15.3718\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 358.6129 - mse: 358.6129 - mae: 15.3371 - val_loss: 350.1317 - val_mse: 350.1317 - val_mae: 15.3332\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 358.3419 - mse: 358.3419 - mae: 15.3781 - val_loss: 350.5738 - val_mse: 350.5738 - val_mae: 15.3307\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 357.9078 - mse: 357.9078 - mae: 15.3218 - val_loss: 350.0546 - val_mse: 350.0546 - val_mae: 15.3473\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 358.3342 - mse: 358.3342 - mae: 15.3691 - val_loss: 351.1505 - val_mse: 351.1505 - val_mae: 15.3762\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 358.3352 - mse: 358.3352 - mae: 15.3227 - val_loss: 349.6017 - val_mse: 349.6017 - val_mae: 15.3280\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 357.8865 - mse: 357.8865 - mae: 15.2919 - val_loss: 349.5334 - val_mse: 349.5334 - val_mae: 15.3114\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 357.4719 - mse: 357.4719 - mae: 15.2934 - val_loss: 350.4628 - val_mse: 350.4628 - val_mae: 15.3013\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 357.7094 - mse: 357.7094 - mae: 15.2857 - val_loss: 349.4580 - val_mse: 349.4580 - val_mae: 15.3353\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 357.0042 - mse: 357.0042 - mae: 15.3411 - val_loss: 351.2637 - val_mse: 351.2637 - val_mae: 15.3092\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 357.7550 - mse: 357.7550 - mae: 15.3070 - val_loss: 349.4452 - val_mse: 349.4452 - val_mae: 15.2679\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 357.1010 - mse: 357.1010 - mae: 15.3148 - val_loss: 349.5479 - val_mse: 349.5479 - val_mae: 15.2749\n",
            "Epoch 19: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1120, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1717, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 334, in check_consistent_length\n",
            "    % [int(l) for l in lengths]\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
            "\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 1s 18ms/step - loss: 716.9308 - mse: 716.9308 - mae: 19.5155 - val_loss: 303.4658 - val_mse: 303.4658 - val_mae: 14.9663\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 350.0299 - mse: 350.0299 - mae: 15.1069 - val_loss: 270.2865 - val_mse: 270.2865 - val_mae: 13.6216\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 316.2127 - mse: 316.2127 - mae: 14.0677 - val_loss: 241.8800 - val_mse: 241.8800 - val_mae: 12.5393\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 311.5391 - mse: 311.5391 - mae: 13.8835 - val_loss: 236.9286 - val_mse: 236.9286 - val_mae: 12.1854\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 299.4737 - mse: 299.4737 - mae: 13.3899 - val_loss: 227.6804 - val_mse: 227.6804 - val_mae: 11.6782\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 292.2304 - mse: 292.2304 - mae: 13.1662 - val_loss: 226.0731 - val_mse: 226.0731 - val_mae: 11.5710\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 289.4340 - mse: 289.4340 - mae: 13.0236 - val_loss: 224.0806 - val_mse: 224.0806 - val_mae: 11.4869\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 287.7921 - mse: 287.7921 - mae: 12.9672 - val_loss: 225.8209 - val_mse: 225.8209 - val_mae: 11.6637\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 288.3753 - mse: 288.3753 - mae: 13.0091 - val_loss: 224.0400 - val_mse: 224.0400 - val_mae: 11.4325\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 286.0690 - mse: 286.0690 - mae: 12.8430 - val_loss: 223.6792 - val_mse: 223.6792 - val_mae: 11.5287\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 286.3888 - mse: 286.3888 - mae: 12.9469 - val_loss: 221.9840 - val_mse: 221.9840 - val_mae: 11.3894\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 285.6976 - mse: 285.6976 - mae: 12.8427 - val_loss: 223.7188 - val_mse: 223.7188 - val_mae: 11.5503\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 284.6819 - mse: 284.6819 - mae: 12.8127 - val_loss: 220.3505 - val_mse: 220.3505 - val_mae: 11.2747\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 283.8109 - mse: 283.8109 - mae: 12.8090 - val_loss: 220.0237 - val_mse: 220.0237 - val_mae: 11.2363\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 282.9856 - mse: 282.9856 - mae: 12.7788 - val_loss: 219.6820 - val_mse: 219.6820 - val_mae: 11.1984\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 283.1891 - mse: 283.1891 - mae: 12.7692 - val_loss: 222.9465 - val_mse: 222.9465 - val_mae: 11.3526\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 281.8428 - mse: 281.8428 - mae: 12.5852 - val_loss: 259.3971 - val_mse: 259.3971 - val_mae: 13.6892\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 286.4150 - mse: 286.4150 - mae: 12.9691 - val_loss: 220.3150 - val_mse: 220.3150 - val_mae: 11.1737\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 282.7457 - mse: 282.7457 - mae: 12.6503 - val_loss: 221.4961 - val_mse: 221.4961 - val_mae: 11.4077\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 281.2121 - mse: 281.2121 - mae: 12.7147 - val_loss: 218.3104 - val_mse: 218.3104 - val_mae: 11.1092\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 280.5394 - mse: 280.5394 - mae: 12.6203 - val_loss: 238.1922 - val_mse: 238.1922 - val_mae: 12.5828\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 281.9393 - mse: 281.9393 - mae: 12.7247 - val_loss: 224.9047 - val_mse: 224.9047 - val_mae: 11.6673\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 281.1327 - mse: 281.1327 - mae: 12.6739 - val_loss: 218.3256 - val_mse: 218.3256 - val_mae: 11.0632\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 282.3615 - mse: 282.3615 - mae: 12.6607 - val_loss: 220.7841 - val_mse: 220.7841 - val_mae: 11.3566\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 281.9079 - mse: 281.9079 - mae: 12.7316 - val_loss: 219.0440 - val_mse: 219.0440 - val_mae: 11.2240\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 279.5195 - mse: 279.5195 - mae: 12.5739 - val_loss: 218.1332 - val_mse: 218.1332 - val_mae: 11.0700\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 279.2752 - mse: 279.2752 - mae: 12.5117 - val_loss: 220.5408 - val_mse: 220.5408 - val_mae: 11.2034\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 280.2601 - mse: 280.2601 - mae: 12.4856 - val_loss: 218.8519 - val_mse: 218.8519 - val_mae: 11.2181\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 280.1859 - mse: 280.1859 - mae: 12.6231 - val_loss: 218.2511 - val_mse: 218.2511 - val_mae: 11.0932\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 279.4208 - mse: 279.4208 - mae: 12.5291 - val_loss: 219.4926 - val_mse: 219.4926 - val_mae: 11.2835\n",
            "Epoch 30: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3,\n",
              "                   estimator=KerasRegressor(callbacks=[<keras.callbacks.EarlyStopping object at 0x7f92b6878090>], model=<function build_model at 0x7f93278d3680>),\n",
              "                   n_iter=30,\n",
              "                   param_distributions={'model__learning_rate': [1e-06, 1e-05,\n",
              "                                                                 0.0001],\n",
              "                                        'model__momentum': [0.1, 0.5, 0.9],\n",
              "                                        'model__n_hidden': [0, 1, 2, 3],\n",
              "                                        'model__n_neurons': [5, 25, 125],\n",
              "                                        'model__optimizer': ['sgd', 'nesterov',\n",
              "                                                             'momentum',\n",
              "                                                             'adam']},\n",
              "                   verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "rnd_search_cv = RandomizedSearchCV(keras_reg,param_distribs,n_iter=30,cv=3,verbose=1)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cc4d980e-4b5b-42a2-b87b-2121263c7004",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc4d980e-4b5b-42a2-b87b-2121263c7004",
        "outputId": "6bb6dd43-c60d-4ba6-9d9d-56200fe3def9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model__optimizer': 'sgd',\n",
              " 'model__n_neurons': 25,\n",
              " 'model__n_hidden': 3,\n",
              " 'model__momentum': 0.9,\n",
              " 'model__learning_rate': 0.0001}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "rnd_search_cv.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "87532508-d7ae-4bca-b138-f8c2f5eec359",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "87532508-d7ae-4bca-b138-f8c2f5eec359"
      },
      "outputs": [],
      "source": [
        "with open('rnd_search.pkl','wb') as f:\n",
        "    pickle.dump(rnd_search_cv.best_params_,f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dc76774f-4546-4377-86a5-1f3aff6e73ea",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc76774f-4546-4377-86a5-1f3aff6e73ea",
        "outputId": "1024b153-8101-4f17-90a4-cb31a87c4b82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "rnd_search_cv.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d4551c82-34e0-4784-96bd-f16deb4a477e",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "d4551c82-34e0-4784-96bd-f16deb4a477e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}