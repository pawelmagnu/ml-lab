{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc572964-c993-4741-b581-488033f5acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24466e1-9232-46e1-88b2-5237ce7a86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd53ac5-4bb5-4a0e-ba8b-b8faca221a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(patience=10,min_delta=1.0,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a604b986-d9ff-4c18-83d6-14d06ca2385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callback(filename,name,value):\n",
    "    root_logdir = os.path.join(os.curdir, filename)\n",
    "    ts = int(time.time())\n",
    "    filen = str(ts)+'_'+str(name)+'_'+str(value)\n",
    "    return tf.keras.callbacks.TensorBoard(os.path.join(root_logdir, filen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f95b73-3077-4438-bf3b-69cb64073c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden, n_neurons, optimizer, learning_rate, momentum=0):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(shape=13))\n",
    "    for i in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    if optimizer == 'sgd': \n",
    "        opt = keras.optimizers.SGD(lr=learning_rate,nesterov=False)\n",
    "    elif optimizer == 'nesterov':\n",
    "        opt = keras.optimizers.SGD(lr=learning_rate,nesterov=True)\n",
    "    elif optimizer == 'momentum':\n",
    "        opt = keras.optimizers.SGD(lr=learning_rate,nesterov=False,momentum=momentum)\n",
    "    elif optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss='mse',optimizer=opt,metrics=['mse','mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade9841c-290c-455b-b266-f32fbd26ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7821be36-dba4-4066-85fe-bf904e71c1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1e-06, 403.6007385253906, 16.933063507080078),\n",
       " (1e-05, 324.1733093261719, 14.382200241088867),\n",
       " (0.0001, 517.9978637695312, 20.347984313964844)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning = [0.000001,0.00001,0.0001]\n",
    "lr = []\n",
    "for rate in learning:\n",
    "    model = build_model(1,25,'sgd',rate)\n",
    "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','lr',rate),es],verbose=0)\n",
    "    result = (rate, history.history['loss'][-1], history.history['mae'][-1])\n",
    "    lr.append(result)\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6968e5-8578-439d-a4d2-be6e4207b0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: early stopping\n",
      "Epoch 87: early stopping\n",
      "Epoch 75: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 23378.021484375, 72.62944793701172),\n",
       " (1, 304.5304870605469, 13.697992324829102),\n",
       " (2, 283.9238586425781, 12.768856048583984),\n",
       " (3, 325.1810607910156, 14.043087005615234)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = [0,1,2,3]\n",
    "hl = []\n",
    "for hid in hidden:\n",
    "    model = build_model(hid,25,'sgd',1e-05)\n",
    "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','hl',hid),es],verbose=0)\n",
    "    result = (hid, history.history['loss'][-1], history.history['mae'][-1])\n",
    "    hl.append(result)\n",
    "hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edaf6ff1-c500-42b8-99e0-58b4ee977c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 188.4024658203125, 9.955838203430176),\n",
       " (25, 307.8041687011719, 13.89139175415039),\n",
       " (125, 304.9493713378906, 13.756514549255371)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons = [5,25,125]\n",
    "nn = []\n",
    "for neuron in neurons:\n",
    "    model = build_model(1,neuron,'sgd',1e-05)\n",
    "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','nn',neuron),es],verbose=0)\n",
    "    result = (neuron, history.history['loss'][-1], history.history['mae'][-1])\n",
    "    nn.append(result)\n",
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "547a394b-5c4c-4516-9551-fb495169e75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: early stopping\n",
      "Epoch 81: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sgd', 381.6230163574219, 16.060335159301758),\n",
       " ('nesterov', 328.0186767578125, 14.635824203491211),\n",
       " ('momentum', 414.0743713378906, 16.966266632080078),\n",
       " ('adam', 10000.6484375, 67.87552642822266)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opti = ['sgd','nesterov','momentum','adam']\n",
    "opt = []\n",
    "for optimizer in opti:\n",
    "    model = build_model(1,25,optimizer,1e-05,0.5)\n",
    "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','opt',optimizer),es],verbose=0)\n",
    "    result = (optimizer, history.history['loss'][-1], history.history['mae'][-1])\n",
    "    opt.append(result)\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76781afb-5280-49eb-a375-65a98a2b82fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82: early stopping\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.1, 319.1545715332031, 14.17618465423584),\n",
       " (0.5, 315.14849853515625, 13.919328689575195),\n",
       " (0.9, 555.3201904296875, 21.264873504638672)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moment = [0.1,0.5,0.9]\n",
    "mom = []\n",
    "for momentum in moment:\n",
    "    model = build_model(1,25,'momentum',1e-05,momentum)\n",
    "    history = model.fit(X_train,y_train,epochs=100,validation_split=0.1,callbacks=[get_callback('tb_logs','mom',momentum),es],verbose=0)\n",
    "    result = (momentum, history.history['loss'][-1], history.history['mae'][-1])\n",
    "    mom.append(result)\n",
    "mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbad199a-b738-417a-8ebe-32df82f777a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lr.pkl','wb') as f:\n",
    "    pickle.dump(lr,f)\n",
    "with open('hl.pkl','wb') as f:\n",
    "    pickle.dump(hl,f)\n",
    "with open('nn.pkl','wb') as f:\n",
    "    pickle.dump(nn,f)\n",
    "with open('opt.pkl','wb') as f:\n",
    "    pickle.dump(opt,f)\n",
    "with open('mom.pkl','wb') as f:\n",
    "    pickle.dump(mom,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea6dcdd-b76b-40bf-ab6f-8045c648223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "\"model__n_hidden\": [0,1,2,3],\n",
    "\"model__n_neurons\": [5,25,125],\n",
    "\"model__learning_rate\": [0.000001,0.00001,0.0001],\n",
    "\"model__optimizer\": ['sgd','nesterov', 'momentum','adam'],\n",
    "\"model__momentum\": [0.1,0.5,0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86721a4e-529f-4d6f-b160-82ddc10818c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = KerasRegressor(build_model, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9da51-b3e8-420d-b627-9777a1c37548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpkgke2r11/assets\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpext5n_tu/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 1268.4097 - mse: 1268.4097 - mae: 26.3360"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 519.9725 - mse: 519.9725 - mae: 18.1849 - val_loss: 251.1025 - val_mse: 251.1025 - val_mae: 13.0880\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 332.2417 - mse: 332.2417 - mae: 14.4841 - val_loss: 218.4224 - val_mse: 218.4224 - val_mae: 11.7252\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 310.1374 - mse: 310.1374 - mae: 13.7093 - val_loss: 216.3014 - val_mse: 216.3014 - val_mae: 11.6033\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 309.6716 - mse: 309.6716 - mae: 13.6500 - val_loss: 222.7689 - val_mse: 222.7689 - val_mae: 12.0556\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 309.6989 - mse: 309.6989 - mae: 13.6722 - val_loss: 213.3573 - val_mse: 213.3573 - val_mae: 11.3626\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 308.9870 - mse: 308.9870 - mae: 13.6676 - val_loss: 213.9463 - val_mse: 213.9463 - val_mae: 11.3632\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 308.1489 - mse: 308.1489 - mae: 13.5511 - val_loss: 212.7933 - val_mse: 212.7933 - val_mae: 11.2854\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 311.1948 - mse: 311.1948 - mae: 13.7378 - val_loss: 220.4519 - val_mse: 220.4519 - val_mae: 11.8914\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 305.4081 - mse: 305.4081 - mae: 13.4645 - val_loss: 228.3675 - val_mse: 228.3675 - val_mae: 12.4305\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 307.6291 - mse: 307.6291 - mae: 13.6351 - val_loss: 213.3689 - val_mse: 213.3689 - val_mae: 11.3620\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 305.4921 - mse: 305.4921 - mae: 13.5121 - val_loss: 211.8539 - val_mse: 211.8539 - val_mae: 11.2335\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 305.5686 - mse: 305.5686 - mae: 13.4757 - val_loss: 212.1134 - val_mse: 212.1134 - val_mae: 11.2666\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 304.5400 - mse: 304.5400 - mae: 13.4473 - val_loss: 223.8997 - val_mse: 223.8997 - val_mae: 12.1564\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 309.9975 - mse: 309.9975 - mae: 13.8293 - val_loss: 210.8627 - val_mse: 210.8627 - val_mae: 11.1527\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 303.1860 - mse: 303.1860 - mae: 13.3860 - val_loss: 212.7614 - val_mse: 212.7614 - val_mae: 11.3422\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 305.4804 - mse: 305.4804 - mae: 13.5026 - val_loss: 211.2432 - val_mse: 211.2432 - val_mae: 11.1345\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 304.9974 - mse: 304.9974 - mae: 13.3854 - val_loss: 213.6322 - val_mse: 213.6322 - val_mae: 11.4376\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 304.2106 - mse: 304.2106 - mae: 13.4152 - val_loss: 210.2565 - val_mse: 210.2565 - val_mae: 11.1269\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 301.8719 - mse: 301.8719 - mae: 13.2840 - val_loss: 211.9040 - val_mse: 211.9040 - val_mae: 11.3121\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 303.3238 - mse: 303.3238 - mae: 13.3601 - val_loss: 218.6579 - val_mse: 218.6579 - val_mae: 11.9033\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 291.0368 - mse: 291.0368 - mae: 13.1167 - val_loss: 196.9016 - val_mse: 196.9016 - val_mae: 10.8251\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 278.9643 - mse: 278.9643 - mae: 12.6691 - val_loss: 187.8841 - val_mse: 187.8841 - val_mae: 10.5921\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 267.1789 - mse: 267.1789 - mae: 12.3004 - val_loss: 185.2742 - val_mse: 185.2742 - val_mae: 10.5344\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 262.1544 - mse: 262.1544 - mae: 12.1049 - val_loss: 180.1323 - val_mse: 180.1323 - val_mae: 10.1144\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 264.3017 - mse: 264.3017 - mae: 12.2311 - val_loss: 180.6934 - val_mse: 180.6934 - val_mae: 10.1138\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 262.7855 - mse: 262.7855 - mae: 12.1174 - val_loss: 183.7083 - val_mse: 183.7083 - val_mae: 10.4507\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 263.2502 - mse: 263.2502 - mae: 12.1678 - val_loss: 177.5629 - val_mse: 177.5629 - val_mae: 9.8752\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 271.2721 - mse: 271.2721 - mae: 12.5132 - val_loss: 179.5506 - val_mse: 179.5506 - val_mae: 10.0601\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 253.2093 - mse: 253.2093 - mae: 11.8717 - val_loss: 166.1482 - val_mse: 166.1482 - val_mae: 9.7749\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 241.1109 - mse: 241.1109 - mae: 11.4597 - val_loss: 160.1146 - val_mse: 160.1146 - val_mae: 9.2097\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 240.5190 - mse: 240.5190 - mae: 11.4117 - val_loss: 160.6921 - val_mse: 160.6921 - val_mae: 9.1638\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 241.2355 - mse: 241.2355 - mae: 11.3111 - val_loss: 171.8517 - val_mse: 171.8517 - val_mae: 10.2209\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 243.2365 - mse: 243.2365 - mae: 11.5179 - val_loss: 165.9939 - val_mse: 165.9939 - val_mae: 9.7629\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 238.5354 - mse: 238.5354 - mae: 11.3497 - val_loss: 164.1539 - val_mse: 164.1539 - val_mae: 9.6124\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 239.5625 - mse: 239.5625 - mae: 11.4210 - val_loss: 161.1602 - val_mse: 161.1602 - val_mae: 9.3788\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 240.2038 - mse: 240.2038 - mae: 11.4124 - val_loss: 159.7477 - val_mse: 159.7477 - val_mae: 9.2049\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 238.7667 - mse: 238.7667 - mae: 11.3309 - val_loss: 158.8578 - val_mse: 158.8578 - val_mae: 9.0879\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 240.1924 - mse: 240.1924 - mae: 11.3809 - val_loss: 164.3775 - val_mse: 164.3775 - val_mae: 9.4143\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 242.2503 - mse: 242.2503 - mae: 11.4585 - val_loss: 162.7529 - val_mse: 162.7529 - val_mae: 9.5081\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 241.4422 - mse: 241.4422 - mae: 11.5386 - val_loss: 162.1777 - val_mse: 162.1777 - val_mae: 9.4766\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 239.4316 - mse: 239.4316 - mae: 11.3628 - val_loss: 160.3385 - val_mse: 160.3385 - val_mae: 9.2810\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 238.0800 - mse: 238.0800 - mae: 11.2894 - val_loss: 159.7107 - val_mse: 159.7107 - val_mae: 9.2333\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 238.6732 - mse: 238.6732 - mae: 11.2955 - val_loss: 158.5888 - val_mse: 158.5888 - val_mae: 9.1331\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 238.3284 - mse: 238.3284 - mae: 11.2552 - val_loss: 159.2209 - val_mse: 159.2209 - val_mae: 9.2112\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 237.0541 - mse: 237.0541 - mae: 11.2845 - val_loss: 159.9575 - val_mse: 159.9575 - val_mae: 9.2556\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 237.4461 - mse: 237.4461 - mae: 11.2841 - val_loss: 158.0967 - val_mse: 158.0967 - val_mae: 8.9852\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 239.8625 - mse: 239.8625 - mae: 11.3070 - val_loss: 161.0296 - val_mse: 161.0296 - val_mae: 9.2239\n",
      "Epoch 47: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp2m2cafc3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 614.8322 - mse: 614.8322 - mae: 20.0281 - val_loss: 290.9041 - val_mse: 290.9041 - val_mae: 14.3578\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 419.3627 - mse: 419.3627 - mae: 16.6434 - val_loss: 263.2251 - val_mse: 263.2251 - val_mae: 13.2161\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 382.1187 - mse: 382.1187 - mae: 15.5566 - val_loss: 237.2076 - val_mse: 237.2076 - val_mae: 12.3228\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 356.5900 - mse: 356.5900 - mae: 14.8762 - val_loss: 234.3418 - val_mse: 234.3418 - val_mae: 12.3195\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 352.9265 - mse: 352.9265 - mae: 14.7640 - val_loss: 227.8775 - val_mse: 227.8775 - val_mae: 11.7918\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 353.6542 - mse: 353.6542 - mae: 14.7681 - val_loss: 225.8348 - val_mse: 225.8348 - val_mae: 11.6154\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 353.1264 - mse: 353.1264 - mae: 14.6914 - val_loss: 226.2921 - val_mse: 226.2921 - val_mae: 11.6972\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 358.6151 - mse: 358.6151 - mae: 14.8668 - val_loss: 230.4100 - val_mse: 230.4100 - val_mae: 12.0812\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 348.9415 - mse: 348.9415 - mae: 14.5624 - val_loss: 238.7999 - val_mse: 238.7999 - val_mae: 12.6856\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 349.2093 - mse: 349.2093 - mae: 14.6293 - val_loss: 238.2686 - val_mse: 238.2686 - val_mae: 12.6438\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 350.0299 - mse: 350.0299 - mae: 14.6606 - val_loss: 225.0455 - val_mse: 225.0455 - val_mae: 11.5707\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 349.0512 - mse: 349.0512 - mae: 14.5237 - val_loss: 232.5799 - val_mse: 232.5799 - val_mae: 12.2228\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 347.9577 - mse: 347.9577 - mae: 14.5342 - val_loss: 234.5818 - val_mse: 234.5818 - val_mae: 12.3261\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 350.4034 - mse: 350.4034 - mae: 14.6755 - val_loss: 224.1922 - val_mse: 224.1922 - val_mae: 11.4952\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 346.0859 - mse: 346.0859 - mae: 14.4051 - val_loss: 226.3284 - val_mse: 226.3284 - val_mae: 11.7367\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 346.5494 - mse: 346.5494 - mae: 14.4217 - val_loss: 224.7062 - val_mse: 224.7062 - val_mae: 11.5596\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 347.1089 - mse: 347.1089 - mae: 14.4001 - val_loss: 236.2271 - val_mse: 236.2271 - val_mae: 12.5216\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 349.9144 - mse: 349.9144 - mae: 14.5792 - val_loss: 225.0830 - val_mse: 225.0830 - val_mae: 11.6166\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 345.0321 - mse: 345.0321 - mae: 14.3385 - val_loss: 227.9676 - val_mse: 227.9676 - val_mae: 11.8656\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 346.7650 - mse: 346.7650 - mae: 14.4205 - val_loss: 232.0094 - val_mse: 232.0094 - val_mae: 12.1966\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 343.8078 - mse: 343.8078 - mae: 14.3995 - val_loss: 224.5403 - val_mse: 224.5403 - val_mae: 11.5313\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 345.8667 - mse: 345.8667 - mae: 14.3537 - val_loss: 223.7407 - val_mse: 223.7407 - val_mae: 11.4134\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 344.4585 - mse: 344.4585 - mae: 14.2539 - val_loss: 229.3912 - val_mse: 229.3912 - val_mae: 11.9663\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 345.1595 - mse: 345.1595 - mae: 14.3386 - val_loss: 228.0419 - val_mse: 228.0419 - val_mae: 11.8646\n",
      "Epoch 24: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmppce05kpn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 2786.9243 - mse: 2786.9243 - mae: 38.4860"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: 997.9592 - mse: 997.9592 - mae: 22.7010 - val_loss: 357.9437 - val_mse: 357.9437 - val_mae: 15.8580\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 354.3618 - mse: 354.3618 - mae: 15.4402 - val_loss: 342.0794 - val_mse: 342.0794 - val_mae: 15.1093\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 347.7727 - mse: 347.7727 - mae: 15.1018 - val_loss: 338.2002 - val_mse: 338.2002 - val_mae: 14.9565\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 343.4048 - mse: 343.4048 - mae: 14.8717 - val_loss: 335.4448 - val_mse: 335.4448 - val_mae: 14.8439\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 342.2011 - mse: 342.2011 - mae: 14.8348 - val_loss: 333.6629 - val_mse: 333.6629 - val_mae: 14.6962\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 341.1838 - mse: 341.1838 - mae: 14.7626 - val_loss: 332.1848 - val_mse: 332.1848 - val_mae: 14.6620\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 341.8785 - mse: 341.8785 - mae: 14.8507 - val_loss: 332.6750 - val_mse: 332.6750 - val_mae: 14.8071\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 340.5845 - mse: 340.5845 - mae: 14.8227 - val_loss: 331.7489 - val_mse: 331.7489 - val_mae: 14.6079\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 339.1863 - mse: 339.1863 - mae: 14.6842 - val_loss: 329.7743 - val_mse: 329.7743 - val_mae: 14.6037\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 338.5314 - mse: 338.5314 - mae: 14.6994 - val_loss: 328.9604 - val_mse: 328.9604 - val_mae: 14.5720\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 338.2407 - mse: 338.2407 - mae: 14.6686 - val_loss: 328.3846 - val_mse: 328.3846 - val_mae: 14.5563\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 338.5565 - mse: 338.5565 - mae: 14.6847 - val_loss: 330.0542 - val_mse: 330.0542 - val_mae: 14.5750\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 338.3254 - mse: 338.3254 - mae: 14.6354 - val_loss: 328.0603 - val_mse: 328.0603 - val_mae: 14.5219\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 338.0616 - mse: 338.0616 - mae: 14.6379 - val_loss: 327.5164 - val_mse: 327.5164 - val_mae: 14.5479\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 337.2758 - mse: 337.2758 - mae: 14.6384 - val_loss: 327.8018 - val_mse: 327.8018 - val_mae: 14.5027\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 337.6273 - mse: 337.6273 - mae: 14.6152 - val_loss: 327.4316 - val_mse: 327.4316 - val_mae: 14.5606\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 337.1225 - mse: 337.1225 - mae: 14.6729 - val_loss: 328.0923 - val_mse: 328.0923 - val_mae: 14.5118\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 337.4467 - mse: 337.4467 - mae: 14.6277 - val_loss: 326.5197 - val_mse: 326.5197 - val_mae: 14.4973\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 337.0562 - mse: 337.0562 - mae: 14.6323 - val_loss: 327.0499 - val_mse: 327.0499 - val_mae: 14.4733\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.8917 - mse: 336.8917 - mae: 14.5786 - val_loss: 326.2194 - val_mse: 326.2194 - val_mae: 14.4899\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 337.5080 - mse: 337.5080 - mae: 14.6459 - val_loss: 326.5500 - val_mse: 326.5500 - val_mae: 14.4693\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.6266 - mse: 336.6266 - mae: 14.5550 - val_loss: 326.0660 - val_mse: 326.0660 - val_mae: 14.4986\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.2737 - mse: 336.2737 - mae: 14.6162 - val_loss: 326.8260 - val_mse: 326.8260 - val_mae: 14.4531\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.6991 - mse: 336.6991 - mae: 14.5443 - val_loss: 327.6057 - val_mse: 327.6057 - val_mae: 14.6318\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.8452 - mse: 336.8452 - mae: 14.6421 - val_loss: 325.5802 - val_mse: 325.5802 - val_mae: 14.4653\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.0921 - mse: 336.0921 - mae: 14.5553 - val_loss: 326.6002 - val_mse: 326.6002 - val_mae: 14.5363\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.7975 - mse: 336.7975 - mae: 14.6446 - val_loss: 325.7502 - val_mse: 325.7502 - val_mae: 14.4376\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.6282 - mse: 336.6282 - mae: 14.6047 - val_loss: 325.3633 - val_mse: 325.3633 - val_mae: 14.4515\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 335.9231 - mse: 335.9231 - mae: 14.5523 - val_loss: 325.5027 - val_mse: 325.5027 - val_mae: 14.4305\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.0613 - mse: 336.0613 - mae: 14.5543 - val_loss: 325.5297 - val_mse: 325.5297 - val_mae: 14.4458\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 335.6757 - mse: 335.6757 - mae: 14.5486 - val_loss: 325.1234 - val_mse: 325.1234 - val_mae: 14.4293\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.1378 - mse: 336.1378 - mae: 14.5423 - val_loss: 326.4516 - val_mse: 326.4516 - val_mae: 14.5439\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.2388 - mse: 336.2388 - mae: 14.6242 - val_loss: 327.2926 - val_mse: 327.2926 - val_mae: 14.4770\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 336.5385 - mse: 336.5385 - mae: 14.5687 - val_loss: 324.8280 - val_mse: 324.8280 - val_mae: 14.4258\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 335.4904 - mse: 335.4904 - mae: 14.5239 - val_loss: 324.8279 - val_mse: 324.8279 - val_mae: 14.4199\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.2774 - mse: 336.2774 - mae: 14.5547 - val_loss: 325.9397 - val_mse: 325.9397 - val_mae: 14.4072\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.4811 - mse: 336.4811 - mae: 14.6005 - val_loss: 327.0044 - val_mse: 327.0044 - val_mae: 14.4578\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.7480 - mse: 336.7480 - mae: 14.5829 - val_loss: 326.1801 - val_mse: 326.1801 - val_mae: 14.4183\n",
      "Epoch 38: early stopping\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpxp1iyoly/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: 2075.9485 - mse: 2075.9485 - mae: 35.2088 - val_loss: 2121.8203 - val_mse: 2121.8203 - val_mae: 35.2303\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2075.5327 - mse: 2075.5327 - mae: 35.2062 - val_loss: 2121.3730 - val_mse: 2121.3730 - val_mae: 35.2276\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2075.1138 - mse: 2075.1138 - mae: 35.2036 - val_loss: 2120.9282 - val_mse: 2120.9282 - val_mae: 35.2249\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2074.6992 - mse: 2074.6992 - mae: 35.2010 - val_loss: 2120.4807 - val_mse: 2120.4807 - val_mae: 35.2221\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2074.2874 - mse: 2074.2874 - mae: 35.1984 - val_loss: 2120.0349 - val_mse: 2120.0349 - val_mae: 35.2194\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2073.8665 - mse: 2073.8665 - mae: 35.1958 - val_loss: 2119.5935 - val_mse: 2119.5935 - val_mae: 35.2167\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2073.4561 - mse: 2073.4561 - mae: 35.1932 - val_loss: 2119.1470 - val_mse: 2119.1470 - val_mae: 35.2139\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2073.0403 - mse: 2073.0403 - mae: 35.1906 - val_loss: 2118.6992 - val_mse: 2118.6992 - val_mae: 35.2112\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2072.6238 - mse: 2072.6238 - mae: 35.1880 - val_loss: 2118.2549 - val_mse: 2118.2549 - val_mae: 35.2084\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2072.2075 - mse: 2072.2075 - mae: 35.1854 - val_loss: 2117.8142 - val_mse: 2117.8142 - val_mae: 35.2057\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2071.8010 - mse: 2071.8010 - mae: 35.1829 - val_loss: 2117.3660 - val_mse: 2117.3660 - val_mae: 35.2030\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2071.3831 - mse: 2071.3831 - mae: 35.1803 - val_loss: 2116.9250 - val_mse: 2116.9250 - val_mae: 35.2003\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2070.9690 - mse: 2070.9690 - mae: 35.1777 - val_loss: 2116.4841 - val_mse: 2116.4841 - val_mae: 35.1976\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2070.5610 - mse: 2070.5610 - mae: 35.1751 - val_loss: 2116.0400 - val_mse: 2116.0400 - val_mae: 35.1948\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2070.1416 - mse: 2070.1416 - mae: 35.1725 - val_loss: 2115.6023 - val_mse: 2115.6023 - val_mae: 35.1921\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2069.7283 - mse: 2069.7283 - mae: 35.1699 - val_loss: 2115.1621 - val_mse: 2115.1621 - val_mae: 35.1894\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2069.3206 - mse: 2069.3206 - mae: 35.1674 - val_loss: 2114.7122 - val_mse: 2114.7122 - val_mae: 35.1867\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2068.9062 - mse: 2068.9062 - mae: 35.1648 - val_loss: 2114.2666 - val_mse: 2114.2666 - val_mae: 35.1840\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2068.4905 - mse: 2068.4905 - mae: 35.1622 - val_loss: 2113.8262 - val_mse: 2113.8262 - val_mae: 35.1812\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2068.0781 - mse: 2068.0781 - mae: 35.1596 - val_loss: 2113.3855 - val_mse: 2113.3855 - val_mae: 35.1785\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2067.6655 - mse: 2067.6655 - mae: 35.1571 - val_loss: 2112.9456 - val_mse: 2112.9456 - val_mae: 35.1758\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2067.2551 - mse: 2067.2551 - mae: 35.1545 - val_loss: 2112.5066 - val_mse: 2112.5066 - val_mae: 35.1731\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2066.8477 - mse: 2066.8477 - mae: 35.1519 - val_loss: 2112.0605 - val_mse: 2112.0605 - val_mae: 35.1704\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2066.4333 - mse: 2066.4333 - mae: 35.1494 - val_loss: 2111.6165 - val_mse: 2111.6165 - val_mae: 35.1677\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2066.0183 - mse: 2066.0183 - mae: 35.1468 - val_loss: 2111.1802 - val_mse: 2111.1802 - val_mae: 35.1650\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2065.6089 - mse: 2065.6089 - mae: 35.1442 - val_loss: 2110.7410 - val_mse: 2110.7410 - val_mae: 35.1623\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2065.1997 - mse: 2065.1997 - mae: 35.1416 - val_loss: 2110.2966 - val_mse: 2110.2966 - val_mae: 35.1595\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2064.7864 - mse: 2064.7864 - mae: 35.1390 - val_loss: 2109.8574 - val_mse: 2109.8574 - val_mae: 35.1568\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2064.3801 - mse: 2064.3801 - mae: 35.1365 - val_loss: 2109.4165 - val_mse: 2109.4165 - val_mae: 35.1541\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2063.9590 - mse: 2063.9590 - mae: 35.1339 - val_loss: 2108.9822 - val_mse: 2108.9822 - val_mae: 35.1514\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2063.5544 - mse: 2063.5544 - mae: 35.1313 - val_loss: 2108.5383 - val_mse: 2108.5383 - val_mae: 35.1487\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2063.1421 - mse: 2063.1421 - mae: 35.1287 - val_loss: 2108.0955 - val_mse: 2108.0955 - val_mae: 35.1460\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2062.7334 - mse: 2062.7334 - mae: 35.1262 - val_loss: 2107.6538 - val_mse: 2107.6538 - val_mae: 35.1433\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2062.3164 - mse: 2062.3164 - mae: 35.1236 - val_loss: 2107.2168 - val_mse: 2107.2168 - val_mae: 35.1406\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2061.9102 - mse: 2061.9102 - mae: 35.1210 - val_loss: 2106.7769 - val_mse: 2106.7769 - val_mae: 35.1378\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2061.4944 - mse: 2061.4944 - mae: 35.1184 - val_loss: 2106.3440 - val_mse: 2106.3440 - val_mae: 35.1352\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2061.0913 - mse: 2061.0913 - mae: 35.1159 - val_loss: 2105.8979 - val_mse: 2105.8979 - val_mae: 35.1324\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2060.6741 - mse: 2060.6741 - mae: 35.1133 - val_loss: 2105.4534 - val_mse: 2105.4534 - val_mae: 35.1297\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2060.2654 - mse: 2060.2654 - mae: 35.1107 - val_loss: 2105.0095 - val_mse: 2105.0095 - val_mae: 35.1269\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2059.8525 - mse: 2059.8525 - mae: 35.1081 - val_loss: 2104.5688 - val_mse: 2104.5688 - val_mae: 35.1242\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2059.4380 - mse: 2059.4380 - mae: 35.1055 - val_loss: 2104.1313 - val_mse: 2104.1313 - val_mae: 35.1215\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2059.0278 - mse: 2059.0278 - mae: 35.1029 - val_loss: 2103.6941 - val_mse: 2103.6941 - val_mae: 35.1188\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2058.6160 - mse: 2058.6160 - mae: 35.1004 - val_loss: 2103.2571 - val_mse: 2103.2571 - val_mae: 35.1161\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2058.2134 - mse: 2058.2134 - mae: 35.0978 - val_loss: 2102.8093 - val_mse: 2102.8093 - val_mae: 35.1134\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2057.7947 - mse: 2057.7947 - mae: 35.0952 - val_loss: 2102.3706 - val_mse: 2102.3706 - val_mae: 35.1107\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2057.3887 - mse: 2057.3887 - mae: 35.0926 - val_loss: 2101.9316 - val_mse: 2101.9316 - val_mae: 35.1079\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2056.9802 - mse: 2056.9802 - mae: 35.0901 - val_loss: 2101.4927 - val_mse: 2101.4927 - val_mae: 35.1052\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2056.5671 - mse: 2056.5671 - mae: 35.0875 - val_loss: 2101.0535 - val_mse: 2101.0535 - val_mae: 35.1025\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2056.1614 - mse: 2056.1614 - mae: 35.0849 - val_loss: 2100.6147 - val_mse: 2100.6147 - val_mae: 35.0998\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2055.7537 - mse: 2055.7537 - mae: 35.0823 - val_loss: 2100.1748 - val_mse: 2100.1748 - val_mae: 35.0971\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2055.3438 - mse: 2055.3438 - mae: 35.0798 - val_loss: 2099.7371 - val_mse: 2099.7371 - val_mae: 35.0944\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2054.9353 - mse: 2054.9353 - mae: 35.0772 - val_loss: 2099.2986 - val_mse: 2099.2986 - val_mae: 35.0917\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2054.5269 - mse: 2054.5269 - mae: 35.0746 - val_loss: 2098.8616 - val_mse: 2098.8616 - val_mae: 35.0890\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2054.1138 - mse: 2054.1138 - mae: 35.0721 - val_loss: 2098.4263 - val_mse: 2098.4263 - val_mae: 35.0863\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2053.7092 - mse: 2053.7092 - mae: 35.0695 - val_loss: 2097.9829 - val_mse: 2097.9829 - val_mae: 35.0836\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2053.2937 - mse: 2053.2937 - mae: 35.0669 - val_loss: 2097.5425 - val_mse: 2097.5425 - val_mae: 35.0809\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2052.8870 - mse: 2052.8870 - mae: 35.0644 - val_loss: 2097.1018 - val_mse: 2097.1018 - val_mae: 35.0781\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2052.4788 - mse: 2052.4788 - mae: 35.0618 - val_loss: 2096.6541 - val_mse: 2096.6541 - val_mae: 35.0754\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2052.0588 - mse: 2052.0588 - mae: 35.0592 - val_loss: 2096.2212 - val_mse: 2096.2212 - val_mae: 35.0727\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2051.6580 - mse: 2051.6580 - mae: 35.0567 - val_loss: 2095.7805 - val_mse: 2095.7805 - val_mae: 35.0700\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2051.2434 - mse: 2051.2434 - mae: 35.0541 - val_loss: 2095.3425 - val_mse: 2095.3425 - val_mae: 35.0673\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2050.8318 - mse: 2050.8318 - mae: 35.0515 - val_loss: 2094.9089 - val_mse: 2094.9089 - val_mae: 35.0646\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2050.4253 - mse: 2050.4253 - mae: 35.0489 - val_loss: 2094.4707 - val_mse: 2094.4707 - val_mae: 35.0619\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2050.0159 - mse: 2050.0159 - mae: 35.0464 - val_loss: 2094.0308 - val_mse: 2094.0308 - val_mae: 35.0592\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2049.6067 - mse: 2049.6067 - mae: 35.0438 - val_loss: 2093.5940 - val_mse: 2093.5940 - val_mae: 35.0564\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2049.1985 - mse: 2049.1985 - mae: 35.0412 - val_loss: 2093.1509 - val_mse: 2093.1509 - val_mae: 35.0537\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2048.7881 - mse: 2048.7881 - mae: 35.0386 - val_loss: 2092.7095 - val_mse: 2092.7095 - val_mae: 35.0510\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2048.3765 - mse: 2048.3765 - mae: 35.0360 - val_loss: 2092.2688 - val_mse: 2092.2688 - val_mae: 35.0483\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2047.9679 - mse: 2047.9679 - mae: 35.0335 - val_loss: 2091.8376 - val_mse: 2091.8376 - val_mae: 35.0456\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2047.5597 - mse: 2047.5597 - mae: 35.0309 - val_loss: 2091.4006 - val_mse: 2091.4006 - val_mae: 35.0429\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2047.1493 - mse: 2047.1493 - mae: 35.0283 - val_loss: 2090.9675 - val_mse: 2090.9675 - val_mae: 35.0402\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2046.7468 - mse: 2046.7468 - mae: 35.0258 - val_loss: 2090.5227 - val_mse: 2090.5227 - val_mae: 35.0374\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2046.3333 - mse: 2046.3333 - mae: 35.0232 - val_loss: 2090.0903 - val_mse: 2090.0903 - val_mae: 35.0348\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2045.9272 - mse: 2045.9272 - mae: 35.0206 - val_loss: 2089.6509 - val_mse: 2089.6509 - val_mae: 35.0320\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2045.5220 - mse: 2045.5220 - mae: 35.0181 - val_loss: 2089.2146 - val_mse: 2089.2146 - val_mae: 35.0293\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2045.1112 - mse: 2045.1112 - mae: 35.0155 - val_loss: 2088.7791 - val_mse: 2088.7791 - val_mae: 35.0266\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2044.7025 - mse: 2044.7025 - mae: 35.0130 - val_loss: 2088.3389 - val_mse: 2088.3389 - val_mae: 35.0239\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2044.2922 - mse: 2044.2922 - mae: 35.0104 - val_loss: 2087.9067 - val_mse: 2087.9067 - val_mae: 35.0213\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2043.8862 - mse: 2043.8862 - mae: 35.0078 - val_loss: 2087.4705 - val_mse: 2087.4705 - val_mae: 35.0186\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2043.4797 - mse: 2043.4797 - mae: 35.0053 - val_loss: 2087.0334 - val_mse: 2087.0334 - val_mae: 35.0158\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2043.0698 - mse: 2043.0698 - mae: 35.0027 - val_loss: 2086.5994 - val_mse: 2086.5994 - val_mae: 35.0132\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2042.6635 - mse: 2042.6635 - mae: 35.0002 - val_loss: 2086.1565 - val_mse: 2086.1565 - val_mae: 35.0104\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2042.2550 - mse: 2042.2550 - mae: 34.9976 - val_loss: 2085.7197 - val_mse: 2085.7197 - val_mae: 35.0077\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2041.8480 - mse: 2041.8480 - mae: 34.9950 - val_loss: 2085.2839 - val_mse: 2085.2839 - val_mae: 35.0050\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2041.4441 - mse: 2041.4441 - mae: 34.9924 - val_loss: 2084.8474 - val_mse: 2084.8474 - val_mae: 35.0023\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2041.0345 - mse: 2041.0345 - mae: 34.9899 - val_loss: 2084.4153 - val_mse: 2084.4153 - val_mae: 34.9996\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2040.6273 - mse: 2040.6273 - mae: 34.9873 - val_loss: 2083.9846 - val_mse: 2083.9846 - val_mae: 34.9969\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2040.2292 - mse: 2040.2292 - mae: 34.9848 - val_loss: 2083.5444 - val_mse: 2083.5444 - val_mae: 34.9942\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2039.8231 - mse: 2039.8231 - mae: 34.9822 - val_loss: 2083.1052 - val_mse: 2083.1052 - val_mae: 34.9915\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2039.4100 - mse: 2039.4100 - mae: 34.9796 - val_loss: 2082.6746 - val_mse: 2082.6746 - val_mae: 34.9888\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2039.0103 - mse: 2039.0103 - mae: 34.9771 - val_loss: 2082.2380 - val_mse: 2082.2380 - val_mae: 34.9861\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2038.5986 - mse: 2038.5986 - mae: 34.9745 - val_loss: 2081.8044 - val_mse: 2081.8044 - val_mae: 34.9834\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2038.1945 - mse: 2038.1945 - mae: 34.9720 - val_loss: 2081.3674 - val_mse: 2081.3674 - val_mae: 34.9807\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2037.7891 - mse: 2037.7891 - mae: 34.9694 - val_loss: 2080.9319 - val_mse: 2080.9319 - val_mae: 34.9780\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2037.3818 - mse: 2037.3818 - mae: 34.9668 - val_loss: 2080.5010 - val_mse: 2080.5010 - val_mae: 34.9753\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2036.9760 - mse: 2036.9760 - mae: 34.9643 - val_loss: 2080.0669 - val_mse: 2080.0669 - val_mae: 34.9726\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2036.5730 - mse: 2036.5730 - mae: 34.9617 - val_loss: 2079.6270 - val_mse: 2079.6270 - val_mae: 34.9699\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2036.1609 - mse: 2036.1609 - mae: 34.9591 - val_loss: 2079.1978 - val_mse: 2079.1978 - val_mae: 34.9673\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2035.7572 - mse: 2035.7572 - mae: 34.9566 - val_loss: 2078.7615 - val_mse: 2078.7615 - val_mae: 34.9646\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2035.3538 - mse: 2035.3538 - mae: 34.9540 - val_loss: 2078.3262 - val_mse: 2078.3262 - val_mae: 34.9619\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp80_o_7dk/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 1867.9078 - mse: 1867.9078 - mae: 32.4916 - val_loss: 1861.3372 - val_mse: 1861.3372 - val_mae: 30.8520\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1867.4370 - mse: 1867.4370 - mae: 32.4874 - val_loss: 1860.8792 - val_mse: 1860.8792 - val_mae: 30.8482\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1866.9598 - mse: 1866.9598 - mae: 32.4831 - val_loss: 1860.4225 - val_mse: 1860.4225 - val_mae: 30.8445\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1866.4956 - mse: 1866.4956 - mae: 32.4789 - val_loss: 1859.9617 - val_mse: 1859.9617 - val_mae: 30.8406\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1866.0353 - mse: 1866.0353 - mae: 32.4747 - val_loss: 1859.4993 - val_mse: 1859.4993 - val_mae: 30.8368\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1865.5587 - mse: 1865.5587 - mae: 32.4704 - val_loss: 1859.0442 - val_mse: 1859.0442 - val_mae: 30.8331\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1865.0989 - mse: 1865.0989 - mae: 32.4662 - val_loss: 1858.5859 - val_mse: 1858.5859 - val_mae: 30.8293\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1864.6265 - mse: 1864.6265 - mae: 32.4620 - val_loss: 1858.1423 - val_mse: 1858.1423 - val_mae: 30.8256\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1864.1746 - mse: 1864.1746 - mae: 32.4580 - val_loss: 1857.6913 - val_mse: 1857.6913 - val_mae: 30.8219\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1863.7102 - mse: 1863.7102 - mae: 32.4538 - val_loss: 1857.2428 - val_mse: 1857.2428 - val_mae: 30.8181\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1863.2548 - mse: 1863.2548 - mae: 32.4497 - val_loss: 1856.7913 - val_mse: 1856.7913 - val_mae: 30.8144\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1862.7928 - mse: 1862.7928 - mae: 32.4456 - val_loss: 1856.3353 - val_mse: 1856.3353 - val_mae: 30.8106\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1862.3268 - mse: 1862.3268 - mae: 32.4413 - val_loss: 1855.8843 - val_mse: 1855.8843 - val_mae: 30.8069\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1861.8481 - mse: 1861.8481 - mae: 32.4370 - val_loss: 1855.4430 - val_mse: 1855.4430 - val_mae: 30.8032\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1861.3948 - mse: 1861.3948 - mae: 32.4330 - val_loss: 1854.9919 - val_mse: 1854.9919 - val_mae: 30.7995\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1860.9358 - mse: 1860.9358 - mae: 32.4289 - val_loss: 1854.5295 - val_mse: 1854.5295 - val_mae: 30.7957\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1860.4659 - mse: 1860.4659 - mae: 32.4246 - val_loss: 1854.0718 - val_mse: 1854.0718 - val_mae: 30.7919\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1859.9977 - mse: 1859.9977 - mae: 32.4204 - val_loss: 1853.6195 - val_mse: 1853.6195 - val_mae: 30.7881\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1859.5332 - mse: 1859.5332 - mae: 32.4162 - val_loss: 1853.1647 - val_mse: 1853.1647 - val_mae: 30.7843\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1859.0756 - mse: 1859.0756 - mae: 32.4121 - val_loss: 1852.7080 - val_mse: 1852.7080 - val_mae: 30.7805\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1858.6031 - mse: 1858.6031 - mae: 32.4078 - val_loss: 1852.2649 - val_mse: 1852.2649 - val_mae: 30.7769\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1858.1445 - mse: 1858.1445 - mae: 32.4037 - val_loss: 1851.8192 - val_mse: 1851.8192 - val_mae: 30.7732\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1857.6901 - mse: 1857.6901 - mae: 32.3996 - val_loss: 1851.3673 - val_mse: 1851.3673 - val_mae: 30.7694\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1857.2213 - mse: 1857.2213 - mae: 32.3954 - val_loss: 1850.9181 - val_mse: 1850.9181 - val_mae: 30.7657\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1856.7604 - mse: 1856.7604 - mae: 32.3913 - val_loss: 1850.4625 - val_mse: 1850.4625 - val_mae: 30.7619\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1856.2947 - mse: 1856.2947 - mae: 32.3870 - val_loss: 1850.0110 - val_mse: 1850.0110 - val_mae: 30.7582\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1855.8424 - mse: 1855.8424 - mae: 32.3830 - val_loss: 1849.5525 - val_mse: 1849.5525 - val_mae: 30.7544\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1855.3617 - mse: 1855.3617 - mae: 32.3786 - val_loss: 1849.1094 - val_mse: 1849.1094 - val_mae: 30.7507\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1854.9084 - mse: 1854.9084 - mae: 32.3745 - val_loss: 1848.6580 - val_mse: 1848.6580 - val_mae: 30.7469\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1854.4434 - mse: 1854.4434 - mae: 32.3704 - val_loss: 1848.2092 - val_mse: 1848.2092 - val_mae: 30.7432\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1853.9828 - mse: 1853.9828 - mae: 32.3662 - val_loss: 1847.7529 - val_mse: 1847.7529 - val_mae: 30.7394\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1853.5188 - mse: 1853.5188 - mae: 32.3620 - val_loss: 1847.2963 - val_mse: 1847.2963 - val_mae: 30.7356\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1853.0516 - mse: 1853.0516 - mae: 32.3578 - val_loss: 1846.8412 - val_mse: 1846.8412 - val_mae: 30.7318\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1852.5980 - mse: 1852.5980 - mae: 32.3536 - val_loss: 1846.3806 - val_mse: 1846.3806 - val_mae: 30.7280\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1852.1118 - mse: 1852.1118 - mae: 32.3493 - val_loss: 1845.9406 - val_mse: 1845.9406 - val_mae: 30.7243\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1851.6566 - mse: 1851.6566 - mae: 32.3453 - val_loss: 1845.4896 - val_mse: 1845.4896 - val_mae: 30.7206\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1851.1960 - mse: 1851.1960 - mae: 32.3411 - val_loss: 1845.0367 - val_mse: 1845.0367 - val_mae: 30.7168\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1850.7307 - mse: 1850.7307 - mae: 32.3369 - val_loss: 1844.5828 - val_mse: 1844.5828 - val_mae: 30.7130\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1850.2594 - mse: 1850.2594 - mae: 32.3326 - val_loss: 1844.1357 - val_mse: 1844.1357 - val_mae: 30.7093\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1849.8116 - mse: 1849.8116 - mae: 32.3285 - val_loss: 1843.6780 - val_mse: 1843.6780 - val_mae: 30.7055\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1849.3473 - mse: 1849.3473 - mae: 32.3244 - val_loss: 1843.2231 - val_mse: 1843.2231 - val_mae: 30.7017\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1848.8756 - mse: 1848.8756 - mae: 32.3201 - val_loss: 1842.7751 - val_mse: 1842.7751 - val_mae: 30.6980\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1848.4210 - mse: 1848.4210 - mae: 32.3160 - val_loss: 1842.3195 - val_mse: 1842.3195 - val_mae: 30.6942\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1847.9482 - mse: 1847.9482 - mae: 32.3116 - val_loss: 1841.8716 - val_mse: 1841.8716 - val_mae: 30.6905\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1847.4941 - mse: 1847.4941 - mae: 32.3076 - val_loss: 1841.4192 - val_mse: 1841.4192 - val_mae: 30.6867\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1847.0281 - mse: 1847.0281 - mae: 32.3034 - val_loss: 1840.9680 - val_mse: 1840.9680 - val_mae: 30.6829\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1846.5764 - mse: 1846.5764 - mae: 32.2993 - val_loss: 1840.5104 - val_mse: 1840.5104 - val_mae: 30.6791\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1846.0985 - mse: 1846.0985 - mae: 32.2950 - val_loss: 1840.0663 - val_mse: 1840.0663 - val_mae: 30.6754\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1845.6425 - mse: 1845.6425 - mae: 32.2909 - val_loss: 1839.6208 - val_mse: 1839.6208 - val_mae: 30.6717\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1845.1847 - mse: 1845.1847 - mae: 32.2868 - val_loss: 1839.1780 - val_mse: 1839.1780 - val_mae: 30.6680\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1844.7330 - mse: 1844.7330 - mae: 32.2827 - val_loss: 1838.7289 - val_mse: 1838.7289 - val_mae: 30.6643\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1844.2765 - mse: 1844.2765 - mae: 32.2785 - val_loss: 1838.2812 - val_mse: 1838.2812 - val_mae: 30.6605\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1843.8259 - mse: 1843.8259 - mae: 32.2744 - val_loss: 1837.8348 - val_mse: 1837.8348 - val_mae: 30.6568\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1843.3678 - mse: 1843.3678 - mae: 32.2703 - val_loss: 1837.3892 - val_mse: 1837.3892 - val_mae: 30.6531\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1842.9094 - mse: 1842.9094 - mae: 32.2661 - val_loss: 1836.9520 - val_mse: 1836.9520 - val_mae: 30.6494\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1842.4552 - mse: 1842.4552 - mae: 32.2620 - val_loss: 1836.5114 - val_mse: 1836.5114 - val_mae: 30.6458\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1842.0012 - mse: 1842.0012 - mae: 32.2579 - val_loss: 1836.0637 - val_mse: 1836.0637 - val_mae: 30.6420\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1841.5428 - mse: 1841.5428 - mae: 32.2537 - val_loss: 1835.6161 - val_mse: 1835.6161 - val_mae: 30.6383\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1841.0887 - mse: 1841.0887 - mae: 32.2496 - val_loss: 1835.1704 - val_mse: 1835.1704 - val_mae: 30.6346\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1840.6320 - mse: 1840.6320 - mae: 32.2455 - val_loss: 1834.7231 - val_mse: 1834.7231 - val_mae: 30.6308\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1840.1720 - mse: 1840.1720 - mae: 32.2413 - val_loss: 1834.2742 - val_mse: 1834.2742 - val_mae: 30.6271\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1839.7113 - mse: 1839.7113 - mae: 32.2371 - val_loss: 1833.8252 - val_mse: 1833.8252 - val_mae: 30.6233\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1839.2506 - mse: 1839.2506 - mae: 32.2330 - val_loss: 1833.3799 - val_mse: 1833.3799 - val_mae: 30.6196\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1838.7975 - mse: 1838.7975 - mae: 32.2289 - val_loss: 1832.9293 - val_mse: 1832.9293 - val_mae: 30.6158\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1838.3402 - mse: 1838.3402 - mae: 32.2247 - val_loss: 1832.4811 - val_mse: 1832.4811 - val_mae: 30.6121\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1837.8717 - mse: 1837.8717 - mae: 32.2205 - val_loss: 1832.0411 - val_mse: 1832.0410 - val_mae: 30.6084\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1837.4303 - mse: 1837.4303 - mae: 32.2164 - val_loss: 1831.5891 - val_mse: 1831.5891 - val_mae: 30.6047\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1836.9592 - mse: 1836.9592 - mae: 32.2122 - val_loss: 1831.1501 - val_mse: 1831.1501 - val_mae: 30.6010\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1836.5083 - mse: 1836.5083 - mae: 32.2081 - val_loss: 1830.7053 - val_mse: 1830.7053 - val_mae: 30.5973\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1836.0564 - mse: 1836.0564 - mae: 32.2040 - val_loss: 1830.2551 - val_mse: 1830.2551 - val_mae: 30.5935\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1835.5961 - mse: 1835.5961 - mae: 32.1997 - val_loss: 1829.8047 - val_mse: 1829.8047 - val_mae: 30.5897\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1835.1318 - mse: 1835.1318 - mae: 32.1956 - val_loss: 1829.3625 - val_mse: 1829.3625 - val_mae: 30.5860\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1834.6774 - mse: 1834.6774 - mae: 32.1915 - val_loss: 1828.9132 - val_mse: 1828.9132 - val_mae: 30.5823\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1834.2277 - mse: 1834.2277 - mae: 32.1873 - val_loss: 1828.4587 - val_mse: 1828.4587 - val_mae: 30.5785\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1833.7559 - mse: 1833.7559 - mae: 32.1831 - val_loss: 1828.0220 - val_mse: 1828.0220 - val_mae: 30.5748\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1833.3112 - mse: 1833.3112 - mae: 32.1791 - val_loss: 1827.5726 - val_mse: 1827.5726 - val_mae: 30.5710\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1832.8425 - mse: 1832.8425 - mae: 32.1748 - val_loss: 1827.1305 - val_mse: 1827.1305 - val_mae: 30.5673\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1832.3937 - mse: 1832.3937 - mae: 32.1707 - val_loss: 1826.6838 - val_mse: 1826.6838 - val_mae: 30.5636\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1831.9331 - mse: 1831.9331 - mae: 32.1665 - val_loss: 1826.2428 - val_mse: 1826.2428 - val_mae: 30.5599\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1831.4858 - mse: 1831.4858 - mae: 32.1624 - val_loss: 1825.7935 - val_mse: 1825.7935 - val_mae: 30.5561\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1831.0168 - mse: 1831.0168 - mae: 32.1582 - val_loss: 1825.3545 - val_mse: 1825.3545 - val_mae: 30.5525\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1830.5707 - mse: 1830.5707 - mae: 32.1541 - val_loss: 1824.9092 - val_mse: 1824.9092 - val_mae: 30.5487\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1830.1166 - mse: 1830.1166 - mae: 32.1500 - val_loss: 1824.4664 - val_mse: 1824.4664 - val_mae: 30.5450\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1829.6710 - mse: 1829.6710 - mae: 32.1459 - val_loss: 1824.0157 - val_mse: 1824.0157 - val_mae: 30.5412\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1829.2078 - mse: 1829.2078 - mae: 32.1417 - val_loss: 1823.5701 - val_mse: 1823.5701 - val_mae: 30.5375\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1828.7417 - mse: 1828.7417 - mae: 32.1375 - val_loss: 1823.1285 - val_mse: 1823.1285 - val_mae: 30.5338\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1828.2917 - mse: 1828.2917 - mae: 32.1334 - val_loss: 1822.6851 - val_mse: 1822.6851 - val_mae: 30.5301\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1827.8439 - mse: 1827.8439 - mae: 32.1293 - val_loss: 1822.2382 - val_mse: 1822.2382 - val_mae: 30.5263\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1827.3788 - mse: 1827.3788 - mae: 32.1250 - val_loss: 1821.8000 - val_mse: 1821.8000 - val_mae: 30.5226\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1826.9254 - mse: 1826.9254 - mae: 32.1209 - val_loss: 1821.3530 - val_mse: 1821.3530 - val_mae: 30.5189\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1826.4623 - mse: 1826.4623 - mae: 32.1167 - val_loss: 1820.9138 - val_mse: 1820.9138 - val_mae: 30.5152\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1826.0286 - mse: 1826.0286 - mae: 32.1127 - val_loss: 1820.4586 - val_mse: 1820.4586 - val_mae: 30.5114\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1825.5563 - mse: 1825.5563 - mae: 32.1084 - val_loss: 1820.0142 - val_mse: 1820.0142 - val_mae: 30.5076\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1825.1085 - mse: 1825.1085 - mae: 32.1043 - val_loss: 1819.5665 - val_mse: 1819.5665 - val_mae: 30.5039\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1824.6521 - mse: 1824.6521 - mae: 32.1002 - val_loss: 1819.1221 - val_mse: 1819.1221 - val_mae: 30.5001\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1824.1896 - mse: 1824.1895 - mae: 32.0960 - val_loss: 1818.6797 - val_mse: 1818.6797 - val_mae: 30.4964\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1823.7450 - mse: 1823.7450 - mae: 32.0918 - val_loss: 1818.2295 - val_mse: 1818.2295 - val_mae: 30.4926\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1823.2745 - mse: 1823.2745 - mae: 32.0876 - val_loss: 1817.7944 - val_mse: 1817.7944 - val_mae: 30.4890\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1822.8351 - mse: 1822.8351 - mae: 32.0836 - val_loss: 1817.3497 - val_mse: 1817.3497 - val_mae: 30.4852\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1822.3756 - mse: 1822.3756 - mae: 32.0794 - val_loss: 1816.9060 - val_mse: 1816.9060 - val_mae: 30.4815\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpb8y1yrt5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 36ms/step - loss: 12770.5322 - mse: 12770.5322 - mae: 80.6448 - val_loss: 13669.3701 - val_mse: 13669.3701 - val_mae: 82.8980\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12769.1377 - mse: 12769.1377 - mae: 80.6409 - val_loss: 13667.8936 - val_mse: 13667.8936 - val_mae: 82.8940\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12767.7842 - mse: 12767.7842 - mae: 80.6371 - val_loss: 13666.4004 - val_mse: 13666.4004 - val_mae: 82.8900\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12766.3994 - mse: 12766.3994 - mae: 80.6332 - val_loss: 13664.9229 - val_mse: 13664.9229 - val_mae: 82.8860\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12765.0312 - mse: 12765.0312 - mae: 80.6293 - val_loss: 13663.4365 - val_mse: 13663.4365 - val_mae: 82.8820\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12763.6416 - mse: 12763.6416 - mae: 80.6255 - val_loss: 13661.9619 - val_mse: 13661.9619 - val_mae: 82.8780\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12762.2715 - mse: 12762.2715 - mae: 80.6216 - val_loss: 13660.4883 - val_mse: 13660.4883 - val_mae: 82.8740\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12760.9043 - mse: 12760.9043 - mae: 80.6178 - val_loss: 13659.0098 - val_mse: 13659.0098 - val_mae: 82.8700\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12759.5264 - mse: 12759.5264 - mae: 80.6139 - val_loss: 13657.5312 - val_mse: 13657.5312 - val_mae: 82.8660\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12758.1602 - mse: 12758.1602 - mae: 80.6101 - val_loss: 13656.0479 - val_mse: 13656.0479 - val_mae: 82.8619\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12756.7764 - mse: 12756.7764 - mae: 80.6062 - val_loss: 13654.5645 - val_mse: 13654.5645 - val_mae: 82.8579\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12755.4082 - mse: 12755.4082 - mae: 80.6024 - val_loss: 13653.0879 - val_mse: 13653.0879 - val_mae: 82.8539\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12754.0537 - mse: 12754.0537 - mae: 80.5985 - val_loss: 13651.5977 - val_mse: 13651.5977 - val_mae: 82.8499\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12752.6631 - mse: 12752.6631 - mae: 80.5947 - val_loss: 13650.1182 - val_mse: 13650.1182 - val_mae: 82.8459\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12751.2852 - mse: 12751.2852 - mae: 80.5908 - val_loss: 13648.6387 - val_mse: 13648.6387 - val_mae: 82.8419\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12749.9102 - mse: 12749.9102 - mae: 80.5870 - val_loss: 13647.1748 - val_mse: 13647.1748 - val_mae: 82.8379\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12748.5371 - mse: 12748.5371 - mae: 80.5832 - val_loss: 13645.7080 - val_mse: 13645.7080 - val_mae: 82.8339\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12747.1807 - mse: 12747.1807 - mae: 80.5793 - val_loss: 13644.2178 - val_mse: 13644.2178 - val_mae: 82.8299\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12745.8105 - mse: 12745.8105 - mae: 80.5755 - val_loss: 13642.7266 - val_mse: 13642.7266 - val_mae: 82.8258\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12744.4219 - mse: 12744.4219 - mae: 80.5716 - val_loss: 13641.2432 - val_mse: 13641.2432 - val_mae: 82.8218\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12743.0498 - mse: 12743.0498 - mae: 80.5677 - val_loss: 13639.7578 - val_mse: 13639.7578 - val_mae: 82.8178\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12741.6777 - mse: 12741.6777 - mae: 80.5639 - val_loss: 13638.2803 - val_mse: 13638.2803 - val_mae: 82.8138\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12740.2969 - mse: 12740.2969 - mae: 80.5600 - val_loss: 13636.8145 - val_mse: 13636.8145 - val_mae: 82.8098\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12738.9414 - mse: 12738.9414 - mae: 80.5562 - val_loss: 13635.3359 - val_mse: 13635.3359 - val_mae: 82.8058\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12737.5654 - mse: 12737.5654 - mae: 80.5524 - val_loss: 13633.8721 - val_mse: 13633.8721 - val_mae: 82.8018\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12736.1963 - mse: 12736.1963 - mae: 80.5486 - val_loss: 13632.4219 - val_mse: 13632.4219 - val_mae: 82.7979\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12734.8467 - mse: 12734.8467 - mae: 80.5448 - val_loss: 13630.9492 - val_mse: 13630.9492 - val_mae: 82.7939\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12733.4971 - mse: 12733.4971 - mae: 80.5409 - val_loss: 13629.4580 - val_mse: 13629.4580 - val_mae: 82.7898\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 12732.1084 - mse: 12732.1084 - mae: 80.5371 - val_loss: 13627.9990 - val_mse: 13627.9990 - val_mae: 82.7859\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12730.7383 - mse: 12730.7383 - mae: 80.5332 - val_loss: 13626.5410 - val_mse: 13626.5410 - val_mae: 82.7819\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12729.3926 - mse: 12729.3926 - mae: 80.5294 - val_loss: 13625.0605 - val_mse: 13625.0605 - val_mae: 82.7779\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12728.0293 - mse: 12728.0293 - mae: 80.5256 - val_loss: 13623.5791 - val_mse: 13623.5791 - val_mae: 82.7739\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12726.6504 - mse: 12726.6504 - mae: 80.5217 - val_loss: 13622.1055 - val_mse: 13622.1055 - val_mae: 82.7699\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12725.2783 - mse: 12725.2783 - mae: 80.5179 - val_loss: 13620.6367 - val_mse: 13620.6367 - val_mae: 82.7659\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12723.9189 - mse: 12723.9189 - mae: 80.5141 - val_loss: 13619.1719 - val_mse: 13619.1719 - val_mae: 82.7619\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12722.5479 - mse: 12722.5479 - mae: 80.5102 - val_loss: 13617.7041 - val_mse: 13617.7031 - val_mae: 82.7579\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12721.1650 - mse: 12721.1650 - mae: 80.5064 - val_loss: 13616.2432 - val_mse: 13616.2432 - val_mae: 82.7540\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12719.8145 - mse: 12719.8145 - mae: 80.5026 - val_loss: 13614.7578 - val_mse: 13614.7578 - val_mae: 82.7499\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12718.4619 - mse: 12718.4619 - mae: 80.4987 - val_loss: 13613.2637 - val_mse: 13613.2637 - val_mae: 82.7459\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12717.0723 - mse: 12717.0723 - mae: 80.4948 - val_loss: 13611.7871 - val_mse: 13611.7871 - val_mae: 82.7419\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12715.6943 - mse: 12715.6943 - mae: 80.4910 - val_loss: 13610.3242 - val_mse: 13610.3242 - val_mae: 82.7379\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12714.3252 - mse: 12714.3252 - mae: 80.4872 - val_loss: 13608.8643 - val_mse: 13608.8643 - val_mae: 82.7339\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12712.9746 - mse: 12712.9746 - mae: 80.4834 - val_loss: 13607.3887 - val_mse: 13607.3887 - val_mae: 82.7299\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12711.6006 - mse: 12711.6006 - mae: 80.4795 - val_loss: 13605.9121 - val_mse: 13605.9121 - val_mae: 82.7259\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 12710.2344 - mse: 12710.2344 - mae: 80.4757 - val_loss: 13604.4326 - val_mse: 13604.4326 - val_mae: 82.7219\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12708.8506 - mse: 12708.8506 - mae: 80.4718 - val_loss: 13602.9697 - val_mse: 13602.9697 - val_mae: 82.7179\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12707.4951 - mse: 12707.4951 - mae: 80.4680 - val_loss: 13601.4863 - val_mse: 13601.4863 - val_mae: 82.7139\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12706.1250 - mse: 12706.1250 - mae: 80.4641 - val_loss: 13600.0078 - val_mse: 13600.0078 - val_mae: 82.7099\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12704.7715 - mse: 12704.7715 - mae: 80.4603 - val_loss: 13598.5205 - val_mse: 13598.5205 - val_mae: 82.7058\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12703.3730 - mse: 12703.3730 - mae: 80.4564 - val_loss: 13597.0645 - val_mse: 13597.0645 - val_mae: 82.7019\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12702.0225 - mse: 12702.0225 - mae: 80.4526 - val_loss: 13595.5908 - val_mse: 13595.5908 - val_mae: 82.6979\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12700.6426 - mse: 12700.6426 - mae: 80.4488 - val_loss: 13594.1162 - val_mse: 13594.1162 - val_mae: 82.6939\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12699.2930 - mse: 12699.2930 - mae: 80.4449 - val_loss: 13592.6299 - val_mse: 13592.6299 - val_mae: 82.6898\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12697.9141 - mse: 12697.9141 - mae: 80.4411 - val_loss: 13591.1641 - val_mse: 13591.1641 - val_mae: 82.6859\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12696.5479 - mse: 12696.5479 - mae: 80.4372 - val_loss: 13589.7109 - val_mse: 13589.7109 - val_mae: 82.6819\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12695.1963 - mse: 12695.1963 - mae: 80.4334 - val_loss: 13588.2344 - val_mse: 13588.2344 - val_mae: 82.6779\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12693.8252 - mse: 12693.8252 - mae: 80.4296 - val_loss: 13586.7539 - val_mse: 13586.7539 - val_mae: 82.6739\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12692.4551 - mse: 12692.4551 - mae: 80.4257 - val_loss: 13585.2822 - val_mse: 13585.2822 - val_mae: 82.6699\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12691.0840 - mse: 12691.0840 - mae: 80.4219 - val_loss: 13583.8242 - val_mse: 13583.8242 - val_mae: 82.6659\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12689.7256 - mse: 12689.7256 - mae: 80.4181 - val_loss: 13582.3545 - val_mse: 13582.3545 - val_mae: 82.6619\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12688.3779 - mse: 12688.3779 - mae: 80.4142 - val_loss: 13580.8701 - val_mse: 13580.8701 - val_mae: 82.6579\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12686.9990 - mse: 12686.9990 - mae: 80.4104 - val_loss: 13579.4287 - val_mse: 13579.4287 - val_mae: 82.6540\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12685.6445 - mse: 12685.6445 - mae: 80.4066 - val_loss: 13577.9785 - val_mse: 13577.9785 - val_mae: 82.6500\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12684.3135 - mse: 12684.3135 - mae: 80.4028 - val_loss: 13576.4932 - val_mse: 13576.4932 - val_mae: 82.6460\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12682.9131 - mse: 12682.9131 - mae: 80.3989 - val_loss: 13575.0439 - val_mse: 13575.0439 - val_mae: 82.6420\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12681.5732 - mse: 12681.5732 - mae: 80.3951 - val_loss: 13573.5703 - val_mse: 13573.5703 - val_mae: 82.6380\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12680.2070 - mse: 12680.2070 - mae: 80.3913 - val_loss: 13572.0957 - val_mse: 13572.0957 - val_mae: 82.6340\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12678.8369 - mse: 12678.8369 - mae: 80.3875 - val_loss: 13570.6270 - val_mse: 13570.6270 - val_mae: 82.6300\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12677.4727 - mse: 12677.4727 - mae: 80.3836 - val_loss: 13569.1504 - val_mse: 13569.1504 - val_mae: 82.6260\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12676.1094 - mse: 12676.1094 - mae: 80.3798 - val_loss: 13567.6846 - val_mse: 13567.6846 - val_mae: 82.6220\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12674.7510 - mse: 12674.7510 - mae: 80.3759 - val_loss: 13566.2109 - val_mse: 13566.2109 - val_mae: 82.6180\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12673.3760 - mse: 12673.3760 - mae: 80.3721 - val_loss: 13564.7568 - val_mse: 13564.7568 - val_mae: 82.6141\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12672.0322 - mse: 12672.0322 - mae: 80.3683 - val_loss: 13563.2881 - val_mse: 13563.2881 - val_mae: 82.6101\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12670.6582 - mse: 12670.6582 - mae: 80.3645 - val_loss: 13561.8291 - val_mse: 13561.8281 - val_mae: 82.6061\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12669.3018 - mse: 12669.3018 - mae: 80.3606 - val_loss: 13560.3506 - val_mse: 13560.3506 - val_mae: 82.6021\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12667.9346 - mse: 12667.9346 - mae: 80.3568 - val_loss: 13558.8750 - val_mse: 13558.8750 - val_mae: 82.5981\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12666.5576 - mse: 12666.5576 - mae: 80.3529 - val_loss: 13557.4189 - val_mse: 13557.4189 - val_mae: 82.5941\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12665.2041 - mse: 12665.2041 - mae: 80.3491 - val_loss: 13555.9502 - val_mse: 13555.9502 - val_mae: 82.5901\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12663.8389 - mse: 12663.8389 - mae: 80.3453 - val_loss: 13554.4863 - val_mse: 13554.4863 - val_mae: 82.5861\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12662.4883 - mse: 12662.4883 - mae: 80.3415 - val_loss: 13553.0039 - val_mse: 13553.0039 - val_mae: 82.5821\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12661.1260 - mse: 12661.1260 - mae: 80.3376 - val_loss: 13551.5283 - val_mse: 13551.5283 - val_mae: 82.5781\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12659.7451 - mse: 12659.7451 - mae: 80.3337 - val_loss: 13550.0615 - val_mse: 13550.0615 - val_mae: 82.5741\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12658.3809 - mse: 12658.3809 - mae: 80.3299 - val_loss: 13548.6016 - val_mse: 13548.6016 - val_mae: 82.5701\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12657.0254 - mse: 12657.0254 - mae: 80.3261 - val_loss: 13547.1504 - val_mse: 13547.1504 - val_mae: 82.5662\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12655.6602 - mse: 12655.6602 - mae: 80.3223 - val_loss: 13545.7080 - val_mse: 13545.7080 - val_mae: 82.5622\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12654.3154 - mse: 12654.3154 - mae: 80.3185 - val_loss: 13544.2383 - val_mse: 13544.2383 - val_mae: 82.5582\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12652.9688 - mse: 12652.9688 - mae: 80.3147 - val_loss: 13542.7500 - val_mse: 13542.7500 - val_mae: 82.5542\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12651.5859 - mse: 12651.5859 - mae: 80.3108 - val_loss: 13541.2852 - val_mse: 13541.2852 - val_mae: 82.5502\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12650.2227 - mse: 12650.2227 - mae: 80.3070 - val_loss: 13539.8193 - val_mse: 13539.8193 - val_mae: 82.5462\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12648.8594 - mse: 12648.8594 - mae: 80.3031 - val_loss: 13538.3516 - val_mse: 13538.3516 - val_mae: 82.5422\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12647.5029 - mse: 12647.5029 - mae: 80.2993 - val_loss: 13536.8799 - val_mse: 13536.8799 - val_mae: 82.5382\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12646.1406 - mse: 12646.1406 - mae: 80.2955 - val_loss: 13535.4170 - val_mse: 13535.4170 - val_mae: 82.5342\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12644.7871 - mse: 12644.7871 - mae: 80.2916 - val_loss: 13533.9541 - val_mse: 13533.9531 - val_mae: 82.5302\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12643.4102 - mse: 12643.4102 - mae: 80.2878 - val_loss: 13532.5166 - val_mse: 13532.5166 - val_mae: 82.5263\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12642.0850 - mse: 12642.0840 - mae: 80.2840 - val_loss: 13531.0322 - val_mse: 13531.0322 - val_mae: 82.5223\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12640.7012 - mse: 12640.7012 - mae: 80.2802 - val_loss: 13529.5645 - val_mse: 13529.5645 - val_mae: 82.5183\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12639.3516 - mse: 12639.3516 - mae: 80.2763 - val_loss: 13528.0918 - val_mse: 13528.0918 - val_mae: 82.5143\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12637.9697 - mse: 12637.9697 - mae: 80.2725 - val_loss: 13526.6436 - val_mse: 13526.6436 - val_mae: 82.5103\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12636.6338 - mse: 12636.6338 - mae: 80.2687 - val_loss: 13525.1719 - val_mse: 13525.1719 - val_mae: 82.5063\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 12635.2617 - mse: 12635.2617 - mae: 80.2649 - val_loss: 13523.7314 - val_mse: 13523.7314 - val_mae: 82.5024\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpch70q7m_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 1850.3003 - mse: 1850.3003 - mae: 28.5135 - val_loss: 366.9758 - val_mse: 366.9758 - val_mae: 17.7532\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 491.3532 - mse: 491.3532 - mae: 19.7616 - val_loss: 361.0521 - val_mse: 361.0521 - val_mae: 17.5472\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 482.5435 - mse: 482.5435 - mae: 19.4858 - val_loss: 356.2562 - val_mse: 356.2562 - val_mae: 17.3738\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 475.5667 - mse: 475.5667 - mae: 19.2710 - val_loss: 352.1903 - val_mse: 352.1903 - val_mae: 17.1973\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 468.9790 - mse: 468.9790 - mae: 19.0647 - val_loss: 348.2986 - val_mse: 348.2986 - val_mae: 17.0325\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 463.6894 - mse: 463.6894 - mae: 18.8899 - val_loss: 344.2361 - val_mse: 344.2361 - val_mae: 16.8824\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 458.6401 - mse: 458.6401 - mae: 18.7419 - val_loss: 340.3820 - val_mse: 340.3820 - val_mae: 16.7413\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 454.1712 - mse: 454.1712 - mae: 18.6014 - val_loss: 336.9600 - val_mse: 336.9600 - val_mae: 16.6005\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 449.8330 - mse: 449.8330 - mae: 18.4616 - val_loss: 333.8289 - val_mse: 333.8289 - val_mae: 16.4660\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 445.7928 - mse: 445.7927 - mae: 18.3238 - val_loss: 330.3215 - val_mse: 330.3215 - val_mae: 16.3545\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 442.3611 - mse: 442.3611 - mae: 18.2161 - val_loss: 327.7338 - val_mse: 327.7338 - val_mae: 16.2577\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 438.9980 - mse: 438.9980 - mae: 18.0968 - val_loss: 324.7314 - val_mse: 324.7314 - val_mae: 16.1709\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 436.1146 - mse: 436.1146 - mae: 18.0022 - val_loss: 322.3548 - val_mse: 322.3548 - val_mae: 16.0911\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 433.5912 - mse: 433.5912 - mae: 17.9110 - val_loss: 319.8760 - val_mse: 319.8760 - val_mae: 16.0103\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 430.8833 - mse: 430.8833 - mae: 17.8212 - val_loss: 317.2411 - val_mse: 317.2411 - val_mae: 15.9283\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 428.7587 - mse: 428.7587 - mae: 17.7388 - val_loss: 314.7619 - val_mse: 314.7619 - val_mae: 15.8491\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 426.6400 - mse: 426.6400 - mae: 17.6765 - val_loss: 313.2693 - val_mse: 313.2693 - val_mae: 15.7786\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 424.6775 - mse: 424.6775 - mae: 17.5997 - val_loss: 312.0250 - val_mse: 312.0250 - val_mae: 15.7175\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 422.9074 - mse: 422.9074 - mae: 17.5276 - val_loss: 310.6295 - val_mse: 310.6295 - val_mae: 15.6644\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 421.3353 - mse: 421.3353 - mae: 17.4652 - val_loss: 309.2447 - val_mse: 309.2447 - val_mae: 15.6136\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 419.8660 - mse: 419.8660 - mae: 17.4119 - val_loss: 307.5807 - val_mse: 307.5807 - val_mae: 15.5567\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 418.5901 - mse: 418.5901 - mae: 17.3597 - val_loss: 306.0969 - val_mse: 306.0969 - val_mae: 15.5041\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 417.2589 - mse: 417.2589 - mae: 17.3094 - val_loss: 305.4037 - val_mse: 305.4037 - val_mae: 15.4673\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 416.1311 - mse: 416.1311 - mae: 17.2596 - val_loss: 304.4236 - val_mse: 304.4236 - val_mae: 15.4243\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 415.0176 - mse: 415.0176 - mae: 17.2158 - val_loss: 303.3874 - val_mse: 303.3874 - val_mae: 15.3795\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 414.0067 - mse: 414.0067 - mae: 17.1740 - val_loss: 302.4005 - val_mse: 302.4005 - val_mae: 15.3354\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 413.1740 - mse: 413.1740 - mae: 17.1392 - val_loss: 301.7011 - val_mse: 301.7011 - val_mae: 15.2975\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 412.4550 - mse: 412.4550 - mae: 17.1045 - val_loss: 300.6878 - val_mse: 300.6878 - val_mae: 15.2550\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 411.5073 - mse: 411.5073 - mae: 17.0630 - val_loss: 300.2152 - val_mse: 300.2152 - val_mae: 15.2190\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 410.7060 - mse: 410.7060 - mae: 17.0287 - val_loss: 298.9543 - val_mse: 298.9543 - val_mae: 15.1735\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 409.9940 - mse: 409.9940 - mae: 16.9893 - val_loss: 297.9579 - val_mse: 297.9579 - val_mae: 15.1343\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 409.2662 - mse: 409.2662 - mae: 16.9620 - val_loss: 297.7627 - val_mse: 297.7627 - val_mae: 15.1004\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 408.6037 - mse: 408.6037 - mae: 16.9282 - val_loss: 297.4955 - val_mse: 297.4955 - val_mae: 15.0668\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 407.9366 - mse: 407.9366 - mae: 16.8996 - val_loss: 296.9267 - val_mse: 296.9267 - val_mae: 15.0299\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 407.4628 - mse: 407.4628 - mae: 16.8735 - val_loss: 296.1995 - val_mse: 296.1995 - val_mae: 14.9911\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 406.7621 - mse: 406.7621 - mae: 16.8400 - val_loss: 294.8412 - val_mse: 294.8412 - val_mae: 14.9473\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 406.1477 - mse: 406.1477 - mae: 16.8082 - val_loss: 294.4199 - val_mse: 294.4199 - val_mae: 14.9127\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 405.6596 - mse: 405.6596 - mae: 16.7869 - val_loss: 293.7007 - val_mse: 293.7007 - val_mae: 14.8763\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 405.1374 - mse: 405.1374 - mae: 16.7579 - val_loss: 293.6860 - val_mse: 293.6860 - val_mae: 14.8449\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 404.7591 - mse: 404.7591 - mae: 16.7406 - val_loss: 293.3966 - val_mse: 293.3966 - val_mae: 14.8119\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 404.0645 - mse: 404.0645 - mae: 16.7159 - val_loss: 292.6215 - val_mse: 292.6215 - val_mae: 14.7744\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 403.5509 - mse: 403.5509 - mae: 16.6875 - val_loss: 291.9730 - val_mse: 291.9730 - val_mae: 14.7390\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 403.1367 - mse: 403.1367 - mae: 16.6684 - val_loss: 291.1687 - val_mse: 291.1687 - val_mae: 14.7019\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 402.6805 - mse: 402.6805 - mae: 16.6427 - val_loss: 290.9760 - val_mse: 290.9760 - val_mae: 14.6723\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 402.2890 - mse: 402.2890 - mae: 16.6295 - val_loss: 290.4316 - val_mse: 290.4316 - val_mae: 14.6393\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 401.7870 - mse: 401.7870 - mae: 16.5998 - val_loss: 289.9518 - val_mse: 289.9518 - val_mae: 14.6078\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 401.6106 - mse: 401.6106 - mae: 16.5886 - val_loss: 289.6735 - val_mse: 289.6735 - val_mae: 14.5822\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 401.0404 - mse: 401.0404 - mae: 16.5620 - val_loss: 289.4471 - val_mse: 289.4471 - val_mae: 14.5589\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 400.6033 - mse: 400.6033 - mae: 16.5415 - val_loss: 289.1913 - val_mse: 289.1913 - val_mae: 14.5355\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 400.2279 - mse: 400.2279 - mae: 16.5220 - val_loss: 288.8131 - val_mse: 288.8131 - val_mae: 14.5081\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 399.8920 - mse: 399.8920 - mae: 16.5022 - val_loss: 288.4282 - val_mse: 288.4282 - val_mae: 14.4788\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 399.6152 - mse: 399.6152 - mae: 16.4921 - val_loss: 288.0000 - val_mse: 288.0000 - val_mae: 14.4491\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 399.2507 - mse: 399.2507 - mae: 16.4749 - val_loss: 287.6771 - val_mse: 287.6771 - val_mae: 14.4217\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 398.9539 - mse: 398.9539 - mae: 16.4704 - val_loss: 286.8157 - val_mse: 286.8157 - val_mae: 14.3787\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 398.5728 - mse: 398.5728 - mae: 16.4426 - val_loss: 286.8199 - val_mse: 286.8199 - val_mae: 14.3612\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 398.1824 - mse: 398.1824 - mae: 16.4307 - val_loss: 286.4487 - val_mse: 286.4487 - val_mae: 14.3319\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 397.8878 - mse: 397.8878 - mae: 16.4115 - val_loss: 286.1032 - val_mse: 286.1032 - val_mae: 14.3032\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 397.6252 - mse: 397.6252 - mae: 16.4034 - val_loss: 286.0411 - val_mse: 286.0411 - val_mae: 14.2877\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 397.4008 - mse: 397.4008 - mae: 16.4014 - val_loss: 285.3598 - val_mse: 285.3598 - val_mae: 14.2520\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 397.0196 - mse: 397.0196 - mae: 16.3686 - val_loss: 285.2866 - val_mse: 285.2866 - val_mae: 14.2403\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 396.8943 - mse: 396.8943 - mae: 16.3627 - val_loss: 284.9949 - val_mse: 284.9949 - val_mae: 14.2226\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 396.4963 - mse: 396.4963 - mae: 16.3576 - val_loss: 284.5038 - val_mse: 284.5038 - val_mae: 14.1969\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 396.2869 - mse: 396.2869 - mae: 16.3414 - val_loss: 284.2977 - val_mse: 284.2977 - val_mae: 14.1813\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 396.0064 - mse: 396.0064 - mae: 16.3278 - val_loss: 284.2285 - val_mse: 284.2285 - val_mae: 14.1698\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 395.8274 - mse: 395.8274 - mae: 16.3198 - val_loss: 283.9014 - val_mse: 283.9014 - val_mae: 14.1498\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 395.5531 - mse: 395.5531 - mae: 16.3066 - val_loss: 283.6927 - val_mse: 283.6927 - val_mae: 14.1328\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 395.3598 - mse: 395.3598 - mae: 16.2933 - val_loss: 283.6298 - val_mse: 283.6298 - val_mae: 14.1208\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 395.1190 - mse: 395.1190 - mae: 16.2957 - val_loss: 283.5185 - val_mse: 283.5185 - val_mae: 14.1066\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 394.9213 - mse: 394.9213 - mae: 16.2875 - val_loss: 283.2537 - val_mse: 283.2537 - val_mae: 14.0876\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 394.6274 - mse: 394.6274 - mae: 16.2697 - val_loss: 283.0415 - val_mse: 283.0415 - val_mae: 14.0704\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 394.5369 - mse: 394.5369 - mae: 16.2818 - val_loss: 282.4557 - val_mse: 282.4557 - val_mae: 14.0380\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 394.4014 - mse: 394.4014 - mae: 16.2357 - val_loss: 282.6413 - val_mse: 282.6413 - val_mae: 14.0374\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 394.1292 - mse: 394.1292 - mae: 16.2576 - val_loss: 282.2591 - val_mse: 282.2591 - val_mae: 14.0105\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 394.0016 - mse: 394.0016 - mae: 16.2355 - val_loss: 282.4226 - val_mse: 282.4226 - val_mae: 14.0141\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 393.8499 - mse: 393.8499 - mae: 16.2383 - val_loss: 282.2839 - val_mse: 282.2839 - val_mae: 14.0003\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 393.7743 - mse: 393.7743 - mae: 16.2386 - val_loss: 282.0094 - val_mse: 282.0094 - val_mae: 13.9781\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 393.5325 - mse: 393.5325 - mae: 16.2347 - val_loss: 281.8873 - val_mse: 281.8873 - val_mae: 13.9647\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 393.3138 - mse: 393.3138 - mae: 16.2269 - val_loss: 281.7600 - val_mse: 281.7600 - val_mae: 13.9511\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 393.1746 - mse: 393.1746 - mae: 16.2197 - val_loss: 281.6284 - val_mse: 281.6284 - val_mae: 13.9364\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 393.1965 - mse: 393.1965 - mae: 16.2234 - val_loss: 281.6336 - val_mse: 281.6336 - val_mae: 13.9289\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 393.0230 - mse: 393.0230 - mae: 16.2250 - val_loss: 281.3409 - val_mse: 281.3409 - val_mae: 13.9133\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 392.8181 - mse: 392.8181 - mae: 16.1977 - val_loss: 281.4414 - val_mse: 281.4414 - val_mae: 13.9143\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 392.7296 - mse: 392.7296 - mae: 16.2078 - val_loss: 281.3348 - val_mse: 281.3348 - val_mae: 13.9047\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 392.5550 - mse: 392.5550 - mae: 16.1952 - val_loss: 281.3052 - val_mse: 281.3052 - val_mae: 13.8982\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 392.4863 - mse: 392.4863 - mae: 16.1895 - val_loss: 281.2577 - val_mse: 281.2577 - val_mae: 13.8923\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 392.2646 - mse: 392.2646 - mae: 16.1898 - val_loss: 281.3081 - val_mse: 281.3081 - val_mae: 13.8923\n",
      "Epoch 86: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpund642xb/assets\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: 1406.1501 - mse: 1406.1501 - mae: 26.5646 - val_loss: 418.2131 - val_mse: 418.2131 - val_mae: 18.2862\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 543.8263 - mse: 543.8263 - mae: 20.2544 - val_loss: 358.8059 - val_mse: 358.8059 - val_mae: 17.2392\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 501.6839 - mse: 501.6839 - mae: 19.5004 - val_loss: 336.7396 - val_mse: 336.7396 - val_mae: 16.8127\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 476.8536 - mse: 476.8536 - mae: 18.9421 - val_loss: 319.8219 - val_mse: 319.8219 - val_mae: 16.3350\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 452.7808 - mse: 452.7808 - mae: 18.2273 - val_loss: 298.4970 - val_mse: 298.4970 - val_mae: 15.6580\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 427.0948 - mse: 427.0948 - mae: 17.4437 - val_loss: 277.0895 - val_mse: 277.0895 - val_mae: 14.7870\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 406.6086 - mse: 406.6086 - mae: 16.7910 - val_loss: 269.2771 - val_mse: 269.2771 - val_mae: 14.3394\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 396.3168 - mse: 396.3168 - mae: 16.4670 - val_loss: 262.5535 - val_mse: 262.5535 - val_mae: 14.0300\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 387.4209 - mse: 387.4209 - mae: 16.1837 - val_loss: 258.9990 - val_mse: 258.9990 - val_mae: 13.8977\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 381.1799 - mse: 381.1799 - mae: 15.9863 - val_loss: 255.9095 - val_mse: 255.9095 - val_mae: 13.7790\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 376.6830 - mse: 376.6830 - mae: 15.8422 - val_loss: 247.0953 - val_mse: 247.0953 - val_mae: 13.4096\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 371.6135 - mse: 371.6135 - mae: 15.6236 - val_loss: 246.7583 - val_mse: 246.7583 - val_mae: 13.4055\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 367.6719 - mse: 367.6719 - mae: 15.5304 - val_loss: 240.6986 - val_mse: 240.6986 - val_mae: 13.0859\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.8882 - mse: 363.8882 - mae: 15.3566 - val_loss: 237.5388 - val_mse: 237.5388 - val_mae: 12.9065\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 360.2625 - mse: 360.2625 - mae: 15.2015 - val_loss: 234.7795 - val_mse: 234.7795 - val_mae: 12.7607\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 358.2199 - mse: 358.2199 - mae: 15.1238 - val_loss: 234.1391 - val_mse: 234.1391 - val_mae: 12.7532\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 355.0327 - mse: 355.0327 - mae: 15.0134 - val_loss: 236.1693 - val_mse: 236.1693 - val_mae: 13.0026\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 352.7798 - mse: 352.7798 - mae: 14.9889 - val_loss: 229.9564 - val_mse: 229.9564 - val_mae: 12.6994\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 348.9392 - mse: 348.9392 - mae: 14.8966 - val_loss: 228.1309 - val_mse: 228.1309 - val_mae: 12.7261\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 345.5490 - mse: 345.5490 - mae: 14.8193 - val_loss: 223.4164 - val_mse: 223.4164 - val_mae: 12.4496\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 340.4334 - mse: 340.4334 - mae: 14.6748 - val_loss: 219.6493 - val_mse: 219.6493 - val_mae: 12.5765\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 329.6571 - mse: 329.6571 - mae: 14.4993 - val_loss: 209.4415 - val_mse: 209.4415 - val_mae: 12.2344\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 319.4030 - mse: 319.4030 - mae: 14.2604 - val_loss: 205.9453 - val_mse: 205.9453 - val_mae: 12.2625\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 310.0314 - mse: 310.0314 - mae: 14.0890 - val_loss: 199.5826 - val_mse: 199.5826 - val_mae: 12.0531\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 301.5589 - mse: 301.5589 - mae: 13.8929 - val_loss: 192.4989 - val_mse: 192.4989 - val_mae: 11.7326\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 294.7491 - mse: 294.7491 - mae: 13.7020 - val_loss: 189.1545 - val_mse: 189.1545 - val_mae: 11.6567\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 288.5536 - mse: 288.5536 - mae: 13.5121 - val_loss: 186.3673 - val_mse: 186.3673 - val_mae: 11.6075\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 283.3570 - mse: 283.3570 - mae: 13.3875 - val_loss: 180.9892 - val_mse: 180.9892 - val_mae: 11.3458\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 278.5050 - mse: 278.5050 - mae: 13.2150 - val_loss: 177.8673 - val_mse: 177.8673 - val_mae: 11.2842\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 274.3152 - mse: 274.3152 - mae: 13.0666 - val_loss: 175.4561 - val_mse: 175.4561 - val_mae: 11.1809\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 271.8190 - mse: 271.8190 - mae: 13.0003 - val_loss: 170.4161 - val_mse: 170.4161 - val_mae: 10.8492\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 266.7789 - mse: 266.7789 - mae: 12.7487 - val_loss: 177.0611 - val_mse: 177.0611 - val_mae: 11.3052\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 263.5335 - mse: 263.5335 - mae: 12.7279 - val_loss: 168.5993 - val_mse: 168.5993 - val_mae: 10.9199\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 260.2484 - mse: 260.2484 - mae: 12.5747 - val_loss: 168.9328 - val_mse: 168.9328 - val_mae: 10.9399\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 257.5065 - mse: 257.5065 - mae: 12.4675 - val_loss: 164.4452 - val_mse: 164.4452 - val_mae: 10.7173\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 254.8768 - mse: 254.8768 - mae: 12.3674 - val_loss: 159.6785 - val_mse: 159.6785 - val_mae: 10.4229\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 253.3994 - mse: 253.3994 - mae: 12.2692 - val_loss: 159.1841 - val_mse: 159.1841 - val_mae: 10.3814\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 251.3569 - mse: 251.3569 - mae: 12.1731 - val_loss: 156.1413 - val_mse: 156.1413 - val_mae: 10.2140\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 250.3718 - mse: 250.3718 - mae: 12.1306 - val_loss: 157.6898 - val_mse: 157.6898 - val_mae: 10.3511\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 247.5677 - mse: 247.5677 - mae: 12.0348 - val_loss: 156.5081 - val_mse: 156.5081 - val_mae: 10.2849\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 246.0759 - mse: 246.0759 - mae: 11.9734 - val_loss: 155.7232 - val_mse: 155.7232 - val_mae: 10.2296\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 245.4080 - mse: 245.4080 - mae: 11.9451 - val_loss: 153.1182 - val_mse: 153.1182 - val_mae: 10.0560\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 244.2364 - mse: 244.2364 - mae: 11.8834 - val_loss: 154.9781 - val_mse: 154.9781 - val_mae: 10.1568\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 243.1088 - mse: 243.1088 - mae: 11.8457 - val_loss: 154.4885 - val_mse: 154.4885 - val_mae: 10.1112\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 241.6343 - mse: 241.6343 - mae: 11.8654 - val_loss: 150.1478 - val_mse: 150.1478 - val_mae: 9.8412\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 242.0032 - mse: 242.0032 - mae: 11.7774 - val_loss: 149.7063 - val_mse: 149.7063 - val_mae: 9.8208\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 240.6635 - mse: 240.6635 - mae: 11.7361 - val_loss: 149.5750 - val_mse: 149.5750 - val_mae: 9.8401\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 240.0507 - mse: 240.0507 - mae: 11.6861 - val_loss: 153.8784 - val_mse: 153.8784 - val_mae: 10.0456\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 238.6595 - mse: 238.6595 - mae: 11.7002 - val_loss: 151.8171 - val_mse: 151.8171 - val_mae: 9.9199\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 238.6592 - mse: 238.6592 - mae: 11.6975 - val_loss: 148.9650 - val_mse: 148.9650 - val_mae: 9.7576\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 237.9534 - mse: 237.9534 - mae: 11.6460 - val_loss: 152.1734 - val_mse: 152.1734 - val_mae: 9.9265\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 237.7589 - mse: 237.7589 - mae: 11.6764 - val_loss: 150.1655 - val_mse: 150.1655 - val_mae: 9.8155\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 237.0221 - mse: 237.0221 - mae: 11.6197 - val_loss: 152.4987 - val_mse: 152.4987 - val_mae: 9.9604\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 237.2969 - mse: 237.2969 - mae: 11.6707 - val_loss: 149.2743 - val_mse: 149.2743 - val_mae: 9.7858\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 236.2619 - mse: 236.2619 - mae: 11.5788 - val_loss: 153.1352 - val_mse: 153.1352 - val_mae: 10.0205\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 236.0671 - mse: 236.0671 - mae: 11.6735 - val_loss: 148.5496 - val_mse: 148.5496 - val_mae: 9.7694\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 235.4673 - mse: 235.4673 - mae: 11.6255 - val_loss: 146.4372 - val_mse: 146.4372 - val_mae: 9.6086\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 235.0908 - mse: 235.0908 - mae: 11.5550 - val_loss: 146.1229 - val_mse: 146.1229 - val_mae: 9.6043\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 234.8461 - mse: 234.8461 - mae: 11.4680 - val_loss: 152.5188 - val_mse: 152.5188 - val_mae: 10.0425\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 234.8098 - mse: 234.8098 - mae: 11.6012 - val_loss: 149.8648 - val_mse: 149.8648 - val_mae: 9.8888\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 234.9804 - mse: 234.9804 - mae: 11.6193 - val_loss: 145.4863 - val_mse: 145.4863 - val_mae: 9.5671\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 233.9414 - mse: 233.9414 - mae: 11.4778 - val_loss: 145.0913 - val_mse: 145.0913 - val_mae: 9.6023\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 233.4787 - mse: 233.4787 - mae: 11.4859 - val_loss: 144.6623 - val_mse: 144.6623 - val_mae: 9.5565\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 233.4058 - mse: 233.4058 - mae: 11.4582 - val_loss: 147.8119 - val_mse: 147.8119 - val_mae: 9.7973\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 233.4978 - mse: 233.4978 - mae: 11.4696 - val_loss: 147.5706 - val_mse: 147.5706 - val_mae: 9.7859\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 232.6079 - mse: 232.6079 - mae: 11.4893 - val_loss: 145.9024 - val_mse: 145.9024 - val_mae: 9.6769\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 232.5258 - mse: 232.5258 - mae: 11.4777 - val_loss: 143.8591 - val_mse: 143.8591 - val_mae: 9.5073\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 232.6621 - mse: 232.6621 - mae: 11.3958 - val_loss: 149.4156 - val_mse: 149.4156 - val_mae: 9.8911\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 233.0715 - mse: 233.0715 - mae: 11.5857 - val_loss: 144.0740 - val_mse: 144.0740 - val_mae: 9.5163\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 231.7531 - mse: 231.7531 - mae: 11.3174 - val_loss: 148.3252 - val_mse: 148.3252 - val_mae: 9.8263\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 231.8161 - mse: 231.8161 - mae: 11.4951 - val_loss: 144.5632 - val_mse: 144.5632 - val_mae: 9.5618\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 231.8031 - mse: 231.8031 - mae: 11.3499 - val_loss: 146.5371 - val_mse: 146.5371 - val_mae: 9.7309\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 231.4711 - mse: 231.4711 - mae: 11.4375 - val_loss: 145.2508 - val_mse: 145.2508 - val_mae: 9.6602\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 231.3148 - mse: 231.3148 - mae: 11.4083 - val_loss: 146.9625 - val_mse: 146.9625 - val_mae: 9.7439\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 231.3485 - mse: 231.3485 - mae: 11.4867 - val_loss: 144.9717 - val_mse: 144.9717 - val_mae: 9.6372\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 231.4790 - mse: 231.4790 - mae: 11.3770 - val_loss: 145.9248 - val_mse: 145.9248 - val_mae: 9.7001\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 230.9705 - mse: 230.9705 - mae: 11.4199 - val_loss: 145.3670 - val_mse: 145.3670 - val_mae: 9.6570\n",
      "Epoch 77: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp5jtvc6in/assets\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 1049.6431 - mse: 1049.6431 - mae: 26.0056 - val_loss: 486.5774 - val_mse: 486.5774 - val_mae: 20.2565\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 496.3748 - mse: 496.3748 - mae: 20.2948 - val_loss: 457.9655 - val_mse: 457.9655 - val_mae: 19.0862\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 476.0456 - mse: 476.0456 - mae: 19.5654 - val_loss: 451.5704 - val_mse: 451.5704 - val_mae: 18.7741\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 463.4196 - mse: 463.4196 - mae: 19.0354 - val_loss: 443.2831 - val_mse: 443.2831 - val_mae: 18.4256\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 453.1530 - mse: 453.1530 - mae: 18.5695 - val_loss: 439.7522 - val_mse: 439.7522 - val_mae: 18.2071\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 449.1968 - mse: 449.1968 - mae: 18.3000 - val_loss: 438.4861 - val_mse: 438.4861 - val_mae: 18.1087\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 447.6186 - mse: 447.6186 - mae: 18.1975 - val_loss: 438.1545 - val_mse: 438.1545 - val_mae: 18.0708\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 446.5320 - mse: 446.5320 - mae: 18.1617 - val_loss: 437.3326 - val_mse: 437.3326 - val_mae: 18.0094\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 446.0184 - mse: 446.0184 - mae: 18.0965 - val_loss: 436.8350 - val_mse: 436.8350 - val_mae: 17.9844\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 445.7791 - mse: 445.7791 - mae: 18.0882 - val_loss: 436.3276 - val_mse: 436.3276 - val_mae: 17.9644\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 445.5953 - mse: 445.5953 - mae: 18.0743 - val_loss: 436.0132 - val_mse: 436.0132 - val_mae: 17.9526\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 445.3763 - mse: 445.3763 - mae: 18.0652 - val_loss: 435.7554 - val_mse: 435.7554 - val_mae: 17.9425\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 444.9153 - mse: 444.9153 - mae: 18.0445 - val_loss: 435.9262 - val_mse: 435.9262 - val_mae: 17.9526\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 444.7442 - mse: 444.7442 - mae: 18.0475 - val_loss: 435.4048 - val_mse: 435.4048 - val_mae: 17.9191\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 444.4284 - mse: 444.4284 - mae: 18.0271 - val_loss: 435.0461 - val_mse: 435.0461 - val_mae: 17.8973\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 444.2215 - mse: 444.2215 - mae: 18.0029 - val_loss: 434.8351 - val_mse: 434.8351 - val_mae: 17.8917\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 443.9994 - mse: 443.9994 - mae: 18.0122 - val_loss: 434.0389 - val_mse: 434.0389 - val_mae: 17.8597\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 443.8540 - mse: 443.8540 - mae: 17.9861 - val_loss: 433.4944 - val_mse: 433.4944 - val_mae: 17.8394\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 443.6055 - mse: 443.6055 - mae: 17.9600 - val_loss: 432.7113 - val_mse: 432.7113 - val_mae: 17.8108\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 442.8842 - mse: 442.8842 - mae: 17.9456 - val_loss: 430.9526 - val_mse: 430.9526 - val_mae: 17.7388\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 441.9672 - mse: 441.9672 - mae: 17.9017 - val_loss: 429.7775 - val_mse: 429.7775 - val_mae: 17.6884\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 440.7096 - mse: 440.7096 - mae: 17.8489 - val_loss: 428.6848 - val_mse: 428.6848 - val_mae: 17.6334\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 439.1941 - mse: 439.1941 - mae: 17.8029 - val_loss: 427.3505 - val_mse: 427.3505 - val_mae: 17.5330\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 437.4152 - mse: 437.4152 - mae: 17.7292 - val_loss: 426.6016 - val_mse: 426.6016 - val_mae: 17.4736\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 436.0848 - mse: 436.0848 - mae: 17.6688 - val_loss: 425.6187 - val_mse: 425.6187 - val_mae: 17.4153\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 434.7440 - mse: 434.7440 - mae: 17.6212 - val_loss: 425.4687 - val_mse: 425.4687 - val_mae: 17.3799\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 432.2887 - mse: 432.2887 - mae: 17.5529 - val_loss: 424.1709 - val_mse: 424.1709 - val_mae: 17.3479\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 428.2442 - mse: 428.2442 - mae: 17.4507 - val_loss: 420.6302 - val_mse: 420.6302 - val_mae: 17.2931\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 419.6268 - mse: 419.6268 - mae: 17.2298 - val_loss: 411.2363 - val_mse: 411.2363 - val_mae: 17.1390\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 407.3146 - mse: 407.3146 - mae: 16.9118 - val_loss: 403.0149 - val_mse: 403.0149 - val_mae: 16.9383\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 396.9398 - mse: 396.9398 - mae: 16.6416 - val_loss: 391.5113 - val_mse: 391.5113 - val_mae: 16.6635\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 387.9130 - mse: 387.9130 - mae: 16.3560 - val_loss: 384.9781 - val_mse: 384.9781 - val_mae: 16.4727\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 383.0510 - mse: 383.0510 - mae: 16.2005 - val_loss: 376.3441 - val_mse: 376.3441 - val_mae: 16.1733\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 379.4230 - mse: 379.4230 - mae: 16.0727 - val_loss: 372.6645 - val_mse: 372.6645 - val_mae: 16.0689\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 375.9212 - mse: 375.9212 - mae: 15.9775 - val_loss: 368.7096 - val_mse: 368.7096 - val_mae: 15.9379\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 374.0189 - mse: 374.0189 - mae: 15.9233 - val_loss: 367.1906 - val_mse: 367.1906 - val_mae: 15.9039\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 372.1513 - mse: 372.1513 - mae: 15.8817 - val_loss: 362.3756 - val_mse: 362.3756 - val_mae: 15.6755\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 371.8976 - mse: 371.8976 - mae: 15.8484 - val_loss: 360.8921 - val_mse: 360.8921 - val_mae: 15.6248\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 370.9289 - mse: 370.9289 - mae: 15.8049 - val_loss: 360.4674 - val_mse: 360.4674 - val_mae: 15.6469\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 369.1334 - mse: 369.1334 - mae: 15.7570 - val_loss: 359.6138 - val_mse: 359.6138 - val_mae: 15.6190\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 368.6335 - mse: 368.6335 - mae: 15.7391 - val_loss: 357.7140 - val_mse: 357.7140 - val_mae: 15.5221\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 367.7514 - mse: 367.7514 - mae: 15.6917 - val_loss: 361.1096 - val_mse: 361.1096 - val_mae: 15.7229\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 367.7942 - mse: 367.7942 - mae: 15.7674 - val_loss: 355.1483 - val_mse: 355.1483 - val_mae: 15.3825\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 367.0121 - mse: 367.0121 - mae: 15.6636 - val_loss: 354.6754 - val_mse: 354.6754 - val_mae: 15.3745\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 366.4324 - mse: 366.4324 - mae: 15.6656 - val_loss: 353.7956 - val_mse: 353.7956 - val_mae: 15.3396\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 365.8257 - mse: 365.8257 - mae: 15.6241 - val_loss: 353.8784 - val_mse: 353.8784 - val_mae: 15.3602\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 365.4122 - mse: 365.4122 - mae: 15.6567 - val_loss: 353.0042 - val_mse: 353.0042 - val_mae: 15.3059\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 365.4211 - mse: 365.4211 - mae: 15.6161 - val_loss: 352.6920 - val_mse: 352.6920 - val_mae: 15.3072\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 365.2208 - mse: 365.2208 - mae: 15.5998 - val_loss: 352.4816 - val_mse: 352.4816 - val_mae: 15.3110\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 365.0181 - mse: 365.0181 - mae: 15.5833 - val_loss: 353.1513 - val_mse: 353.1513 - val_mae: 15.3642\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 364.7386 - mse: 364.7386 - mae: 15.6223 - val_loss: 352.1501 - val_mse: 352.1501 - val_mae: 15.3090\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 364.7382 - mse: 364.7382 - mae: 15.5561 - val_loss: 352.0067 - val_mse: 352.0067 - val_mae: 15.3214\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 364.2405 - mse: 364.2405 - mae: 15.5603 - val_loss: 352.1893 - val_mse: 352.1893 - val_mae: 15.3352\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 364.0312 - mse: 364.0312 - mae: 15.5657 - val_loss: 352.0730 - val_mse: 352.0730 - val_mae: 15.3364\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 364.2552 - mse: 364.2552 - mae: 15.5814 - val_loss: 351.6611 - val_mse: 351.6611 - val_mae: 15.3246\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 364.0163 - mse: 364.0163 - mae: 15.5728 - val_loss: 351.7146 - val_mse: 351.7146 - val_mae: 15.3103\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.7142 - mse: 363.7142 - mae: 15.5362 - val_loss: 351.4959 - val_mse: 351.4959 - val_mae: 15.3198\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.8513 - mse: 363.8513 - mae: 15.5524 - val_loss: 351.6201 - val_mse: 351.6201 - val_mae: 15.3357\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.7198 - mse: 363.7198 - mae: 15.5650 - val_loss: 351.8420 - val_mse: 351.8420 - val_mae: 15.3495\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.4914 - mse: 363.4914 - mae: 15.5792 - val_loss: 351.7652 - val_mse: 351.7652 - val_mae: 15.3110\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.4107 - mse: 363.4107 - mae: 15.5161 - val_loss: 351.5761 - val_mse: 351.5761 - val_mae: 15.3424\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.0954 - mse: 363.0954 - mae: 15.5519 - val_loss: 351.2239 - val_mse: 351.2239 - val_mae: 15.3205\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.1093 - mse: 363.1093 - mae: 15.5393 - val_loss: 351.3828 - val_mse: 351.3828 - val_mae: 15.3132\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.2010 - mse: 363.2010 - mae: 15.5285 - val_loss: 351.1390 - val_mse: 351.1390 - val_mae: 15.3239\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.1783 - mse: 363.1783 - mae: 15.5544 - val_loss: 351.2066 - val_mse: 351.2066 - val_mae: 15.3143\n",
      "Epoch 65: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpvlwolnq_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 3473.1650 - mse: 3473.1650 - mae: 40.6803"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 2631.7666 - mse: 2631.7666 - mae: 36.1489 - val_loss: 1934.0570 - val_mse: 1934.0570 - val_mae: 31.0619\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1622.1151 - mse: 1622.1151 - mae: 29.4699 - val_loss: 1296.2512 - val_mse: 1296.2512 - val_mae: 26.3373\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1140.9857 - mse: 1140.9857 - mae: 25.5649 - val_loss: 964.0113 - val_mse: 964.0113 - val_mae: 23.3794\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 885.9088 - mse: 885.9088 - mae: 23.1346 - val_loss: 773.1920 - val_mse: 773.1920 - val_mae: 21.4363\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 736.7421 - mse: 736.7421 - mae: 21.4981 - val_loss: 652.3109 - val_mse: 652.3109 - val_mae: 19.9936\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 641.5434 - mse: 641.5434 - mae: 20.3212 - val_loss: 571.1158 - val_mse: 571.1158 - val_mae: 18.9486\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 578.2335 - mse: 578.2335 - mae: 19.4938 - val_loss: 514.8663 - val_mse: 514.8663 - val_mae: 18.2690\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 534.8492 - mse: 534.8492 - mae: 18.8770 - val_loss: 474.5495 - val_mse: 474.5495 - val_mae: 17.7794\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 503.4010 - mse: 503.4010 - mae: 18.4375 - val_loss: 444.4805 - val_mse: 444.4805 - val_mae: 17.3793\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 479.5669 - mse: 479.5669 - mae: 18.0830 - val_loss: 419.9955 - val_mse: 419.9955 - val_mae: 17.0183\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 460.4315 - mse: 460.4315 - mae: 17.7816 - val_loss: 400.5118 - val_mse: 400.5118 - val_mae: 16.7007\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 445.0214 - mse: 445.0214 - mae: 17.5085 - val_loss: 383.9557 - val_mse: 383.9557 - val_mae: 16.4091\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 432.0837 - mse: 432.0837 - mae: 17.2606 - val_loss: 370.1669 - val_mse: 370.1669 - val_mae: 16.1505\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 421.5898 - mse: 421.5898 - mae: 17.0613 - val_loss: 358.4565 - val_mse: 358.4565 - val_mae: 15.9158\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 412.4935 - mse: 412.4935 - mae: 16.8665 - val_loss: 348.2596 - val_mse: 348.2596 - val_mae: 15.7017\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 404.7242 - mse: 404.7242 - mae: 16.6969 - val_loss: 339.1865 - val_mse: 339.1865 - val_mae: 15.5008\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 397.8593 - mse: 397.8593 - mae: 16.5440 - val_loss: 331.3611 - val_mse: 331.3611 - val_mae: 15.3176\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 391.8807 - mse: 391.8807 - mae: 16.4021 - val_loss: 324.4316 - val_mse: 324.4316 - val_mae: 15.1481\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 386.4637 - mse: 386.4637 - mae: 16.2648 - val_loss: 318.2269 - val_mse: 318.2269 - val_mae: 15.0007\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 381.6005 - mse: 381.6005 - mae: 16.1401 - val_loss: 312.8006 - val_mse: 312.8006 - val_mae: 14.8706\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 377.2487 - mse: 377.2487 - mae: 16.0279 - val_loss: 307.8645 - val_mse: 307.8645 - val_mae: 14.7478\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 373.2688 - mse: 373.2688 - mae: 15.9244 - val_loss: 303.1499 - val_mse: 303.1499 - val_mae: 14.6299\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 369.5381 - mse: 369.5381 - mae: 15.8231 - val_loss: 298.8646 - val_mse: 298.8646 - val_mae: 14.5217\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 366.1719 - mse: 366.1719 - mae: 15.7348 - val_loss: 295.0556 - val_mse: 295.0556 - val_mae: 14.4193\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.0374 - mse: 363.0374 - mae: 15.6448 - val_loss: 291.3977 - val_mse: 291.3977 - val_mae: 14.3239\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 360.1181 - mse: 360.1181 - mae: 15.5603 - val_loss: 288.0471 - val_mse: 288.0471 - val_mae: 14.2421\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 357.5180 - mse: 357.5180 - mae: 15.4929 - val_loss: 285.0286 - val_mse: 285.0286 - val_mae: 14.1657\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 354.9907 - mse: 354.9907 - mae: 15.4180 - val_loss: 282.1632 - val_mse: 282.1632 - val_mae: 14.0945\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 352.6966 - mse: 352.6966 - mae: 15.3538 - val_loss: 279.3426 - val_mse: 279.3426 - val_mae: 14.0233\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 350.4014 - mse: 350.4014 - mae: 15.2842 - val_loss: 276.6561 - val_mse: 276.6561 - val_mae: 13.9568\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 348.3541 - mse: 348.3541 - mae: 15.2300 - val_loss: 274.1868 - val_mse: 274.1868 - val_mae: 13.8923\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 346.3947 - mse: 346.3947 - mae: 15.1696 - val_loss: 271.9682 - val_mse: 271.9682 - val_mae: 13.8310\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 344.5574 - mse: 344.5574 - mae: 15.1175 - val_loss: 269.8280 - val_mse: 269.8280 - val_mae: 13.7701\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 342.7555 - mse: 342.7555 - mae: 15.0624 - val_loss: 267.7736 - val_mse: 267.7736 - val_mae: 13.7099\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 341.1166 - mse: 341.1166 - mae: 15.0135 - val_loss: 265.6847 - val_mse: 265.6847 - val_mae: 13.6492\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 339.4148 - mse: 339.4148 - mae: 14.9596 - val_loss: 263.5629 - val_mse: 263.5629 - val_mae: 13.5912\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 337.7376 - mse: 337.7376 - mae: 14.9183 - val_loss: 261.4331 - val_mse: 261.4331 - val_mae: 13.5319\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 335.9349 - mse: 335.9349 - mae: 14.8626 - val_loss: 259.2509 - val_mse: 259.2509 - val_mae: 13.4702\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 334.1546 - mse: 334.1546 - mae: 14.8153 - val_loss: 257.2079 - val_mse: 257.2079 - val_mae: 13.4107\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 332.3922 - mse: 332.3922 - mae: 14.7644 - val_loss: 255.2123 - val_mse: 255.2123 - val_mae: 13.3524\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 330.5214 - mse: 330.5214 - mae: 14.7167 - val_loss: 253.2814 - val_mse: 253.2814 - val_mae: 13.2943\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 328.7711 - mse: 328.7711 - mae: 14.6643 - val_loss: 251.3870 - val_mse: 251.3870 - val_mae: 13.2399\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 327.0160 - mse: 327.0160 - mae: 14.6170 - val_loss: 249.4566 - val_mse: 249.4566 - val_mae: 13.1852\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 325.2345 - mse: 325.2345 - mae: 14.5728 - val_loss: 247.5632 - val_mse: 247.5632 - val_mae: 13.1303\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 323.4366 - mse: 323.4366 - mae: 14.5220 - val_loss: 245.8144 - val_mse: 245.8144 - val_mae: 13.0798\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 321.6036 - mse: 321.6036 - mae: 14.4680 - val_loss: 244.1100 - val_mse: 244.1100 - val_mae: 13.0282\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 319.9875 - mse: 319.9875 - mae: 14.4198 - val_loss: 242.3916 - val_mse: 242.3916 - val_mae: 12.9758\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 318.2849 - mse: 318.2849 - mae: 14.3684 - val_loss: 240.9296 - val_mse: 240.9296 - val_mae: 12.9322\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 316.7870 - mse: 316.7870 - mae: 14.3239 - val_loss: 239.6278 - val_mse: 239.6278 - val_mae: 12.8922\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 315.3914 - mse: 315.3914 - mae: 14.2826 - val_loss: 238.3074 - val_mse: 238.3074 - val_mae: 12.8475\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 313.9930 - mse: 313.9930 - mae: 14.2369 - val_loss: 237.0722 - val_mse: 237.0722 - val_mae: 12.8059\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 312.6898 - mse: 312.6898 - mae: 14.1991 - val_loss: 235.9091 - val_mse: 235.9091 - val_mae: 12.7642\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 311.4653 - mse: 311.4653 - mae: 14.1570 - val_loss: 234.8156 - val_mse: 234.8156 - val_mae: 12.7271\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 310.3146 - mse: 310.3146 - mae: 14.1242 - val_loss: 233.5671 - val_mse: 233.5671 - val_mae: 12.6823\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 309.2261 - mse: 309.2261 - mae: 14.0809 - val_loss: 232.5456 - val_mse: 232.5456 - val_mae: 12.6465\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 308.2151 - mse: 308.2151 - mae: 14.0509 - val_loss: 231.5820 - val_mse: 231.5820 - val_mae: 12.6092\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 307.2594 - mse: 307.2594 - mae: 14.0176 - val_loss: 230.5658 - val_mse: 230.5658 - val_mae: 12.5708\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 306.3723 - mse: 306.3723 - mae: 13.9878 - val_loss: 229.7812 - val_mse: 229.7812 - val_mae: 12.5391\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 305.5776 - mse: 305.5776 - mae: 13.9641 - val_loss: 228.8673 - val_mse: 228.8673 - val_mae: 12.5010\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 304.7252 - mse: 304.7252 - mae: 13.9322 - val_loss: 228.0775 - val_mse: 228.0775 - val_mae: 12.4675\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 304.0049 - mse: 304.0049 - mae: 13.9059 - val_loss: 227.3408 - val_mse: 227.3408 - val_mae: 12.4356\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 303.2445 - mse: 303.2445 - mae: 13.8818 - val_loss: 226.5674 - val_mse: 226.5674 - val_mae: 12.4017\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 302.5289 - mse: 302.5289 - mae: 13.8551 - val_loss: 225.8407 - val_mse: 225.8407 - val_mae: 12.3709\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 301.8727 - mse: 301.8727 - mae: 13.8328 - val_loss: 225.2498 - val_mse: 225.2498 - val_mae: 12.3463\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 301.2190 - mse: 301.2190 - mae: 13.8166 - val_loss: 224.5034 - val_mse: 224.5034 - val_mae: 12.3159\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 300.5533 - mse: 300.5533 - mae: 13.7972 - val_loss: 223.7896 - val_mse: 223.7896 - val_mae: 12.2880\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 299.9520 - mse: 299.9520 - mae: 13.7773 - val_loss: 223.1331 - val_mse: 223.1331 - val_mae: 12.2632\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 299.3453 - mse: 299.3453 - mae: 13.7590 - val_loss: 222.5595 - val_mse: 222.5595 - val_mae: 12.2404\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 298.8371 - mse: 298.8371 - mae: 13.7455 - val_loss: 221.9448 - val_mse: 221.9448 - val_mae: 12.2168\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 298.2202 - mse: 298.2202 - mae: 13.7216 - val_loss: 221.3096 - val_mse: 221.3096 - val_mae: 12.1911\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 297.7200 - mse: 297.7200 - mae: 13.7062 - val_loss: 220.6286 - val_mse: 220.6286 - val_mae: 12.1659\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 297.2013 - mse: 297.2013 - mae: 13.6838 - val_loss: 220.0809 - val_mse: 220.0809 - val_mae: 12.1442\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 296.6838 - mse: 296.6838 - mae: 13.6690 - val_loss: 219.5090 - val_mse: 219.5090 - val_mae: 12.1233\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 296.1914 - mse: 296.1914 - mae: 13.6508 - val_loss: 219.0989 - val_mse: 219.0989 - val_mae: 12.1096\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.7188 - mse: 295.7188 - mae: 13.6426 - val_loss: 218.5752 - val_mse: 218.5752 - val_mae: 12.0901\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.2761 - mse: 295.2761 - mae: 13.6257 - val_loss: 217.9937 - val_mse: 217.9937 - val_mae: 12.0693\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.7677 - mse: 294.7677 - mae: 13.6129 - val_loss: 217.5097 - val_mse: 217.5097 - val_mae: 12.0501\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 294.3014 - mse: 294.3014 - mae: 13.5977 - val_loss: 217.0583 - val_mse: 217.0583 - val_mae: 12.0333\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.8312 - mse: 293.8312 - mae: 13.5830 - val_loss: 216.5725 - val_mse: 216.5725 - val_mae: 12.0145\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 293.4142 - mse: 293.4142 - mae: 13.5701 - val_loss: 216.1580 - val_mse: 216.1580 - val_mae: 11.9997\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 292.9697 - mse: 292.9697 - mae: 13.5619 - val_loss: 215.6276 - val_mse: 215.6276 - val_mae: 11.9809\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 292.4484 - mse: 292.4484 - mae: 13.5409 - val_loss: 215.1930 - val_mse: 215.1930 - val_mae: 11.9647\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 292.0210 - mse: 292.0210 - mae: 13.5309 - val_loss: 214.7309 - val_mse: 214.7309 - val_mae: 11.9468\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 291.6069 - mse: 291.6069 - mae: 13.5151 - val_loss: 214.3071 - val_mse: 214.3071 - val_mae: 11.9305\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 291.2422 - mse: 291.2422 - mae: 13.4992 - val_loss: 213.8977 - val_mse: 213.8977 - val_mae: 11.9159\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.7739 - mse: 290.7739 - mae: 13.4872 - val_loss: 213.5350 - val_mse: 213.5350 - val_mae: 11.9029\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.3452 - mse: 290.3452 - mae: 13.4831 - val_loss: 213.0065 - val_mse: 213.0065 - val_mae: 11.8792\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.9562 - mse: 289.9562 - mae: 13.4654 - val_loss: 212.5927 - val_mse: 212.5927 - val_mae: 11.8622\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 289.6011 - mse: 289.6011 - mae: 13.4543 - val_loss: 212.1201 - val_mse: 212.1201 - val_mae: 11.8410\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 289.1906 - mse: 289.1906 - mae: 13.4454 - val_loss: 211.6435 - val_mse: 211.6435 - val_mae: 11.8198\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.8231 - mse: 288.8231 - mae: 13.4266 - val_loss: 211.2048 - val_mse: 211.2048 - val_mae: 11.8018\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 288.5049 - mse: 288.5049 - mae: 13.4160 - val_loss: 210.7825 - val_mse: 210.7825 - val_mae: 11.7846\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.1215 - mse: 288.1215 - mae: 13.4003 - val_loss: 210.3158 - val_mse: 210.3158 - val_mae: 11.7640\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.7481 - mse: 287.7481 - mae: 13.3821 - val_loss: 209.9450 - val_mse: 209.9450 - val_mae: 11.7500\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 287.3665 - mse: 287.3665 - mae: 13.3760 - val_loss: 209.5406 - val_mse: 209.5406 - val_mae: 11.7332\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 287.0156 - mse: 287.0156 - mae: 13.3639 - val_loss: 209.1077 - val_mse: 209.1077 - val_mae: 11.7158\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 286.7075 - mse: 286.7075 - mae: 13.3518 - val_loss: 208.7939 - val_mse: 208.7939 - val_mae: 11.7016\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 286.3640 - mse: 286.3640 - mae: 13.3512 - val_loss: 208.4065 - val_mse: 208.4065 - val_mae: 11.6828\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 286.0378 - mse: 286.0378 - mae: 13.3378 - val_loss: 208.0175 - val_mse: 208.0175 - val_mae: 11.6634\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.7183 - mse: 285.7183 - mae: 13.3177 - val_loss: 207.6688 - val_mse: 207.6688 - val_mae: 11.6481\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpkczelahr/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 1921.9139 - mse: 1921.9139 - mae: 32.3242"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 1627.9929 - mse: 1627.9929 - mae: 29.9224 - val_loss: 1334.0969 - val_mse: 1334.0969 - val_mae: 26.6857\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1209.3868 - mse: 1209.3868 - mae: 26.4666 - val_loss: 1010.4329 - val_mse: 1010.4329 - val_mae: 23.7831\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 965.2598 - mse: 965.2598 - mae: 24.1181 - val_loss: 811.8436 - val_mse: 811.8436 - val_mae: 21.7930\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 814.8498 - mse: 814.8498 - mae: 22.5297 - val_loss: 682.6125 - val_mse: 682.6125 - val_mae: 20.3879\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 716.2822 - mse: 716.2822 - mae: 21.3972 - val_loss: 593.4778 - val_mse: 593.4778 - val_mae: 19.3647\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 648.0739 - mse: 648.0739 - mae: 20.5647 - val_loss: 530.0462 - val_mse: 530.0462 - val_mae: 18.5742\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 599.5166 - mse: 599.5166 - mae: 19.9262 - val_loss: 483.2104 - val_mse: 483.2104 - val_mae: 17.9185\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 563.7805 - mse: 563.7805 - mae: 19.4421 - val_loss: 448.2820 - val_mse: 448.2820 - val_mae: 17.3809\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 536.8901 - mse: 536.8901 - mae: 19.0466 - val_loss: 421.5450 - val_mse: 421.5450 - val_mae: 16.9644\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 516.4324 - mse: 516.4324 - mae: 18.7366 - val_loss: 400.3953 - val_mse: 400.3953 - val_mae: 16.6287\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 500.2079 - mse: 500.2079 - mae: 18.4739 - val_loss: 383.4297 - val_mse: 383.4297 - val_mae: 16.3486\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 487.0328 - mse: 487.0328 - mae: 18.2435 - val_loss: 369.6538 - val_mse: 369.6538 - val_mae: 16.1163\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 476.1602 - mse: 476.1602 - mae: 18.0486 - val_loss: 358.1601 - val_mse: 358.1601 - val_mae: 15.9108\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 466.9324 - mse: 466.9324 - mae: 17.8746 - val_loss: 348.1014 - val_mse: 348.1014 - val_mae: 15.7235\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 458.7676 - mse: 458.7676 - mae: 17.7162 - val_loss: 339.6859 - val_mse: 339.6859 - val_mae: 15.5503\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 451.7442 - mse: 451.7442 - mae: 17.5719 - val_loss: 332.1683 - val_mse: 332.1683 - val_mae: 15.3908\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 445.4282 - mse: 445.4282 - mae: 17.4468 - val_loss: 325.4555 - val_mse: 325.4555 - val_mae: 15.2413\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 439.6388 - mse: 439.6388 - mae: 17.3244 - val_loss: 319.2740 - val_mse: 319.2740 - val_mae: 15.1038\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 434.2652 - mse: 434.2652 - mae: 17.2145 - val_loss: 313.8246 - val_mse: 313.8246 - val_mae: 14.9804\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 429.3587 - mse: 429.3587 - mae: 17.1086 - val_loss: 308.8346 - val_mse: 308.8346 - val_mae: 14.8639\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 424.8819 - mse: 424.8819 - mae: 17.0118 - val_loss: 304.1341 - val_mse: 304.1341 - val_mae: 14.7542\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 420.6092 - mse: 420.6092 - mae: 16.9193 - val_loss: 299.6767 - val_mse: 299.6767 - val_mae: 14.6462\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 416.4854 - mse: 416.4854 - mae: 16.8255 - val_loss: 295.5462 - val_mse: 295.5462 - val_mae: 14.5445\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 412.6367 - mse: 412.6367 - mae: 16.7379 - val_loss: 291.7997 - val_mse: 291.7997 - val_mae: 14.4512\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 409.0845 - mse: 409.0845 - mae: 16.6566 - val_loss: 288.1949 - val_mse: 288.1949 - val_mae: 14.3630\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 405.7226 - mse: 405.7226 - mae: 16.5781 - val_loss: 284.8758 - val_mse: 284.8758 - val_mae: 14.2813\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 402.5359 - mse: 402.5359 - mae: 16.4996 - val_loss: 281.6824 - val_mse: 281.6824 - val_mae: 14.2036\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 399.5753 - mse: 399.5753 - mae: 16.4307 - val_loss: 278.6888 - val_mse: 278.6888 - val_mae: 14.1294\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 396.7054 - mse: 396.7054 - mae: 16.3579 - val_loss: 275.8506 - val_mse: 275.8506 - val_mae: 14.0593\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 394.0594 - mse: 394.0594 - mae: 16.2926 - val_loss: 273.2617 - val_mse: 273.2617 - val_mae: 13.9926\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 391.5832 - mse: 391.5832 - mae: 16.2298 - val_loss: 270.7557 - val_mse: 270.7557 - val_mae: 13.9268\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 389.2519 - mse: 389.2519 - mae: 16.1671 - val_loss: 268.4846 - val_mse: 268.4846 - val_mae: 13.8643\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 386.9924 - mse: 386.9924 - mae: 16.1098 - val_loss: 266.2593 - val_mse: 266.2593 - val_mae: 13.8052\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 384.8994 - mse: 384.8994 - mae: 16.0540 - val_loss: 264.1444 - val_mse: 264.1444 - val_mae: 13.7478\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 382.9106 - mse: 382.9106 - mae: 16.0008 - val_loss: 262.2179 - val_mse: 262.2179 - val_mae: 13.6939\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 381.0620 - mse: 381.0620 - mae: 15.9486 - val_loss: 260.3941 - val_mse: 260.3941 - val_mae: 13.6423\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 379.2903 - mse: 379.2903 - mae: 15.8980 - val_loss: 258.6100 - val_mse: 258.6100 - val_mae: 13.5915\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 377.5823 - mse: 377.5823 - mae: 15.8497 - val_loss: 256.8853 - val_mse: 256.8853 - val_mae: 13.5416\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 375.9721 - mse: 375.9721 - mae: 15.8035 - val_loss: 255.2427 - val_mse: 255.2427 - val_mae: 13.4948\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 374.4005 - mse: 374.4005 - mae: 15.7559 - val_loss: 253.7361 - val_mse: 253.7361 - val_mae: 13.4519\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 372.9207 - mse: 372.9207 - mae: 15.7150 - val_loss: 252.3017 - val_mse: 252.3017 - val_mae: 13.4088\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 371.5175 - mse: 371.5175 - mae: 15.6724 - val_loss: 250.9135 - val_mse: 250.9135 - val_mae: 13.3661\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 370.2065 - mse: 370.2065 - mae: 15.6298 - val_loss: 249.6222 - val_mse: 249.6222 - val_mae: 13.3270\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 368.9672 - mse: 368.9672 - mae: 15.5940 - val_loss: 248.3801 - val_mse: 248.3801 - val_mae: 13.2877\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 367.8102 - mse: 367.8102 - mae: 15.5622 - val_loss: 247.1318 - val_mse: 247.1318 - val_mae: 13.2464\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 366.5759 - mse: 366.5759 - mae: 15.5186 - val_loss: 245.9099 - val_mse: 245.9099 - val_mae: 13.2059\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 365.4427 - mse: 365.4427 - mae: 15.4847 - val_loss: 244.7061 - val_mse: 244.7061 - val_mae: 13.1670\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 364.3750 - mse: 364.3750 - mae: 15.4521 - val_loss: 243.6259 - val_mse: 243.6259 - val_mae: 13.1314\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 363.2792 - mse: 363.2792 - mae: 15.4174 - val_loss: 242.6593 - val_mse: 242.6593 - val_mae: 13.1010\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 362.2673 - mse: 362.2673 - mae: 15.3896 - val_loss: 241.6225 - val_mse: 241.6225 - val_mae: 13.0653\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 361.2492 - mse: 361.2492 - mae: 15.3573 - val_loss: 240.6280 - val_mse: 240.6280 - val_mae: 13.0312\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 360.3193 - mse: 360.3193 - mae: 15.3299 - val_loss: 239.7134 - val_mse: 239.7134 - val_mae: 13.0017\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 359.3182 - mse: 359.3182 - mae: 15.3010 - val_loss: 238.8515 - val_mse: 238.8515 - val_mae: 12.9725\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 358.4492 - mse: 358.4492 - mae: 15.2757 - val_loss: 237.9761 - val_mse: 237.9761 - val_mae: 12.9421\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 357.5258 - mse: 357.5258 - mae: 15.2486 - val_loss: 237.1632 - val_mse: 237.1632 - val_mae: 12.9130\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 356.6750 - mse: 356.6750 - mae: 15.2268 - val_loss: 236.3814 - val_mse: 236.3814 - val_mae: 12.8839\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 355.8368 - mse: 355.8368 - mae: 15.2001 - val_loss: 235.6094 - val_mse: 235.6094 - val_mae: 12.8558\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 355.0229 - mse: 355.0229 - mae: 15.1783 - val_loss: 234.8506 - val_mse: 234.8506 - val_mae: 12.8266\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 354.2289 - mse: 354.2289 - mae: 15.1506 - val_loss: 234.1685 - val_mse: 234.1685 - val_mae: 12.8011\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 353.4267 - mse: 353.4267 - mae: 15.1328 - val_loss: 233.4848 - val_mse: 233.4848 - val_mae: 12.7742\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 352.7331 - mse: 352.7331 - mae: 15.1141 - val_loss: 232.7526 - val_mse: 232.7526 - val_mae: 12.7459\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 351.9323 - mse: 351.9323 - mae: 15.0885 - val_loss: 232.0564 - val_mse: 232.0564 - val_mae: 12.7188\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 351.1915 - mse: 351.1915 - mae: 15.0663 - val_loss: 231.3976 - val_mse: 231.3976 - val_mae: 12.6929\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 350.5175 - mse: 350.5175 - mae: 15.0442 - val_loss: 230.7556 - val_mse: 230.7556 - val_mae: 12.6684\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 349.9010 - mse: 349.9010 - mae: 15.0205 - val_loss: 230.1045 - val_mse: 230.1045 - val_mae: 12.6450\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 349.1632 - mse: 349.1632 - mae: 15.0012 - val_loss: 229.5350 - val_mse: 229.5350 - val_mae: 12.6255\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 348.4978 - mse: 348.4978 - mae: 14.9861 - val_loss: 228.9032 - val_mse: 228.9032 - val_mae: 12.6013\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 347.8807 - mse: 347.8807 - mae: 14.9667 - val_loss: 228.3053 - val_mse: 228.3053 - val_mae: 12.5782\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 347.2847 - mse: 347.2847 - mae: 14.9489 - val_loss: 227.7252 - val_mse: 227.7252 - val_mae: 12.5575\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 346.5907 - mse: 346.5907 - mae: 14.9206 - val_loss: 227.1231 - val_mse: 227.1231 - val_mae: 12.5374\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 345.9195 - mse: 345.9195 - mae: 14.9080 - val_loss: 226.3948 - val_mse: 226.3948 - val_mae: 12.5109\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 345.2083 - mse: 345.2083 - mae: 14.8801 - val_loss: 225.7135 - val_mse: 225.7135 - val_mae: 12.4860\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 344.4226 - mse: 344.4226 - mae: 14.8574 - val_loss: 225.0056 - val_mse: 225.0056 - val_mae: 12.4612\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 343.6147 - mse: 343.6147 - mae: 14.8318 - val_loss: 224.3755 - val_mse: 224.3755 - val_mae: 12.4395\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 342.8289 - mse: 342.8289 - mae: 14.8162 - val_loss: 223.6586 - val_mse: 223.6586 - val_mae: 12.4101\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 342.1358 - mse: 342.1358 - mae: 14.7835 - val_loss: 222.9523 - val_mse: 222.9523 - val_mae: 12.3807\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 341.2835 - mse: 341.2835 - mae: 14.7596 - val_loss: 222.3489 - val_mse: 222.3489 - val_mae: 12.3589\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 340.6012 - mse: 340.6012 - mae: 14.7452 - val_loss: 221.6746 - val_mse: 221.6746 - val_mae: 12.3301\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 339.9094 - mse: 339.9094 - mae: 14.7154 - val_loss: 221.0988 - val_mse: 221.0988 - val_mae: 12.3063\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 339.2868 - mse: 339.2868 - mae: 14.6955 - val_loss: 220.5797 - val_mse: 220.5797 - val_mae: 12.2843\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 338.6907 - mse: 338.6907 - mae: 14.6764 - val_loss: 220.0190 - val_mse: 220.0190 - val_mae: 12.2605\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 338.0675 - mse: 338.0675 - mae: 14.6484 - val_loss: 219.5392 - val_mse: 219.5392 - val_mae: 12.2422\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 337.5471 - mse: 337.5471 - mae: 14.6354 - val_loss: 219.1052 - val_mse: 219.1052 - val_mae: 12.2255\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.9994 - mse: 336.9994 - mae: 14.6164 - val_loss: 218.6408 - val_mse: 218.6408 - val_mae: 12.2059\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 336.4595 - mse: 336.4595 - mae: 14.5934 - val_loss: 218.2057 - val_mse: 218.2057 - val_mae: 12.1894\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 335.9603 - mse: 335.9603 - mae: 14.5810 - val_loss: 217.7966 - val_mse: 217.7966 - val_mae: 12.1737\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 335.4611 - mse: 335.4611 - mae: 14.5610 - val_loss: 217.4494 - val_mse: 217.4494 - val_mae: 12.1626\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 335.0431 - mse: 335.0431 - mae: 14.5485 - val_loss: 217.1373 - val_mse: 217.1373 - val_mae: 12.1541\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 334.5751 - mse: 334.5751 - mae: 14.5387 - val_loss: 216.7639 - val_mse: 216.7639 - val_mae: 12.1399\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 334.1147 - mse: 334.1147 - mae: 14.5260 - val_loss: 216.3246 - val_mse: 216.3246 - val_mae: 12.1202\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 333.7139 - mse: 333.7139 - mae: 14.5070 - val_loss: 215.9506 - val_mse: 215.9506 - val_mae: 12.1053\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 333.3440 - mse: 333.3440 - mae: 14.4903 - val_loss: 215.5970 - val_mse: 215.5970 - val_mae: 12.0903\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 332.8945 - mse: 332.8945 - mae: 14.4707 - val_loss: 215.3184 - val_mse: 215.3184 - val_mae: 12.0815\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 332.5110 - mse: 332.5110 - mae: 14.4612 - val_loss: 214.9996 - val_mse: 214.9996 - val_mae: 12.0689\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 332.0213 - mse: 332.0213 - mae: 14.4519 - val_loss: 214.5958 - val_mse: 214.5958 - val_mae: 12.0484\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 331.6267 - mse: 331.6267 - mae: 14.4305 - val_loss: 214.2650 - val_mse: 214.2650 - val_mae: 12.0329\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 331.2603 - mse: 331.2603 - mae: 14.4151 - val_loss: 214.0362 - val_mse: 214.0362 - val_mae: 12.0250\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 330.9204 - mse: 330.9204 - mae: 14.4115 - val_loss: 213.7411 - val_mse: 213.7411 - val_mae: 12.0114\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 330.5891 - mse: 330.5891 - mae: 14.3963 - val_loss: 213.4492 - val_mse: 213.4492 - val_mae: 11.9976\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 330.2151 - mse: 330.2151 - mae: 14.3804 - val_loss: 213.1624 - val_mse: 213.1624 - val_mae: 11.9832\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpo7crvvr7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 9975.4883 - mse: 9975.4883 - mae: 64.0659 - val_loss: 5825.7842 - val_mse: 5825.7842 - val_mae: 51.5029\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 4245.3672 - mse: 4245.3672 - mae: 44.4891 - val_loss: 2876.2134 - val_mse: 2876.2134 - val_mae: 38.6316\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 2327.2678 - mse: 2327.2678 - mae: 34.6843 - val_loss: 1708.3999 - val_mse: 1708.3999 - val_mae: 31.4134\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1498.6676 - mse: 1498.6676 - mae: 29.0160 - val_loss: 1146.6597 - val_mse: 1146.6597 - val_mae: 26.8308\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1077.0347 - mse: 1077.0347 - mae: 25.3603 - val_loss: 848.2526 - val_mse: 848.2526 - val_mae: 23.7862\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 844.5894 - mse: 844.5894 - mae: 22.9149 - val_loss: 680.9205 - val_mse: 680.9205 - val_mae: 21.7229\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 708.3424 - mse: 708.3424 - mae: 21.2208 - val_loss: 579.7343 - val_mse: 579.7343 - val_mae: 20.2262\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 622.5741 - mse: 622.5741 - mae: 20.0014 - val_loss: 516.1720 - val_mse: 516.1720 - val_mae: 19.1733\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 566.5484 - mse: 566.5484 - mae: 19.0972 - val_loss: 473.8094 - val_mse: 473.8094 - val_mae: 18.3734\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 527.1523 - mse: 527.1523 - mae: 18.4084 - val_loss: 445.1466 - val_mse: 445.1466 - val_mae: 17.7677\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 499.9342 - mse: 499.9342 - mae: 17.8964 - val_loss: 424.7828 - val_mse: 424.7828 - val_mae: 17.3074\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 479.2367 - mse: 479.2367 - mae: 17.4942 - val_loss: 409.4147 - val_mse: 409.4147 - val_mae: 16.9640\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 463.0719 - mse: 463.0719 - mae: 17.1914 - val_loss: 397.8333 - val_mse: 397.8333 - val_mae: 16.7089\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 450.4601 - mse: 450.4601 - mae: 16.9523 - val_loss: 389.0159 - val_mse: 389.0159 - val_mae: 16.4977\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 440.4220 - mse: 440.4220 - mae: 16.7662 - val_loss: 382.1372 - val_mse: 382.1372 - val_mae: 16.3226\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 432.1500 - mse: 432.1500 - mae: 16.6185 - val_loss: 376.5825 - val_mse: 376.5825 - val_mae: 16.1855\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 425.2420 - mse: 425.2420 - mae: 16.4927 - val_loss: 372.0172 - val_mse: 372.0172 - val_mae: 16.0720\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 419.2764 - mse: 419.2764 - mae: 16.3917 - val_loss: 368.3151 - val_mse: 368.3151 - val_mae: 15.9777\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 413.9512 - mse: 413.9512 - mae: 16.3037 - val_loss: 365.2665 - val_mse: 365.2665 - val_mae: 15.8954\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 409.2266 - mse: 409.2266 - mae: 16.2306 - val_loss: 362.6560 - val_mse: 362.6560 - val_mae: 15.8257\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 405.3846 - mse: 405.3846 - mae: 16.1655 - val_loss: 360.5010 - val_mse: 360.5010 - val_mae: 15.7672\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 401.5760 - mse: 401.5760 - mae: 16.1087 - val_loss: 358.5922 - val_mse: 358.5922 - val_mae: 15.7127\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 398.2925 - mse: 398.2925 - mae: 16.0542 - val_loss: 356.8257 - val_mse: 356.8257 - val_mae: 15.6622\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 395.2033 - mse: 395.2033 - mae: 16.0048 - val_loss: 355.2862 - val_mse: 355.2862 - val_mae: 15.6157\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 392.3027 - mse: 392.3027 - mae: 15.9564 - val_loss: 353.8459 - val_mse: 353.8459 - val_mae: 15.5737\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 389.6088 - mse: 389.6088 - mae: 15.9128 - val_loss: 352.4621 - val_mse: 352.4621 - val_mae: 15.5349\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 387.0451 - mse: 387.0451 - mae: 15.8755 - val_loss: 351.1144 - val_mse: 351.1144 - val_mae: 15.4984\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 384.6697 - mse: 384.6697 - mae: 15.8373 - val_loss: 349.9271 - val_mse: 349.9271 - val_mae: 15.4648\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 382.3933 - mse: 382.3933 - mae: 15.7976 - val_loss: 348.6711 - val_mse: 348.6711 - val_mae: 15.4313\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 380.3825 - mse: 380.3825 - mae: 15.7664 - val_loss: 347.4585 - val_mse: 347.4585 - val_mae: 15.4005\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 378.3540 - mse: 378.3540 - mae: 15.7344 - val_loss: 346.3639 - val_mse: 346.3639 - val_mae: 15.3724\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 376.3072 - mse: 376.3072 - mae: 15.7019 - val_loss: 345.2101 - val_mse: 345.2101 - val_mae: 15.3425\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 374.3384 - mse: 374.3384 - mae: 15.6708 - val_loss: 343.8557 - val_mse: 343.8557 - val_mae: 15.3084\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 372.4044 - mse: 372.4044 - mae: 15.6400 - val_loss: 342.4189 - val_mse: 342.4189 - val_mae: 15.2709\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 370.4359 - mse: 370.4359 - mae: 15.6054 - val_loss: 340.8261 - val_mse: 340.8261 - val_mae: 15.2304\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 368.4816 - mse: 368.4816 - mae: 15.5682 - val_loss: 339.1111 - val_mse: 339.1111 - val_mae: 15.1890\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 366.1111 - mse: 366.1111 - mae: 15.5265 - val_loss: 336.8255 - val_mse: 336.8255 - val_mae: 15.1354\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 362.9787 - mse: 362.9787 - mae: 15.4636 - val_loss: 333.8532 - val_mse: 333.8532 - val_mae: 15.0631\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 359.0427 - mse: 359.0427 - mae: 15.3766 - val_loss: 330.8517 - val_mse: 330.8517 - val_mae: 14.9833\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 355.2021 - mse: 355.2021 - mae: 15.2814 - val_loss: 328.1986 - val_mse: 328.1986 - val_mae: 14.8991\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 352.0872 - mse: 352.0872 - mae: 15.1918 - val_loss: 325.7783 - val_mse: 325.7783 - val_mae: 14.8147\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 349.4252 - mse: 349.4252 - mae: 15.1142 - val_loss: 323.7752 - val_mse: 323.7752 - val_mae: 14.7410\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 347.0885 - mse: 347.0885 - mae: 15.0464 - val_loss: 322.0678 - val_mse: 322.0678 - val_mae: 14.6736\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 345.0330 - mse: 345.0330 - mae: 14.9858 - val_loss: 320.5461 - val_mse: 320.5461 - val_mae: 14.6111\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 343.0534 - mse: 343.0534 - mae: 14.9306 - val_loss: 318.9860 - val_mse: 318.9860 - val_mae: 14.5527\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 341.2374 - mse: 341.2374 - mae: 14.8826 - val_loss: 317.5186 - val_mse: 317.5186 - val_mae: 14.5069\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 339.5648 - mse: 339.5648 - mae: 14.8438 - val_loss: 316.1245 - val_mse: 316.1245 - val_mae: 14.4660\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 337.9067 - mse: 337.9067 - mae: 14.8020 - val_loss: 314.7828 - val_mse: 314.7828 - val_mae: 14.4269\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 336.3464 - mse: 336.3464 - mae: 14.7645 - val_loss: 313.5088 - val_mse: 313.5088 - val_mae: 14.3907\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 334.7398 - mse: 334.7398 - mae: 14.7251 - val_loss: 312.2679 - val_mse: 312.2679 - val_mae: 14.3558\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 333.2133 - mse: 333.2133 - mae: 14.6900 - val_loss: 311.0680 - val_mse: 311.0680 - val_mae: 14.3221\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 331.8080 - mse: 331.8080 - mae: 14.6576 - val_loss: 309.8715 - val_mse: 309.8715 - val_mae: 14.2887\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 330.2408 - mse: 330.2408 - mae: 14.6197 - val_loss: 308.7272 - val_mse: 308.7272 - val_mae: 14.2587\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 328.5208 - mse: 328.5208 - mae: 14.5836 - val_loss: 307.4006 - val_mse: 307.4006 - val_mae: 14.2255\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 326.7179 - mse: 326.7179 - mae: 14.5408 - val_loss: 305.9969 - val_mse: 305.9969 - val_mae: 14.1883\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 324.9167 - mse: 324.9167 - mae: 14.4977 - val_loss: 304.6619 - val_mse: 304.6619 - val_mae: 14.1505\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 323.1237 - mse: 323.1237 - mae: 14.4496 - val_loss: 303.4129 - val_mse: 303.4129 - val_mae: 14.1119\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 321.4661 - mse: 321.4661 - mae: 14.4040 - val_loss: 302.2895 - val_mse: 302.2895 - val_mae: 14.0777\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 319.9360 - mse: 319.9360 - mae: 14.3588 - val_loss: 301.2468 - val_mse: 301.2468 - val_mae: 14.0447\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 318.4860 - mse: 318.4860 - mae: 14.3146 - val_loss: 300.3033 - val_mse: 300.3033 - val_mae: 14.0124\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 317.1313 - mse: 317.1313 - mae: 14.2742 - val_loss: 299.3476 - val_mse: 299.3476 - val_mae: 13.9772\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 315.7067 - mse: 315.7067 - mae: 14.2270 - val_loss: 298.4573 - val_mse: 298.4573 - val_mae: 13.9432\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 314.3481 - mse: 314.3481 - mae: 14.1848 - val_loss: 297.5495 - val_mse: 297.5495 - val_mae: 13.9106\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 313.0747 - mse: 313.0747 - mae: 14.1477 - val_loss: 296.6363 - val_mse: 296.6363 - val_mae: 13.8769\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 311.7893 - mse: 311.7893 - mae: 14.1067 - val_loss: 295.7398 - val_mse: 295.7398 - val_mae: 13.8435\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 310.6141 - mse: 310.6141 - mae: 14.0706 - val_loss: 294.8159 - val_mse: 294.8159 - val_mae: 13.8114\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 309.5172 - mse: 309.5172 - mae: 14.0345 - val_loss: 293.9463 - val_mse: 293.9463 - val_mae: 13.7789\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 308.4033 - mse: 308.4033 - mae: 13.9995 - val_loss: 292.9824 - val_mse: 292.9824 - val_mae: 13.7431\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 307.3120 - mse: 307.3120 - mae: 13.9643 - val_loss: 292.0999 - val_mse: 292.0999 - val_mae: 13.7089\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 306.1724 - mse: 306.1724 - mae: 13.9290 - val_loss: 291.2486 - val_mse: 291.2486 - val_mae: 13.6780\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 305.0757 - mse: 305.0757 - mae: 13.8922 - val_loss: 290.3771 - val_mse: 290.3771 - val_mae: 13.6485\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 304.0110 - mse: 304.0110 - mae: 13.8576 - val_loss: 289.4955 - val_mse: 289.4955 - val_mae: 13.6189\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 303.0838 - mse: 303.0838 - mae: 13.8252 - val_loss: 288.6978 - val_mse: 288.6978 - val_mae: 13.5917\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 302.1281 - mse: 302.1281 - mae: 13.7964 - val_loss: 287.8758 - val_mse: 287.8758 - val_mae: 13.5647\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 301.2727 - mse: 301.2727 - mae: 13.7697 - val_loss: 287.1122 - val_mse: 287.1122 - val_mae: 13.5395\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 300.4796 - mse: 300.4796 - mae: 13.7414 - val_loss: 286.4458 - val_mse: 286.4458 - val_mae: 13.5168\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 299.7133 - mse: 299.7133 - mae: 13.7184 - val_loss: 285.7941 - val_mse: 285.7941 - val_mae: 13.4934\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 299.0074 - mse: 299.0074 - mae: 13.6966 - val_loss: 285.1993 - val_mse: 285.1993 - val_mae: 13.4719\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 298.3176 - mse: 298.3176 - mae: 13.6752 - val_loss: 284.6024 - val_mse: 284.6024 - val_mae: 13.4488\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 297.6397 - mse: 297.6397 - mae: 13.6526 - val_loss: 283.9904 - val_mse: 283.9904 - val_mae: 13.4248\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 297.0380 - mse: 297.0380 - mae: 13.6342 - val_loss: 283.4284 - val_mse: 283.4284 - val_mae: 13.4023\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 296.4110 - mse: 296.4110 - mae: 13.6114 - val_loss: 282.9099 - val_mse: 282.9099 - val_mae: 13.3827\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.8000 - mse: 295.8000 - mae: 13.5904 - val_loss: 282.3763 - val_mse: 282.3763 - val_mae: 13.3635\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.2013 - mse: 295.2013 - mae: 13.5720 - val_loss: 281.8369 - val_mse: 281.8369 - val_mae: 13.3435\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.6659 - mse: 294.6659 - mae: 13.5539 - val_loss: 281.2997 - val_mse: 281.2997 - val_mae: 13.3223\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.1355 - mse: 294.1355 - mae: 13.5369 - val_loss: 280.7542 - val_mse: 280.7542 - val_mae: 13.3014\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.5851 - mse: 293.5851 - mae: 13.5188 - val_loss: 280.3003 - val_mse: 280.3003 - val_mae: 13.2831\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.0130 - mse: 293.0130 - mae: 13.4988 - val_loss: 279.8220 - val_mse: 279.8220 - val_mae: 13.2662\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.4969 - mse: 292.4969 - mae: 13.4835 - val_loss: 279.3701 - val_mse: 279.3701 - val_mae: 13.2498\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.0478 - mse: 292.0478 - mae: 13.4703 - val_loss: 278.9544 - val_mse: 278.9544 - val_mae: 13.2355\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.4975 - mse: 291.4975 - mae: 13.4536 - val_loss: 278.5397 - val_mse: 278.5397 - val_mae: 13.2210\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.0313 - mse: 291.0313 - mae: 13.4395 - val_loss: 278.1267 - val_mse: 278.1267 - val_mae: 13.2077\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.5642 - mse: 290.5642 - mae: 13.4281 - val_loss: 277.7244 - val_mse: 277.7244 - val_mae: 13.1929\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.1471 - mse: 290.1471 - mae: 13.4130 - val_loss: 277.2879 - val_mse: 277.2879 - val_mae: 13.1766\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.6942 - mse: 289.6942 - mae: 13.4006 - val_loss: 276.8842 - val_mse: 276.8842 - val_mae: 13.1627\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.2318 - mse: 289.2318 - mae: 13.3847 - val_loss: 276.5352 - val_mse: 276.5352 - val_mae: 13.1493\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.7910 - mse: 288.7910 - mae: 13.3717 - val_loss: 276.1598 - val_mse: 276.1598 - val_mae: 13.1358\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.3808 - mse: 288.3808 - mae: 13.3570 - val_loss: 275.7828 - val_mse: 275.7828 - val_mae: 13.1225\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.9554 - mse: 287.9554 - mae: 13.3436 - val_loss: 275.4297 - val_mse: 275.4297 - val_mae: 13.1095\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.5613 - mse: 287.5613 - mae: 13.3315 - val_loss: 275.0700 - val_mse: 275.0700 - val_mae: 13.0957\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpm8xup4dj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 24492.2461 - mse: 24492.2461 - mae: 74.6546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpogezg65z/assets\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpykevjme7/assets\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpjgx8_ul7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 983.9561 - mse: 983.9561 - mae: 23.8831 - val_loss: 945.1413 - val_mse: 945.1413 - val_mae: 22.8375\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 975.5127 - mse: 975.5127 - mae: 23.7752 - val_loss: 936.1058 - val_mse: 936.1058 - val_mae: 22.7221\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 967.0176 - mse: 967.0176 - mae: 23.6674 - val_loss: 927.2361 - val_mse: 927.2361 - val_mae: 22.6080\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 958.7381 - mse: 958.7381 - mae: 23.5607 - val_loss: 918.4093 - val_mse: 918.4093 - val_mae: 22.4944\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 950.6455 - mse: 950.6455 - mae: 23.4559 - val_loss: 909.7343 - val_mse: 909.7343 - val_mae: 22.3826\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 942.4321 - mse: 942.4321 - mae: 23.3510 - val_loss: 901.2758 - val_mse: 901.2758 - val_mae: 22.2736\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 934.5555 - mse: 934.5555 - mae: 23.2491 - val_loss: 892.8070 - val_mse: 892.8070 - val_mae: 22.1644\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 926.6842 - mse: 926.6842 - mae: 23.1474 - val_loss: 884.3995 - val_mse: 884.3995 - val_mae: 22.0561\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 918.8649 - mse: 918.8649 - mae: 23.0466 - val_loss: 876.1824 - val_mse: 876.1824 - val_mae: 21.9505\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 911.1924 - mse: 911.1924 - mae: 22.9471 - val_loss: 868.1500 - val_mse: 868.1500 - val_mae: 21.8463\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 903.7960 - mse: 903.7960 - mae: 22.8497 - val_loss: 860.0878 - val_mse: 860.0878 - val_mae: 21.7416\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 896.3123 - mse: 896.3123 - mae: 22.7525 - val_loss: 852.2804 - val_mse: 852.2804 - val_mae: 21.6397\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 888.9909 - mse: 888.9909 - mae: 22.6576 - val_loss: 844.5499 - val_mse: 844.5499 - val_mae: 21.5387\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 881.9260 - mse: 881.9260 - mae: 22.5626 - val_loss: 836.8553 - val_mse: 836.8553 - val_mae: 21.4385\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 874.6566 - mse: 874.6566 - mae: 22.4684 - val_loss: 829.4002 - val_mse: 829.4002 - val_mae: 21.3405\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 867.6555 - mse: 867.6555 - mae: 22.3759 - val_loss: 821.9783 - val_mse: 821.9783 - val_mae: 21.2428\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 860.8306 - mse: 860.8305 - mae: 22.2850 - val_loss: 814.5130 - val_mse: 814.5130 - val_mae: 21.1458\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 853.9755 - mse: 853.9755 - mae: 22.1945 - val_loss: 807.2049 - val_mse: 807.2049 - val_mae: 21.0503\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 847.1646 - mse: 847.1646 - mae: 22.1049 - val_loss: 800.1119 - val_mse: 800.1119 - val_mae: 20.9575\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 840.5441 - mse: 840.5441 - mae: 22.0176 - val_loss: 793.0804 - val_mse: 793.0804 - val_mae: 20.8652\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 833.9973 - mse: 833.9973 - mae: 21.9301 - val_loss: 786.1828 - val_mse: 786.1828 - val_mae: 20.7747\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 827.5706 - mse: 827.5706 - mae: 21.8452 - val_loss: 779.3604 - val_mse: 779.3604 - val_mae: 20.6849\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 821.2852 - mse: 821.2852 - mae: 21.7603 - val_loss: 772.5301 - val_mse: 772.5301 - val_mae: 20.5953\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 814.9346 - mse: 814.9346 - mae: 21.6754 - val_loss: 765.8538 - val_mse: 765.8538 - val_mae: 20.5075\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 808.7269 - mse: 808.7269 - mae: 21.5930 - val_loss: 759.3190 - val_mse: 759.3190 - val_mae: 20.4216\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 802.5881 - mse: 802.5881 - mae: 21.5109 - val_loss: 752.8756 - val_mse: 752.8756 - val_mae: 20.3371\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 796.5762 - mse: 796.5762 - mae: 21.4292 - val_loss: 746.4403 - val_mse: 746.4403 - val_mae: 20.2529\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 790.6075 - mse: 790.6075 - mae: 21.3488 - val_loss: 740.1298 - val_mse: 740.1298 - val_mae: 20.1695\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 784.7725 - mse: 784.7725 - mae: 21.2692 - val_loss: 733.8670 - val_mse: 733.8670 - val_mae: 20.0863\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 778.8123 - mse: 778.8123 - mae: 21.1902 - val_loss: 727.7457 - val_mse: 727.7457 - val_mae: 20.0048\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 773.1010 - mse: 773.1010 - mae: 21.1111 - val_loss: 721.6003 - val_mse: 721.6003 - val_mae: 19.9232\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 767.3591 - mse: 767.3591 - mae: 21.0325 - val_loss: 715.5260 - val_mse: 715.5260 - val_mae: 19.8417\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 761.7065 - mse: 761.7065 - mae: 20.9538 - val_loss: 709.5247 - val_mse: 709.5247 - val_mae: 19.7605\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 755.9944 - mse: 755.9944 - mae: 20.8755 - val_loss: 703.6425 - val_mse: 703.6425 - val_mae: 19.6798\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 750.4944 - mse: 750.4944 - mae: 20.7985 - val_loss: 697.7527 - val_mse: 697.7527 - val_mae: 19.5985\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 744.8688 - mse: 744.8688 - mae: 20.7207 - val_loss: 692.0605 - val_mse: 692.0605 - val_mae: 19.5188\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 739.5123 - mse: 739.5123 - mae: 20.6441 - val_loss: 686.2117 - val_mse: 686.2117 - val_mae: 19.4376\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 734.0192 - mse: 734.0192 - mae: 20.5679 - val_loss: 680.4559 - val_mse: 680.4559 - val_mae: 19.3574\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 728.6979 - mse: 728.6979 - mae: 20.4918 - val_loss: 674.7888 - val_mse: 674.7888 - val_mae: 19.2778\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 723.3723 - mse: 723.3723 - mae: 20.4168 - val_loss: 669.2179 - val_mse: 669.2179 - val_mae: 19.1991\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 718.1254 - mse: 718.1254 - mae: 20.3418 - val_loss: 663.7778 - val_mse: 663.7778 - val_mae: 19.1214\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 712.9529 - mse: 712.9529 - mae: 20.2677 - val_loss: 658.4168 - val_mse: 658.4168 - val_mae: 19.0449\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 707.8392 - mse: 707.8392 - mae: 20.1951 - val_loss: 653.1137 - val_mse: 653.1137 - val_mae: 18.9685\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 702.9018 - mse: 702.9018 - mae: 20.1230 - val_loss: 647.7277 - val_mse: 647.7277 - val_mae: 18.8912\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 697.8068 - mse: 697.8068 - mae: 20.0500 - val_loss: 642.5220 - val_mse: 642.5220 - val_mae: 18.8148\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 692.9276 - mse: 692.9276 - mae: 19.9780 - val_loss: 637.3405 - val_mse: 637.3405 - val_mae: 18.7380\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 688.0504 - mse: 688.0504 - mae: 19.9071 - val_loss: 632.2138 - val_mse: 632.2138 - val_mae: 18.6615\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 683.1994 - mse: 683.1994 - mae: 19.8358 - val_loss: 627.1083 - val_mse: 627.1083 - val_mae: 18.5850\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 678.4294 - mse: 678.4294 - mae: 19.7652 - val_loss: 622.0628 - val_mse: 622.0628 - val_mae: 18.5091\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 673.7009 - mse: 673.7009 - mae: 19.6944 - val_loss: 617.0555 - val_mse: 617.0555 - val_mae: 18.4336\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 668.9604 - mse: 668.9604 - mae: 19.6242 - val_loss: 612.1752 - val_mse: 612.1752 - val_mae: 18.3598\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 664.3250 - mse: 664.3250 - mae: 19.5544 - val_loss: 607.3008 - val_mse: 607.3008 - val_mae: 18.2859\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 659.7088 - mse: 659.7088 - mae: 19.4853 - val_loss: 602.5145 - val_mse: 602.5145 - val_mae: 18.2136\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 655.1703 - mse: 655.1703 - mae: 19.4176 - val_loss: 597.7913 - val_mse: 597.7913 - val_mae: 18.1429\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 650.7389 - mse: 650.7389 - mae: 19.3502 - val_loss: 592.9794 - val_mse: 592.9794 - val_mae: 18.0709\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 646.2368 - mse: 646.2368 - mae: 19.2835 - val_loss: 588.3235 - val_mse: 588.3235 - val_mae: 18.0010\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 641.9382 - mse: 641.9382 - mae: 19.2180 - val_loss: 583.7145 - val_mse: 583.7145 - val_mae: 17.9314\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 637.6960 - mse: 637.6960 - mae: 19.1539 - val_loss: 579.1012 - val_mse: 579.1012 - val_mae: 17.8626\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 633.3782 - mse: 633.3782 - mae: 19.0903 - val_loss: 574.7393 - val_mse: 574.7393 - val_mae: 17.7967\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 629.3129 - mse: 629.3129 - mae: 19.0279 - val_loss: 570.3557 - val_mse: 570.3557 - val_mae: 17.7312\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 625.1689 - mse: 625.1689 - mae: 18.9667 - val_loss: 566.0618 - val_mse: 566.0618 - val_mae: 17.6676\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 621.1252 - mse: 621.1252 - mae: 18.9064 - val_loss: 561.8862 - val_mse: 561.8862 - val_mae: 17.6055\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 617.1853 - mse: 617.1853 - mae: 18.8471 - val_loss: 557.7086 - val_mse: 557.7086 - val_mae: 17.5433\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 613.2546 - mse: 613.2546 - mae: 18.7889 - val_loss: 553.5667 - val_mse: 553.5667 - val_mae: 17.4821\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 609.3716 - mse: 609.3716 - mae: 18.7310 - val_loss: 549.5213 - val_mse: 549.5213 - val_mae: 17.4226\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 605.6078 - mse: 605.6078 - mae: 18.6745 - val_loss: 545.4256 - val_mse: 545.4256 - val_mae: 17.3629\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 601.8126 - mse: 601.8126 - mae: 18.6180 - val_loss: 541.4282 - val_mse: 541.4282 - val_mae: 17.3043\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.1049 - mse: 598.1049 - mae: 18.5627 - val_loss: 537.4933 - val_mse: 537.4933 - val_mae: 17.2473\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 594.4548 - mse: 594.4548 - mae: 18.5092 - val_loss: 533.7309 - val_mse: 533.7309 - val_mae: 17.1925\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 590.8671 - mse: 590.8671 - mae: 18.4557 - val_loss: 529.9488 - val_mse: 529.9488 - val_mae: 17.1377\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 587.3261 - mse: 587.3261 - mae: 18.4036 - val_loss: 526.2615 - val_mse: 526.2615 - val_mae: 17.0838\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 583.9229 - mse: 583.9229 - mae: 18.3533 - val_loss: 522.4744 - val_mse: 522.4744 - val_mae: 17.0287\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 580.4436 - mse: 580.4436 - mae: 18.3027 - val_loss: 518.9103 - val_mse: 518.9103 - val_mae: 16.9763\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 577.1233 - mse: 577.1233 - mae: 18.2537 - val_loss: 515.2883 - val_mse: 515.2883 - val_mae: 16.9236\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 573.8159 - mse: 573.8159 - mae: 18.2053 - val_loss: 511.7545 - val_mse: 511.7545 - val_mae: 16.8725\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 570.5223 - mse: 570.5223 - mae: 18.1579 - val_loss: 508.2811 - val_mse: 508.2811 - val_mae: 16.8222\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 567.2983 - mse: 567.2983 - mae: 18.1110 - val_loss: 504.8302 - val_mse: 504.8302 - val_mae: 16.7725\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 564.1005 - mse: 564.1005 - mae: 18.0649 - val_loss: 501.4984 - val_mse: 501.4984 - val_mae: 16.7245\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 560.9886 - mse: 560.9886 - mae: 18.0198 - val_loss: 498.1479 - val_mse: 498.1479 - val_mae: 16.6768\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 557.8779 - mse: 557.8779 - mae: 17.9746 - val_loss: 494.8683 - val_mse: 494.8683 - val_mae: 16.6300\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 554.8419 - mse: 554.8419 - mae: 17.9313 - val_loss: 491.6287 - val_mse: 491.6287 - val_mae: 16.5841\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 551.8199 - mse: 551.8199 - mae: 17.8875 - val_loss: 488.3263 - val_mse: 488.3263 - val_mae: 16.5372\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 548.8351 - mse: 548.8351 - mae: 17.8450 - val_loss: 485.1494 - val_mse: 485.1494 - val_mae: 16.4923\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 545.9042 - mse: 545.9042 - mae: 17.8025 - val_loss: 482.0214 - val_mse: 482.0214 - val_mae: 16.4479\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 543.0242 - mse: 543.0242 - mae: 17.7608 - val_loss: 478.9467 - val_mse: 478.9467 - val_mae: 16.4043\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 540.1653 - mse: 540.1653 - mae: 17.7200 - val_loss: 475.9248 - val_mse: 475.9248 - val_mae: 16.3615\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 537.3599 - mse: 537.3599 - mae: 17.6801 - val_loss: 472.9669 - val_mse: 472.9669 - val_mae: 16.3196\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 534.6475 - mse: 534.6475 - mae: 17.6407 - val_loss: 469.9362 - val_mse: 469.9362 - val_mae: 16.2767\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 531.9038 - mse: 531.9038 - mae: 17.6009 - val_loss: 466.9424 - val_mse: 466.9424 - val_mae: 16.2342\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 529.1254 - mse: 529.1254 - mae: 17.5618 - val_loss: 464.0937 - val_mse: 464.0937 - val_mae: 16.1939\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 526.4792 - mse: 526.4792 - mae: 17.5237 - val_loss: 461.2106 - val_mse: 461.2106 - val_mae: 16.1531\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 523.7885 - mse: 523.7885 - mae: 17.4856 - val_loss: 458.3755 - val_mse: 458.3755 - val_mae: 16.1127\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 521.1755 - mse: 521.1755 - mae: 17.4482 - val_loss: 455.5204 - val_mse: 455.5204 - val_mae: 16.0715\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 518.5897 - mse: 518.5897 - mae: 17.4109 - val_loss: 452.7025 - val_mse: 452.7025 - val_mae: 16.0307\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 515.9993 - mse: 515.9993 - mae: 17.3739 - val_loss: 449.9794 - val_mse: 449.9794 - val_mae: 15.9918\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 513.4741 - mse: 513.4741 - mae: 17.3375 - val_loss: 447.2368 - val_mse: 447.2368 - val_mae: 15.9523\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 510.9862 - mse: 510.9862 - mae: 17.3016 - val_loss: 444.4911 - val_mse: 444.4911 - val_mae: 15.9129\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 508.4604 - mse: 508.4604 - mae: 17.2659 - val_loss: 441.8875 - val_mse: 441.8875 - val_mae: 15.8753\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 506.0215 - mse: 506.0215 - mae: 17.2309 - val_loss: 439.1966 - val_mse: 439.1966 - val_mae: 15.8365\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 503.6057 - mse: 503.6057 - mae: 17.1961 - val_loss: 436.5290 - val_mse: 436.5290 - val_mae: 15.7975\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpvoi9g5ix/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 1215.8516 - mse: 1215.8516 - mae: 26.4845 - val_loss: 1169.7723 - val_mse: 1169.7723 - val_mae: 25.3474\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1203.9308 - mse: 1203.9308 - mae: 26.3473 - val_loss: 1157.2701 - val_mse: 1157.2701 - val_mae: 25.2028\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1192.2729 - mse: 1192.2729 - mae: 26.2127 - val_loss: 1144.8700 - val_mse: 1144.8700 - val_mae: 25.0591\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1180.7284 - mse: 1180.7284 - mae: 26.0788 - val_loss: 1132.6907 - val_mse: 1132.6907 - val_mae: 24.9169\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1169.3040 - mse: 1169.3040 - mae: 25.9473 - val_loss: 1120.7485 - val_mse: 1120.7485 - val_mae: 24.7774\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1158.0184 - mse: 1158.0184 - mae: 25.8166 - val_loss: 1109.0143 - val_mse: 1109.0143 - val_mae: 24.6402\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1147.0074 - mse: 1147.0074 - mae: 25.6890 - val_loss: 1097.3512 - val_mse: 1097.3512 - val_mae: 24.5035\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1136.1746 - mse: 1136.1746 - mae: 25.5637 - val_loss: 1085.7701 - val_mse: 1085.7701 - val_mae: 24.3684\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1125.2460 - mse: 1125.2460 - mae: 25.4380 - val_loss: 1074.5208 - val_mse: 1074.5208 - val_mae: 24.2367\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1114.6991 - mse: 1114.6991 - mae: 25.3144 - val_loss: 1063.3805 - val_mse: 1063.3805 - val_mae: 24.1062\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1104.2136 - mse: 1104.2136 - mae: 25.1921 - val_loss: 1052.3815 - val_mse: 1052.3815 - val_mae: 23.9769\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1093.8420 - mse: 1093.8420 - mae: 25.0712 - val_loss: 1041.5790 - val_mse: 1041.5790 - val_mae: 23.8488\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1083.6359 - mse: 1083.6359 - mae: 24.9512 - val_loss: 1030.8937 - val_mse: 1030.8937 - val_mae: 23.7214\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1073.6732 - mse: 1073.6732 - mae: 24.8319 - val_loss: 1020.2701 - val_mse: 1020.2701 - val_mae: 23.5950\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1063.6934 - mse: 1063.6934 - mae: 24.7148 - val_loss: 1009.8452 - val_mse: 1009.8452 - val_mae: 23.4702\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1053.7856 - mse: 1053.7856 - mae: 24.5977 - val_loss: 999.6929 - val_mse: 999.6929 - val_mae: 23.3485\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1044.2526 - mse: 1044.2526 - mae: 24.4830 - val_loss: 989.5129 - val_mse: 989.5129 - val_mae: 23.2259\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1034.7225 - mse: 1034.7225 - mae: 24.3686 - val_loss: 979.4630 - val_mse: 979.4630 - val_mae: 23.1044\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1025.2606 - mse: 1025.2606 - mae: 24.2553 - val_loss: 969.5585 - val_mse: 969.5585 - val_mae: 22.9837\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1015.8914 - mse: 1015.8914 - mae: 24.1426 - val_loss: 959.7985 - val_mse: 959.7985 - val_mae: 22.8648\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1006.6859 - mse: 1006.6859 - mae: 24.0305 - val_loss: 950.0914 - val_mse: 950.0914 - val_mae: 22.7462\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 997.5854 - mse: 997.5854 - mae: 23.9197 - val_loss: 940.4025 - val_mse: 940.4025 - val_mae: 22.6267\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 988.4659 - mse: 988.4659 - mae: 23.8081 - val_loss: 930.8355 - val_mse: 930.8355 - val_mae: 22.5075\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 979.4866 - mse: 979.4866 - mae: 23.6965 - val_loss: 921.4261 - val_mse: 921.4261 - val_mae: 22.3896\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 970.6820 - mse: 970.6820 - mae: 23.5875 - val_loss: 912.1534 - val_mse: 912.1533 - val_mae: 22.2722\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 961.9543 - mse: 961.9543 - mae: 23.4774 - val_loss: 902.9406 - val_mse: 902.9406 - val_mae: 22.1550\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 953.2272 - mse: 953.2272 - mae: 23.3681 - val_loss: 893.9121 - val_mse: 893.9121 - val_mae: 22.0393\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 944.7644 - mse: 944.7644 - mae: 23.2595 - val_loss: 884.8413 - val_mse: 884.8413 - val_mae: 21.9233\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 936.2543 - mse: 936.2543 - mae: 23.1520 - val_loss: 875.9591 - val_mse: 875.9591 - val_mae: 21.8079\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 927.9368 - mse: 927.9368 - mae: 23.0451 - val_loss: 867.1954 - val_mse: 867.1954 - val_mae: 21.6933\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 919.6391 - mse: 919.6391 - mae: 22.9388 - val_loss: 858.6823 - val_mse: 858.6823 - val_mae: 21.5815\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 911.6202 - mse: 911.6202 - mae: 22.8351 - val_loss: 850.2285 - val_mse: 850.2285 - val_mae: 21.4703\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 903.6776 - mse: 903.6776 - mae: 22.7329 - val_loss: 841.9675 - val_mse: 841.9675 - val_mae: 21.3619\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 895.8327 - mse: 895.8327 - mae: 22.6318 - val_loss: 833.8386 - val_mse: 833.8386 - val_mae: 21.2550\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 888.2132 - mse: 888.2132 - mae: 22.5319 - val_loss: 825.7200 - val_mse: 825.7200 - val_mae: 21.1495\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 880.4934 - mse: 880.4934 - mae: 22.4336 - val_loss: 817.8909 - val_mse: 817.8909 - val_mae: 21.0467\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 873.0845 - mse: 873.0845 - mae: 22.3364 - val_loss: 810.0009 - val_mse: 810.0009 - val_mae: 20.9434\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 865.6640 - mse: 865.6640 - mae: 22.2404 - val_loss: 802.2366 - val_mse: 802.2366 - val_mae: 20.8415\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 858.4021 - mse: 858.4021 - mae: 22.1456 - val_loss: 794.5708 - val_mse: 794.5708 - val_mae: 20.7409\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 851.2022 - mse: 851.2022 - mae: 22.0519 - val_loss: 786.9754 - val_mse: 786.9754 - val_mae: 20.6406\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 844.0081 - mse: 844.0081 - mae: 21.9588 - val_loss: 779.6036 - val_mse: 779.6036 - val_mae: 20.5434\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 837.0662 - mse: 837.0662 - mae: 21.8673 - val_loss: 772.2579 - val_mse: 772.2579 - val_mae: 20.4464\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 830.1620 - mse: 830.1620 - mae: 21.7768 - val_loss: 764.9751 - val_mse: 764.9751 - val_mae: 20.3492\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 823.3932 - mse: 823.3932 - mae: 21.6865 - val_loss: 757.7493 - val_mse: 757.7493 - val_mae: 20.2534\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 816.5388 - mse: 816.5388 - mae: 21.5977 - val_loss: 750.7935 - val_mse: 750.7935 - val_mae: 20.1607\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 810.0092 - mse: 810.0092 - mae: 21.5091 - val_loss: 743.7563 - val_mse: 743.7563 - val_mae: 20.0660\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 803.4031 - mse: 803.4031 - mae: 21.4215 - val_loss: 736.8368 - val_mse: 736.8368 - val_mae: 19.9717\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 796.9089 - mse: 796.9089 - mae: 21.3341 - val_loss: 730.0042 - val_mse: 730.0042 - val_mae: 19.8785\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 790.5667 - mse: 790.5667 - mae: 21.2486 - val_loss: 723.2538 - val_mse: 723.2538 - val_mae: 19.7859\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 784.1915 - mse: 784.1915 - mae: 21.1631 - val_loss: 716.6871 - val_mse: 716.6871 - val_mae: 19.6961\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 777.9590 - mse: 777.9590 - mae: 21.0799 - val_loss: 710.1481 - val_mse: 710.1481 - val_mae: 19.6064\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 771.8353 - mse: 771.8353 - mae: 20.9974 - val_loss: 703.6561 - val_mse: 703.6561 - val_mae: 19.5170\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 765.7509 - mse: 765.7509 - mae: 20.9153 - val_loss: 697.2811 - val_mse: 697.2811 - val_mae: 19.4288\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 759.7261 - mse: 759.7261 - mae: 20.8346 - val_loss: 691.0461 - val_mse: 691.0461 - val_mae: 19.3424\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 753.8762 - mse: 753.8762 - mae: 20.7547 - val_loss: 684.7919 - val_mse: 684.7919 - val_mae: 19.2551\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 748.0943 - mse: 748.0943 - mae: 20.6753 - val_loss: 678.6366 - val_mse: 678.6366 - val_mae: 19.1694\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 742.3317 - mse: 742.3317 - mae: 20.5967 - val_loss: 672.6052 - val_mse: 672.6052 - val_mae: 19.0844\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 736.6854 - mse: 736.6854 - mae: 20.5198 - val_loss: 666.6152 - val_mse: 666.6152 - val_mae: 18.9995\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 731.0571 - mse: 731.0571 - mae: 20.4436 - val_loss: 660.7038 - val_mse: 660.7038 - val_mae: 18.9150\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 725.5585 - mse: 725.5585 - mae: 20.3681 - val_loss: 654.8813 - val_mse: 654.8813 - val_mae: 18.8320\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 720.0632 - mse: 720.0632 - mae: 20.2939 - val_loss: 649.1788 - val_mse: 649.1788 - val_mae: 18.7513\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 714.7045 - mse: 714.7045 - mae: 20.2204 - val_loss: 643.5121 - val_mse: 643.5121 - val_mae: 18.6715\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 709.3436 - mse: 709.3436 - mae: 20.1475 - val_loss: 637.8995 - val_mse: 637.8995 - val_mae: 18.5926\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 704.0598 - mse: 704.0598 - mae: 20.0755 - val_loss: 632.3454 - val_mse: 632.3454 - val_mae: 18.5145\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 698.8869 - mse: 698.8869 - mae: 20.0035 - val_loss: 626.7700 - val_mse: 626.7700 - val_mae: 18.4357\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 693.7339 - mse: 693.7339 - mae: 19.9317 - val_loss: 621.2805 - val_mse: 621.2805 - val_mae: 18.3581\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 688.5412 - mse: 688.5412 - mae: 19.8607 - val_loss: 616.0101 - val_mse: 616.0101 - val_mae: 18.2828\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 683.5869 - mse: 683.5869 - mae: 19.7921 - val_loss: 610.6918 - val_mse: 610.6918 - val_mae: 18.2072\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 678.6008 - mse: 678.6008 - mae: 19.7237 - val_loss: 605.5098 - val_mse: 605.5098 - val_mae: 18.1331\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 673.7043 - mse: 673.7043 - mae: 19.6559 - val_loss: 600.3950 - val_mse: 600.3950 - val_mae: 18.0600\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 668.8749 - mse: 668.8749 - mae: 19.5886 - val_loss: 595.3800 - val_mse: 595.3800 - val_mae: 17.9887\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 664.1278 - mse: 664.1278 - mae: 19.5226 - val_loss: 590.3401 - val_mse: 590.3401 - val_mae: 17.9160\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 659.3745 - mse: 659.3745 - mae: 19.4566 - val_loss: 585.4630 - val_mse: 585.4630 - val_mae: 17.8457\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 654.7346 - mse: 654.7346 - mae: 19.3915 - val_loss: 580.6285 - val_mse: 580.6285 - val_mae: 17.7762\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 650.2069 - mse: 650.2069 - mae: 19.3287 - val_loss: 575.8276 - val_mse: 575.8276 - val_mae: 17.7076\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 645.7090 - mse: 645.7090 - mae: 19.2658 - val_loss: 571.0621 - val_mse: 571.0621 - val_mae: 17.6393\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 641.2653 - mse: 641.2653 - mae: 19.2032 - val_loss: 566.3934 - val_mse: 566.3934 - val_mae: 17.5724\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 636.8188 - mse: 636.8188 - mae: 19.1422 - val_loss: 561.9205 - val_mse: 561.9205 - val_mae: 17.5079\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 632.5659 - mse: 632.5659 - mae: 19.0816 - val_loss: 557.2870 - val_mse: 557.2870 - val_mae: 17.4406\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 628.2515 - mse: 628.2515 - mae: 19.0208 - val_loss: 552.7831 - val_mse: 552.7831 - val_mae: 17.3752\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 624.0120 - mse: 624.0120 - mae: 18.9620 - val_loss: 548.3753 - val_mse: 548.3753 - val_mae: 17.3119\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 619.8505 - mse: 619.8505 - mae: 18.9027 - val_loss: 543.9806 - val_mse: 543.9806 - val_mae: 17.2481\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 615.7503 - mse: 615.7503 - mae: 18.8455 - val_loss: 539.6602 - val_mse: 539.6602 - val_mae: 17.1856\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 611.7026 - mse: 611.7026 - mae: 18.7881 - val_loss: 535.4837 - val_mse: 535.4837 - val_mae: 17.1263\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 607.7778 - mse: 607.7778 - mae: 18.7327 - val_loss: 531.2947 - val_mse: 531.2947 - val_mae: 17.0667\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 603.8926 - mse: 603.8926 - mae: 18.6782 - val_loss: 527.1592 - val_mse: 527.1592 - val_mae: 17.0080\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 600.0052 - mse: 600.0052 - mae: 18.6237 - val_loss: 523.0817 - val_mse: 523.0817 - val_mae: 16.9501\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 596.1857 - mse: 596.1857 - mae: 18.5709 - val_loss: 519.0620 - val_mse: 519.0620 - val_mae: 16.8925\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 592.4146 - mse: 592.4146 - mae: 18.5180 - val_loss: 515.1143 - val_mse: 515.1143 - val_mae: 16.8377\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 588.6151 - mse: 588.6151 - mae: 18.4654 - val_loss: 511.1993 - val_mse: 511.1993 - val_mae: 16.7828\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 584.8889 - mse: 584.8889 - mae: 18.4128 - val_loss: 507.1807 - val_mse: 507.1807 - val_mae: 16.7255\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 581.0441 - mse: 581.0441 - mae: 18.3587 - val_loss: 503.2904 - val_mse: 503.2904 - val_mae: 16.6694\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 577.3535 - mse: 577.3535 - mae: 18.3054 - val_loss: 499.3268 - val_mse: 499.3268 - val_mae: 16.6119\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 573.6229 - mse: 573.6229 - mae: 18.2510 - val_loss: 495.4481 - val_mse: 495.4481 - val_mae: 16.5552\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 569.9076 - mse: 569.9076 - mae: 18.1985 - val_loss: 491.7095 - val_mse: 491.7095 - val_mae: 16.5015\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 566.3413 - mse: 566.3413 - mae: 18.1462 - val_loss: 487.9107 - val_mse: 487.9107 - val_mae: 16.4462\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 562.7391 - mse: 562.7391 - mae: 18.0936 - val_loss: 484.2014 - val_mse: 484.2014 - val_mae: 16.3914\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 559.2490 - mse: 559.2490 - mae: 18.0424 - val_loss: 480.5316 - val_mse: 480.5316 - val_mae: 16.3378\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 555.7468 - mse: 555.7468 - mae: 17.9914 - val_loss: 476.8818 - val_mse: 476.8818 - val_mae: 16.2848\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 552.3486 - mse: 552.3486 - mae: 17.9404 - val_loss: 473.2432 - val_mse: 473.2432 - val_mae: 16.2318\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpwzx6qto4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 33ms/step - loss: 726.0087 - mse: 726.0087 - mae: 22.3659 - val_loss: 736.2354 - val_mse: 736.2354 - val_mae: 22.5504\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 719.0154 - mse: 719.0154 - mae: 22.2587 - val_loss: 729.0385 - val_mse: 729.0385 - val_mae: 22.4426\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 712.2152 - mse: 712.2152 - mae: 22.1516 - val_loss: 721.8661 - val_mse: 721.8661 - val_mae: 22.3340\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 705.3958 - mse: 705.3958 - mae: 22.0446 - val_loss: 714.8021 - val_mse: 714.8021 - val_mae: 22.2262\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 698.6697 - mse: 698.6697 - mae: 21.9366 - val_loss: 707.7885 - val_mse: 707.7885 - val_mae: 22.1180\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 691.9544 - mse: 691.9544 - mae: 21.8298 - val_loss: 700.9053 - val_mse: 700.9053 - val_mae: 22.0106\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 685.3812 - mse: 685.3812 - mae: 21.7232 - val_loss: 694.0835 - val_mse: 694.0835 - val_mae: 21.9025\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 678.8630 - mse: 678.8630 - mae: 21.6158 - val_loss: 687.3712 - val_mse: 687.3712 - val_mae: 21.7951\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 672.4344 - mse: 672.4344 - mae: 21.5097 - val_loss: 680.7327 - val_mse: 680.7327 - val_mae: 21.6876\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 666.1284 - mse: 666.1284 - mae: 21.4048 - val_loss: 674.1738 - val_mse: 674.1738 - val_mae: 21.5805\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 659.8764 - mse: 659.8764 - mae: 21.3000 - val_loss: 667.7318 - val_mse: 667.7318 - val_mae: 21.4738\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 653.7611 - mse: 653.7611 - mae: 21.1965 - val_loss: 661.4018 - val_mse: 661.4018 - val_mae: 21.3678\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 647.7999 - mse: 647.7999 - mae: 21.0931 - val_loss: 655.1052 - val_mse: 655.1052 - val_mae: 21.2617\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 641.8010 - mse: 641.8010 - mae: 20.9905 - val_loss: 648.9624 - val_mse: 648.9624 - val_mae: 21.1576\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 635.9534 - mse: 635.9534 - mae: 20.8893 - val_loss: 642.9434 - val_mse: 642.9434 - val_mae: 21.0545\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 630.2028 - mse: 630.2028 - mae: 20.7902 - val_loss: 637.0612 - val_mse: 637.0612 - val_mae: 20.9530\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 624.5620 - mse: 624.5620 - mae: 20.6909 - val_loss: 631.2800 - val_mse: 631.2800 - val_mae: 20.8526\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 619.0653 - mse: 619.0653 - mae: 20.5930 - val_loss: 625.4896 - val_mse: 625.4896 - val_mae: 20.7520\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 613.5982 - mse: 613.5982 - mae: 20.4962 - val_loss: 619.7733 - val_mse: 619.7733 - val_mae: 20.6516\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 608.1550 - mse: 608.1550 - mae: 20.3991 - val_loss: 614.2173 - val_mse: 614.2173 - val_mae: 20.5538\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 602.8472 - mse: 602.8472 - mae: 20.3042 - val_loss: 608.7370 - val_mse: 608.7370 - val_mae: 20.4560\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 597.6501 - mse: 597.6501 - mae: 20.2094 - val_loss: 603.3458 - val_mse: 603.3458 - val_mae: 20.3599\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 592.4775 - mse: 592.4775 - mae: 20.1159 - val_loss: 598.0545 - val_mse: 598.0545 - val_mae: 20.2643\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 587.4280 - mse: 587.4280 - mae: 20.0229 - val_loss: 592.8080 - val_mse: 592.8080 - val_mae: 20.1678\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 582.4004 - mse: 582.4004 - mae: 19.9300 - val_loss: 587.6594 - val_mse: 587.6594 - val_mae: 20.0723\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 577.4394 - mse: 577.4394 - mae: 19.8371 - val_loss: 582.5802 - val_mse: 582.5802 - val_mae: 19.9773\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 572.5765 - mse: 572.5765 - mae: 19.7450 - val_loss: 577.5327 - val_mse: 577.5327 - val_mae: 19.8821\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 567.7802 - mse: 567.7802 - mae: 19.6538 - val_loss: 572.4869 - val_mse: 572.4869 - val_mae: 19.7857\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 562.9382 - mse: 562.9382 - mae: 19.5619 - val_loss: 567.5956 - val_mse: 567.5956 - val_mae: 19.6914\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 558.2270 - mse: 558.2270 - mae: 19.4716 - val_loss: 562.7898 - val_mse: 562.7898 - val_mae: 19.5990\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6536 - mse: 553.6536 - mae: 19.3824 - val_loss: 557.9921 - val_mse: 557.9921 - val_mae: 19.5063\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 549.1199 - mse: 549.1199 - mae: 19.2936 - val_loss: 553.2286 - val_mse: 553.2286 - val_mae: 19.4135\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 544.6093 - mse: 544.6093 - mae: 19.2060 - val_loss: 548.5651 - val_mse: 548.5651 - val_mae: 19.3217\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 540.2119 - mse: 540.2119 - mae: 19.1187 - val_loss: 543.9698 - val_mse: 543.9698 - val_mae: 19.2308\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 535.8982 - mse: 535.8982 - mae: 19.0346 - val_loss: 539.4536 - val_mse: 539.4536 - val_mae: 19.1413\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 531.6216 - mse: 531.6216 - mae: 18.9518 - val_loss: 535.0181 - val_mse: 535.0181 - val_mae: 19.0530\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 527.4064 - mse: 527.4064 - mae: 18.8681 - val_loss: 530.6647 - val_mse: 530.6647 - val_mae: 18.9661\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 523.2859 - mse: 523.2859 - mae: 18.7868 - val_loss: 526.3221 - val_mse: 526.3221 - val_mae: 18.8799\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 519.2361 - mse: 519.2361 - mae: 18.7063 - val_loss: 522.0331 - val_mse: 522.0331 - val_mae: 18.7943\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 515.1812 - mse: 515.1812 - mae: 18.6252 - val_loss: 517.8701 - val_mse: 517.8701 - val_mae: 18.7108\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 511.2234 - mse: 511.2234 - mae: 18.5472 - val_loss: 513.8493 - val_mse: 513.8493 - val_mae: 18.6295\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 507.3613 - mse: 507.3613 - mae: 18.4698 - val_loss: 509.8945 - val_mse: 509.8945 - val_mae: 18.5499\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 503.6131 - mse: 503.6131 - mae: 18.3937 - val_loss: 506.0002 - val_mse: 506.0002 - val_mae: 18.4714\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 499.8920 - mse: 499.8920 - mae: 18.3183 - val_loss: 502.1930 - val_mse: 502.1930 - val_mae: 18.3948\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 496.2586 - mse: 496.2586 - mae: 18.2438 - val_loss: 498.4314 - val_mse: 498.4314 - val_mae: 18.3199\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 492.6458 - mse: 492.6458 - mae: 18.1704 - val_loss: 494.7523 - val_mse: 494.7523 - val_mae: 18.2463\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 489.1467 - mse: 489.1467 - mae: 18.0991 - val_loss: 491.0907 - val_mse: 491.0907 - val_mae: 18.1735\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 485.6783 - mse: 485.6783 - mae: 18.0274 - val_loss: 487.4654 - val_mse: 487.4654 - val_mae: 18.1017\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 482.2815 - mse: 482.2815 - mae: 17.9570 - val_loss: 483.8756 - val_mse: 483.8756 - val_mae: 18.0300\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 478.8605 - mse: 478.8605 - mae: 17.8874 - val_loss: 480.4235 - val_mse: 480.4235 - val_mae: 17.9610\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 475.5703 - mse: 475.5703 - mae: 17.8195 - val_loss: 476.9787 - val_mse: 476.9787 - val_mae: 17.8927\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 472.2959 - mse: 472.2959 - mae: 17.7525 - val_loss: 473.5709 - val_mse: 473.5709 - val_mae: 17.8253\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 469.0939 - mse: 469.0939 - mae: 17.6863 - val_loss: 470.1786 - val_mse: 470.1786 - val_mae: 17.7583\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 465.8866 - mse: 465.8866 - mae: 17.6203 - val_loss: 466.8829 - val_mse: 466.8829 - val_mae: 17.6929\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 462.7234 - mse: 462.7234 - mae: 17.5550 - val_loss: 463.6719 - val_mse: 463.6719 - val_mae: 17.6290\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 459.6397 - mse: 459.6397 - mae: 17.4918 - val_loss: 460.4879 - val_mse: 460.4879 - val_mae: 17.5656\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 456.5781 - mse: 456.5781 - mae: 17.4284 - val_loss: 457.3287 - val_mse: 457.3287 - val_mae: 17.5026\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 453.5741 - mse: 453.5741 - mae: 17.3668 - val_loss: 454.2279 - val_mse: 454.2279 - val_mae: 17.4402\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 450.6078 - mse: 450.6078 - mae: 17.3052 - val_loss: 451.2224 - val_mse: 451.2224 - val_mae: 17.3796\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 447.7009 - mse: 447.7009 - mae: 17.2446 - val_loss: 448.2490 - val_mse: 448.2490 - val_mae: 17.3193\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 444.8531 - mse: 444.8531 - mae: 17.1843 - val_loss: 445.3039 - val_mse: 445.3039 - val_mae: 17.2599\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 441.9889 - mse: 441.9889 - mae: 17.1252 - val_loss: 442.4586 - val_mse: 442.4586 - val_mae: 17.2018\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 439.2110 - mse: 439.2110 - mae: 17.0663 - val_loss: 439.6479 - val_mse: 439.6479 - val_mae: 17.1447\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 436.5073 - mse: 436.5073 - mae: 17.0087 - val_loss: 436.8289 - val_mse: 436.8289 - val_mae: 17.0876\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 433.7785 - mse: 433.7785 - mae: 16.9512 - val_loss: 434.1353 - val_mse: 434.1353 - val_mae: 17.0323\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 431.1676 - mse: 431.1676 - mae: 16.8948 - val_loss: 431.4652 - val_mse: 431.4652 - val_mae: 16.9774\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.5815 - mse: 428.5815 - mae: 16.8397 - val_loss: 428.8557 - val_mse: 428.8557 - val_mae: 16.9234\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 426.0599 - mse: 426.0599 - mae: 16.7852 - val_loss: 426.3105 - val_mse: 426.3105 - val_mae: 16.8702\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 423.5895 - mse: 423.5895 - mae: 16.7324 - val_loss: 423.8004 - val_mse: 423.8004 - val_mae: 16.8176\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 421.1683 - mse: 421.1683 - mae: 16.6796 - val_loss: 421.3429 - val_mse: 421.3429 - val_mae: 16.7651\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 418.7805 - mse: 418.7805 - mae: 16.6271 - val_loss: 418.8901 - val_mse: 418.8901 - val_mae: 16.7132\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 416.3763 - mse: 416.3763 - mae: 16.5749 - val_loss: 416.5150 - val_mse: 416.5150 - val_mae: 16.6623\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 414.0563 - mse: 414.0563 - mae: 16.5230 - val_loss: 414.1076 - val_mse: 414.1076 - val_mae: 16.6102\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 411.7241 - mse: 411.7241 - mae: 16.4715 - val_loss: 411.7480 - val_mse: 411.7480 - val_mae: 16.5585\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 409.4734 - mse: 409.4734 - mae: 16.4211 - val_loss: 409.4068 - val_mse: 409.4068 - val_mae: 16.5072\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 407.2435 - mse: 407.2435 - mae: 16.3718 - val_loss: 407.1319 - val_mse: 407.1319 - val_mae: 16.4570\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 405.0728 - mse: 405.0728 - mae: 16.3225 - val_loss: 404.9201 - val_mse: 404.9201 - val_mae: 16.4081\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 402.9597 - mse: 402.9597 - mae: 16.2754 - val_loss: 402.7382 - val_mse: 402.7382 - val_mae: 16.3598\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 400.8877 - mse: 400.8877 - mae: 16.2286 - val_loss: 400.5915 - val_mse: 400.5915 - val_mae: 16.3119\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 398.8381 - mse: 398.8381 - mae: 16.1827 - val_loss: 398.4288 - val_mse: 398.4288 - val_mae: 16.2639\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 396.8173 - mse: 396.8173 - mae: 16.1374 - val_loss: 396.2650 - val_mse: 396.2650 - val_mae: 16.2152\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 394.7579 - mse: 394.7579 - mae: 16.0916 - val_loss: 394.1707 - val_mse: 394.1707 - val_mae: 16.1670\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 392.7554 - mse: 392.7554 - mae: 16.0457 - val_loss: 392.0777 - val_mse: 392.0777 - val_mae: 16.1191\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 390.7743 - mse: 390.7743 - mae: 16.0006 - val_loss: 389.9889 - val_mse: 389.9889 - val_mae: 16.0713\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 388.7785 - mse: 388.7785 - mae: 15.9559 - val_loss: 387.9250 - val_mse: 387.9250 - val_mae: 16.0236\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 386.7977 - mse: 386.7977 - mae: 15.9106 - val_loss: 385.8105 - val_mse: 385.8105 - val_mae: 15.9752\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 384.7942 - mse: 384.7942 - mae: 15.8649 - val_loss: 383.6264 - val_mse: 383.6264 - val_mae: 15.9251\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 382.7198 - mse: 382.7198 - mae: 15.8177 - val_loss: 381.4985 - val_mse: 381.4985 - val_mae: 15.8753\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 380.6983 - mse: 380.6983 - mae: 15.7707 - val_loss: 379.4214 - val_mse: 379.4214 - val_mae: 15.8272\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 378.7358 - mse: 378.7358 - mae: 15.7252 - val_loss: 377.3877 - val_mse: 377.3877 - val_mae: 15.7801\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 376.8422 - mse: 376.8422 - mae: 15.6811 - val_loss: 375.4177 - val_mse: 375.4177 - val_mae: 15.7347\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 375.0085 - mse: 375.0085 - mae: 15.6389 - val_loss: 373.5041 - val_mse: 373.5041 - val_mae: 15.6900\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 373.2345 - mse: 373.2345 - mae: 15.5972 - val_loss: 371.6369 - val_mse: 371.6369 - val_mae: 15.6462\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 371.4821 - mse: 371.4821 - mae: 15.5566 - val_loss: 369.8406 - val_mse: 369.8406 - val_mae: 15.6039\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 369.7976 - mse: 369.7976 - mae: 15.5169 - val_loss: 368.0109 - val_mse: 368.0109 - val_mae: 15.5613\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 368.1097 - mse: 368.1097 - mae: 15.4771 - val_loss: 366.2513 - val_mse: 366.2513 - val_mae: 15.5202\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 366.4810 - mse: 366.4810 - mae: 15.4385 - val_loss: 364.5406 - val_mse: 364.5406 - val_mae: 15.4809\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 364.8617 - mse: 364.8617 - mae: 15.3999 - val_loss: 362.8775 - val_mse: 362.8775 - val_mae: 15.4424\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 363.3155 - mse: 363.3155 - mae: 15.3626 - val_loss: 361.2047 - val_mse: 361.2047 - val_mae: 15.4035\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 361.7494 - mse: 361.7494 - mae: 15.3262 - val_loss: 359.6041 - val_mse: 359.6041 - val_mae: 15.3659\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpspozt589/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 5271.5146 - mse: 5271.5146 - mae: 49.8664"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 1359.7433 - mse: 1359.7433 - mae: 27.2883 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.7146 - mse: 598.7146 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 598.7147 - mse: 598.7147 - mae: 22.7033 - val_loss: 449.3011 - val_mse: 449.3011 - val_mae: 20.2778\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpbcxkcw_7/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 3513.0066 - mse: 3513.0066 - mae: 39.3029"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 17ms/step - loss: 1106.9739 - mse: 1106.9739 - mae: 25.2231 - val_loss: 428.9830 - val_mse: 428.9830 - val_mae: 19.6353\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 613.9508 - mse: 613.9508 - mae: 22.5200 - val_loss: 425.7628 - val_mse: 425.7628 - val_mae: 19.4896\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 600.5897 - mse: 600.5897 - mae: 22.1369 - val_loss: 417.2221 - val_mse: 417.2221 - val_mae: 19.1572\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 585.8323 - mse: 585.8323 - mae: 21.7114 - val_loss: 406.6393 - val_mse: 406.6393 - val_mae: 18.7853\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 582.8121 - mse: 582.8121 - mae: 21.6069 - val_loss: 406.5931 - val_mse: 406.5931 - val_mae: 18.7827\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 578.2479 - mse: 578.2479 - mae: 21.4674 - val_loss: 399.0290 - val_mse: 399.0290 - val_mae: 18.3812\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 575.5038 - mse: 575.5038 - mae: 21.3748 - val_loss: 401.7193 - val_mse: 401.7193 - val_mae: 18.5579\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 567.0165 - mse: 567.0165 - mae: 21.1731 - val_loss: 389.4554 - val_mse: 389.4554 - val_mae: 18.1246\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 557.5786 - mse: 557.5786 - mae: 20.8593 - val_loss: 389.2589 - val_mse: 389.2589 - val_mae: 18.1508\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 550.5732 - mse: 550.5732 - mae: 20.6065 - val_loss: 377.9600 - val_mse: 377.9600 - val_mae: 17.7693\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 496.2637 - mse: 496.2637 - mae: 19.0649 - val_loss: 338.4955 - val_mse: 338.4955 - val_mae: 16.2464\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 488.5785 - mse: 488.5785 - mae: 18.8561 - val_loss: 331.0305 - val_mse: 331.0305 - val_mae: 15.7802\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 485.2293 - mse: 485.2293 - mae: 18.6740 - val_loss: 327.6593 - val_mse: 327.6593 - val_mae: 15.6481\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 467.7518 - mse: 467.7518 - mae: 18.1593 - val_loss: 319.0632 - val_mse: 319.0632 - val_mae: 15.3658\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 466.8058 - mse: 466.8058 - mae: 18.1878 - val_loss: 313.2342 - val_mse: 313.2342 - val_mae: 15.0499\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 465.0514 - mse: 465.0514 - mae: 18.0068 - val_loss: 316.8187 - val_mse: 316.8187 - val_mae: 15.3826\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 462.7094 - mse: 462.7094 - mae: 18.0032 - val_loss: 313.5180 - val_mse: 313.5180 - val_mae: 15.0916\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 462.3238 - mse: 462.3238 - mae: 17.9582 - val_loss: 316.9919 - val_mse: 316.9919 - val_mae: 15.3768\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 467.0111 - mse: 467.0111 - mae: 18.2667 - val_loss: 317.5935 - val_mse: 317.5935 - val_mae: 15.3771\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 462.4165 - mse: 462.4165 - mae: 17.9693 - val_loss: 311.5265 - val_mse: 311.5265 - val_mae: 14.8960\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 463.1433 - mse: 463.1433 - mae: 18.0354 - val_loss: 312.6258 - val_mse: 312.6258 - val_mae: 15.0410\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 465.4766 - mse: 465.4766 - mae: 18.0446 - val_loss: 322.8945 - val_mse: 322.8945 - val_mae: 15.7671\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 463.2152 - mse: 463.2152 - mae: 18.0211 - val_loss: 323.8972 - val_mse: 323.8972 - val_mae: 15.8566\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 461.0995 - mse: 461.0995 - mae: 17.9194 - val_loss: 313.3065 - val_mse: 313.3065 - val_mae: 15.1201\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 459.1868 - mse: 459.1868 - mae: 17.8163 - val_loss: 311.3051 - val_mse: 311.3051 - val_mae: 14.9019\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 459.3497 - mse: 459.3497 - mae: 17.8652 - val_loss: 310.9574 - val_mse: 310.9574 - val_mae: 14.8600\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 460.4490 - mse: 460.4490 - mae: 17.8595 - val_loss: 316.0317 - val_mse: 316.0317 - val_mae: 15.3117\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 460.5180 - mse: 460.5180 - mae: 17.8759 - val_loss: 311.1758 - val_mse: 311.1758 - val_mae: 14.8950\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 460.6858 - mse: 460.6858 - mae: 17.8431 - val_loss: 310.5182 - val_mse: 310.5182 - val_mae: 14.8175\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 461.7712 - mse: 461.7712 - mae: 17.9402 - val_loss: 310.6279 - val_mse: 310.6279 - val_mae: 14.8239\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 462.0474 - mse: 462.0474 - mae: 17.9054 - val_loss: 310.2279 - val_mse: 310.2279 - val_mae: 14.8055\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 460.1964 - mse: 460.1964 - mae: 17.7024 - val_loss: 328.0771 - val_mse: 328.0771 - val_mae: 16.1572\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 461.3568 - mse: 461.3568 - mae: 17.8993 - val_loss: 321.4155 - val_mse: 321.4155 - val_mae: 15.6907\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 460.9796 - mse: 460.9796 - mae: 17.9272 - val_loss: 338.7510 - val_mse: 338.7510 - val_mae: 16.7012\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 468.1466 - mse: 468.1466 - mae: 18.1904 - val_loss: 338.4126 - val_mse: 338.4126 - val_mae: 16.6863\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 462.1749 - mse: 462.1749 - mae: 17.9584 - val_loss: 319.4424 - val_mse: 319.4424 - val_mae: 15.5383\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 460.7014 - mse: 460.7014 - mae: 17.9141 - val_loss: 315.9932 - val_mse: 315.9932 - val_mae: 15.3037\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 458.2799 - mse: 458.2799 - mae: 17.7958 - val_loss: 312.7837 - val_mse: 312.7837 - val_mae: 15.0624\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 456.2581 - mse: 456.2581 - mae: 17.7247 - val_loss: 309.9154 - val_mse: 309.9154 - val_mae: 14.7724\n",
      "Epoch 39: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpc5b29ny2/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 3560.2710 - mse: 3560.2710 - mae: 44.4479"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 17ms/step - loss: 936.2183 - mse: 936.2183 - mae: 23.7022 - val_loss: 526.7562 - val_mse: 526.7562 - val_mae: 20.8968\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 502.3900 - mse: 502.3900 - mae: 20.2300 - val_loss: 462.9322 - val_mse: 462.9322 - val_mae: 19.2322\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 474.4496 - mse: 474.4496 - mae: 19.4108 - val_loss: 459.5667 - val_mse: 459.5667 - val_mae: 19.0952\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 464.6908 - mse: 464.6908 - mae: 19.0148 - val_loss: 450.0067 - val_mse: 450.0067 - val_mae: 18.6148\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 461.2832 - mse: 461.2832 - mae: 18.8044 - val_loss: 449.3142 - val_mse: 449.3142 - val_mae: 18.5309\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 457.7337 - mse: 457.7337 - mae: 18.6857 - val_loss: 436.4950 - val_mse: 436.4950 - val_mae: 18.2705\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 446.3051 - mse: 446.3051 - mae: 18.3932 - val_loss: 432.1754 - val_mse: 432.1754 - val_mae: 18.1223\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.8564 - mse: 440.8564 - mae: 18.1508 - val_loss: 436.3636 - val_mse: 436.3636 - val_mae: 18.3122\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.5171 - mse: 440.5171 - mae: 18.0767 - val_loss: 423.8631 - val_mse: 423.8631 - val_mae: 17.6494\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 434.0247 - mse: 434.0247 - mae: 17.8236 - val_loss: 423.7397 - val_mse: 423.7397 - val_mae: 17.6305\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 433.0314 - mse: 433.0314 - mae: 17.7923 - val_loss: 422.0764 - val_mse: 422.0764 - val_mae: 17.6181\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 430.7487 - mse: 430.7487 - mae: 17.7929 - val_loss: 418.8887 - val_mse: 418.8887 - val_mae: 17.5921\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 431.7597 - mse: 431.7597 - mae: 18.0553 - val_loss: 413.7810 - val_mse: 413.7810 - val_mae: 17.5184\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 435.4193 - mse: 435.4193 - mae: 18.1691 - val_loss: 424.1333 - val_mse: 424.1333 - val_mae: 18.1374\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 431.5866 - mse: 431.5866 - mae: 18.0000 - val_loss: 412.5361 - val_mse: 412.5361 - val_mae: 17.5226\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 422.2576 - mse: 422.2576 - mae: 17.5150 - val_loss: 408.8256 - val_mse: 408.8256 - val_mae: 17.3454\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 421.0365 - mse: 421.0365 - mae: 17.5618 - val_loss: 415.5118 - val_mse: 415.5118 - val_mae: 17.5445\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 424.2527 - mse: 424.2527 - mae: 17.7479 - val_loss: 422.3434 - val_mse: 422.3434 - val_mae: 17.9291\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 426.2099 - mse: 426.2099 - mae: 17.8927 - val_loss: 414.5682 - val_mse: 414.5682 - val_mae: 17.4806\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 424.4329 - mse: 424.4329 - mae: 17.6010 - val_loss: 408.5824 - val_mse: 408.5824 - val_mae: 17.2417\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 421.8321 - mse: 421.8321 - mae: 17.4789 - val_loss: 408.7935 - val_mse: 408.7935 - val_mae: 17.3158\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 421.2348 - mse: 421.2348 - mae: 17.4706 - val_loss: 411.3380 - val_mse: 411.3380 - val_mae: 17.4608\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 427.3405 - mse: 427.3405 - mae: 17.7921 - val_loss: 415.0930 - val_mse: 415.0930 - val_mae: 17.6470\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 424.5562 - mse: 424.5562 - mae: 17.6645 - val_loss: 417.5485 - val_mse: 417.5485 - val_mae: 17.8050\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 424.1747 - mse: 424.1747 - mae: 17.6342 - val_loss: 409.9900 - val_mse: 409.9900 - val_mae: 17.3437\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 421.5580 - mse: 421.5580 - mae: 17.5213 - val_loss: 407.8330 - val_mse: 407.8330 - val_mae: 17.2211\n",
      "Epoch 26: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpebg9e0e8/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpxvsmjg5m/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 23560.8262 - mse: 23560.8262 - mae: 73.5918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 16ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpwuv97_s3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 21635.4844 - mse: 21635.4844 - mae: 71.4221"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp1eu3ad5v/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 24492.2461 - mse: 24492.2461 - mae: 74.6546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpci0v9324/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 23560.8262 - mse: 23560.8262 - mae: 73.5918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp1g750yqg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpe1qtyhs1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 24492.2461 - mse: 24492.2461 - mae: 74.6546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 27ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpm6ndzn54/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 23560.8262 - mse: 23560.8262 - mae: 73.5918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 16ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpvmgy8ldr/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 21635.4844 - mse: 21635.4844 - mae: 71.4221"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 16ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp0_8ojs_e/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 24492.2461 - mse: 24492.2461 - mae: 74.6546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpq8615v8l/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 23560.8262 - mse: 23560.8262 - mae: 73.5918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 14ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpy_jcb_q1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 21635.4844 - mse: 21635.4844 - mae: 71.4221"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpy22zncj0/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 1300.3489 - mse: 1300.3489 - mae: 27.8009"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 17ms/step - loss: 1230.5099 - mse: 1230.5099 - mae: 27.6728 - val_loss: 1156.8307 - val_mse: 1156.8307 - val_mae: 26.3766\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1121.9858 - mse: 1121.9858 - mae: 26.7546 - val_loss: 1046.5934 - val_mse: 1046.5934 - val_mae: 25.4359\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1024.8640 - mse: 1024.8640 - mae: 25.8830 - val_loss: 953.3627 - val_mse: 953.3627 - val_mae: 24.5875\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 942.4117 - mse: 942.4117 - mae: 25.0830 - val_loss: 872.6797 - val_mse: 872.6797 - val_mae: 23.8017\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 872.7255 - mse: 872.7255 - mae: 24.3610 - val_loss: 804.5296 - val_mse: 804.5296 - val_mae: 23.0904\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 812.6160 - mse: 812.6160 - mae: 23.7127 - val_loss: 745.8941 - val_mse: 745.8941 - val_mae: 22.4411\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 761.2930 - mse: 761.2930 - mae: 23.1161 - val_loss: 694.5762 - val_mse: 694.5762 - val_mae: 21.8392\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 717.2548 - mse: 717.2548 - mae: 22.5735 - val_loss: 650.7367 - val_mse: 650.7367 - val_mae: 21.2983\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 679.5407 - mse: 679.5407 - mae: 22.0898 - val_loss: 613.0020 - val_mse: 613.0020 - val_mae: 20.8071\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 647.5405 - mse: 647.5405 - mae: 21.6546 - val_loss: 581.1792 - val_mse: 581.1792 - val_mae: 20.3697\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 620.2335 - mse: 620.2335 - mae: 21.2572 - val_loss: 552.7504 - val_mse: 552.7504 - val_mae: 19.9596\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 596.6156 - mse: 596.6156 - mae: 20.9010 - val_loss: 529.0543 - val_mse: 529.0543 - val_mae: 19.5962\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 576.2736 - mse: 576.2736 - mae: 20.5805 - val_loss: 507.9510 - val_mse: 507.9510 - val_mae: 19.2589\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 558.8835 - mse: 558.8835 - mae: 20.2884 - val_loss: 489.4555 - val_mse: 489.4555 - val_mae: 18.9523\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 543.5204 - mse: 543.5204 - mae: 20.0274 - val_loss: 473.6639 - val_mse: 473.6639 - val_mae: 18.6763\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.2406 - mse: 530.2406 - mae: 19.7899 - val_loss: 459.4871 - val_mse: 459.4871 - val_mae: 18.4244\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 518.6773 - mse: 518.6773 - mae: 19.5758 - val_loss: 446.6550 - val_mse: 446.6550 - val_mae: 18.1943\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 508.6486 - mse: 508.6486 - mae: 19.3826 - val_loss: 435.7575 - val_mse: 435.7575 - val_mae: 17.9890\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 499.9987 - mse: 499.9987 - mae: 19.2089 - val_loss: 426.3628 - val_mse: 426.3628 - val_mae: 17.8062\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 492.4407 - mse: 492.4407 - mae: 19.0530 - val_loss: 417.8556 - val_mse: 417.8556 - val_mae: 17.6397\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 485.7863 - mse: 485.7863 - mae: 18.9113 - val_loss: 410.4320 - val_mse: 410.4320 - val_mae: 17.4924\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 479.9353 - mse: 479.9353 - mae: 18.7848 - val_loss: 403.7289 - val_mse: 403.7289 - val_mae: 17.3580\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 474.7538 - mse: 474.7538 - mae: 18.6693 - val_loss: 397.5920 - val_mse: 397.5920 - val_mae: 17.2363\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 470.1592 - mse: 470.1592 - mae: 18.5642 - val_loss: 392.2255 - val_mse: 392.2255 - val_mae: 17.1267\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 466.0843 - mse: 466.0843 - mae: 18.4711 - val_loss: 387.4765 - val_mse: 387.4765 - val_mae: 17.0292\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 462.4474 - mse: 462.4474 - mae: 18.3861 - val_loss: 383.0760 - val_mse: 383.0760 - val_mae: 16.9390\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 459.1631 - mse: 459.1631 - mae: 18.3083 - val_loss: 379.0304 - val_mse: 379.0304 - val_mae: 16.8566\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 456.2070 - mse: 456.2070 - mae: 18.2365 - val_loss: 375.4222 - val_mse: 375.4222 - val_mae: 16.7809\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 453.5154 - mse: 453.5154 - mae: 18.1714 - val_loss: 372.0648 - val_mse: 372.0648 - val_mae: 16.7106\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 450.9951 - mse: 450.9951 - mae: 18.1104 - val_loss: 368.9762 - val_mse: 368.9762 - val_mae: 16.6460\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 448.7339 - mse: 448.7339 - mae: 18.0537 - val_loss: 366.0380 - val_mse: 366.0380 - val_mae: 16.5844\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 446.6214 - mse: 446.6214 - mae: 18.0005 - val_loss: 363.3311 - val_mse: 363.3311 - val_mae: 16.5263\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 444.7100 - mse: 444.7100 - mae: 17.9519 - val_loss: 360.8842 - val_mse: 360.8842 - val_mae: 16.4741\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 442.9327 - mse: 442.9327 - mae: 17.9063 - val_loss: 358.6269 - val_mse: 358.6269 - val_mae: 16.4248\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 441.3042 - mse: 441.3042 - mae: 17.8640 - val_loss: 356.4265 - val_mse: 356.4265 - val_mae: 16.3771\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 439.6752 - mse: 439.6752 - mae: 17.8212 - val_loss: 354.4494 - val_mse: 354.4494 - val_mae: 16.3331\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 438.1259 - mse: 438.1259 - mae: 17.7845 - val_loss: 352.3194 - val_mse: 352.3194 - val_mae: 16.2881\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 436.6686 - mse: 436.6686 - mae: 17.7463 - val_loss: 350.3637 - val_mse: 350.3637 - val_mae: 16.2449\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 435.2869 - mse: 435.2869 - mae: 17.7109 - val_loss: 348.5648 - val_mse: 348.5648 - val_mae: 16.2044\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 434.0252 - mse: 434.0252 - mae: 17.6774 - val_loss: 346.8121 - val_mse: 346.8121 - val_mae: 16.1654\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 432.7046 - mse: 432.7046 - mae: 17.6425 - val_loss: 345.2150 - val_mse: 345.2150 - val_mae: 16.1291\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 431.4660 - mse: 431.4660 - mae: 17.6100 - val_loss: 343.6126 - val_mse: 343.6126 - val_mae: 16.0920\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 430.2101 - mse: 430.2101 - mae: 17.5774 - val_loss: 341.9816 - val_mse: 341.9816 - val_mae: 16.0538\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.9521 - mse: 428.9521 - mae: 17.5460 - val_loss: 340.3065 - val_mse: 340.3065 - val_mae: 16.0145\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 427.6497 - mse: 427.6497 - mae: 17.5110 - val_loss: 338.7875 - val_mse: 338.7875 - val_mae: 15.9764\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 426.3473 - mse: 426.3473 - mae: 17.4771 - val_loss: 337.2961 - val_mse: 337.2961 - val_mae: 15.9384\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 425.1010 - mse: 425.1010 - mae: 17.4437 - val_loss: 335.8349 - val_mse: 335.8349 - val_mae: 15.9012\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 423.8454 - mse: 423.8454 - mae: 17.4099 - val_loss: 334.4243 - val_mse: 334.4243 - val_mae: 15.8654\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 422.6711 - mse: 422.6711 - mae: 17.3778 - val_loss: 333.1658 - val_mse: 333.1658 - val_mae: 15.8321\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 421.5763 - mse: 421.5763 - mae: 17.3474 - val_loss: 331.9572 - val_mse: 331.9572 - val_mae: 15.8002\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 420.5273 - mse: 420.5273 - mae: 17.3178 - val_loss: 330.8039 - val_mse: 330.8039 - val_mae: 15.7686\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 419.5200 - mse: 419.5200 - mae: 17.2889 - val_loss: 329.6601 - val_mse: 329.6601 - val_mae: 15.7374\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 418.5573 - mse: 418.5573 - mae: 17.2620 - val_loss: 328.6082 - val_mse: 328.6082 - val_mae: 15.7081\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 417.6562 - mse: 417.6562 - mae: 17.2364 - val_loss: 327.6092 - val_mse: 327.6092 - val_mae: 15.6802\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 416.7574 - mse: 416.7574 - mae: 17.2116 - val_loss: 326.5441 - val_mse: 326.5441 - val_mae: 15.6518\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 415.9137 - mse: 415.9137 - mae: 17.1870 - val_loss: 325.6048 - val_mse: 325.6048 - val_mae: 15.6255\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 415.0960 - mse: 415.0960 - mae: 17.1636 - val_loss: 324.6830 - val_mse: 324.6830 - val_mae: 15.5998\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 414.3083 - mse: 414.3083 - mae: 17.1422 - val_loss: 323.7081 - val_mse: 323.7081 - val_mae: 15.5745\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 413.5446 - mse: 413.5446 - mae: 17.1200 - val_loss: 322.9594 - val_mse: 322.9594 - val_mae: 15.5521\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 412.7930 - mse: 412.7930 - mae: 17.0990 - val_loss: 322.1178 - val_mse: 322.1178 - val_mae: 15.5287\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 412.0705 - mse: 412.0705 - mae: 17.0789 - val_loss: 321.2711 - val_mse: 321.2711 - val_mae: 15.5058\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 411.3907 - mse: 411.3907 - mae: 17.0595 - val_loss: 320.5250 - val_mse: 320.5250 - val_mae: 15.4852\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 410.6992 - mse: 410.6992 - mae: 17.0404 - val_loss: 319.7342 - val_mse: 319.7342 - val_mae: 15.4639\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 410.0304 - mse: 410.0304 - mae: 17.0224 - val_loss: 318.9717 - val_mse: 318.9717 - val_mae: 15.4437\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 409.3904 - mse: 409.3904 - mae: 17.0044 - val_loss: 318.2686 - val_mse: 318.2686 - val_mae: 15.4248\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 408.7500 - mse: 408.7500 - mae: 16.9874 - val_loss: 317.4826 - val_mse: 317.4826 - val_mae: 15.4051\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 408.1253 - mse: 408.1253 - mae: 16.9702 - val_loss: 316.7789 - val_mse: 316.7789 - val_mae: 15.3867\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 407.5306 - mse: 407.5306 - mae: 16.9538 - val_loss: 316.0784 - val_mse: 316.0784 - val_mae: 15.3687\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 406.9459 - mse: 406.9459 - mae: 16.9367 - val_loss: 315.5033 - val_mse: 315.5033 - val_mae: 15.3522\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 406.3358 - mse: 406.3358 - mae: 16.9200 - val_loss: 314.8107 - val_mse: 314.8107 - val_mae: 15.3342\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 405.7725 - mse: 405.7725 - mae: 16.9041 - val_loss: 314.1734 - val_mse: 314.1734 - val_mae: 15.3167\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 405.2106 - mse: 405.2106 - mae: 16.8880 - val_loss: 313.4637 - val_mse: 313.4637 - val_mae: 15.2984\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 404.6625 - mse: 404.6625 - mae: 16.8717 - val_loss: 312.9254 - val_mse: 312.9254 - val_mae: 15.2824\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 404.1214 - mse: 404.1214 - mae: 16.8560 - val_loss: 312.2694 - val_mse: 312.2694 - val_mae: 15.2650\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 403.5722 - mse: 403.5722 - mae: 16.8406 - val_loss: 311.7063 - val_mse: 311.7063 - val_mae: 15.2492\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 403.0527 - mse: 403.0527 - mae: 16.8254 - val_loss: 311.0910 - val_mse: 311.0910 - val_mae: 15.2319\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 402.5097 - mse: 402.5097 - mae: 16.8095 - val_loss: 310.4508 - val_mse: 310.4508 - val_mae: 15.2143\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 401.9795 - mse: 401.9795 - mae: 16.7934 - val_loss: 309.9105 - val_mse: 309.9105 - val_mae: 15.1980\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 401.4500 - mse: 401.4500 - mae: 16.7780 - val_loss: 309.3112 - val_mse: 309.3112 - val_mae: 15.1808\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 400.9422 - mse: 400.9422 - mae: 16.7626 - val_loss: 308.7382 - val_mse: 308.7382 - val_mae: 15.1635\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 400.4268 - mse: 400.4268 - mae: 16.7478 - val_loss: 308.1666 - val_mse: 308.1666 - val_mae: 15.1469\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 399.8715 - mse: 399.8715 - mae: 16.7320 - val_loss: 307.4985 - val_mse: 307.4985 - val_mae: 15.1284\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 399.3376 - mse: 399.3376 - mae: 16.7163 - val_loss: 306.9720 - val_mse: 306.9720 - val_mae: 15.1121\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 398.8066 - mse: 398.8066 - mae: 16.7003 - val_loss: 306.4140 - val_mse: 306.4140 - val_mae: 15.0951\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 398.2843 - mse: 398.2843 - mae: 16.6842 - val_loss: 305.8746 - val_mse: 305.8746 - val_mae: 15.0784\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 397.7142 - mse: 397.7142 - mae: 16.6673 - val_loss: 305.3384 - val_mse: 305.3384 - val_mae: 15.0616\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 397.1511 - mse: 397.1511 - mae: 16.6506 - val_loss: 304.8116 - val_mse: 304.8116 - val_mae: 15.0449\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 396.5959 - mse: 396.5959 - mae: 16.6345 - val_loss: 304.2177 - val_mse: 304.2177 - val_mae: 15.0269\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 396.0421 - mse: 396.0421 - mae: 16.6177 - val_loss: 303.6561 - val_mse: 303.6561 - val_mae: 15.0092\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 395.4637 - mse: 395.4637 - mae: 16.6002 - val_loss: 303.1401 - val_mse: 303.1401 - val_mae: 14.9925\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 394.9108 - mse: 394.9108 - mae: 16.5836 - val_loss: 302.5778 - val_mse: 302.5778 - val_mae: 14.9745\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 394.3696 - mse: 394.3696 - mae: 16.5673 - val_loss: 302.0167 - val_mse: 302.0167 - val_mae: 14.9569\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 393.8233 - mse: 393.8233 - mae: 16.5507 - val_loss: 301.4469 - val_mse: 301.4469 - val_mae: 14.9386\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 393.2995 - mse: 393.2995 - mae: 16.5350 - val_loss: 300.9195 - val_mse: 300.9195 - val_mae: 14.9213\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 392.7712 - mse: 392.7712 - mae: 16.5192 - val_loss: 300.4362 - val_mse: 300.4362 - val_mae: 14.9051\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 392.2655 - mse: 392.2655 - mae: 16.5041 - val_loss: 299.9116 - val_mse: 299.9116 - val_mae: 14.8881\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 391.7715 - mse: 391.7715 - mae: 16.4897 - val_loss: 299.3958 - val_mse: 299.3958 - val_mae: 14.8717\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 391.3030 - mse: 391.3030 - mae: 16.4756 - val_loss: 298.9578 - val_mse: 298.9578 - val_mae: 14.8567\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 390.8080 - mse: 390.8080 - mae: 16.4619 - val_loss: 298.4354 - val_mse: 298.4354 - val_mae: 14.8409\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 390.3423 - mse: 390.3423 - mae: 16.4481 - val_loss: 297.9570 - val_mse: 297.9570 - val_mae: 14.8259\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpo_sjcayb/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 1780.0554 - mse: 1780.0554 - mae: 31.7064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 16ms/step - loss: 1678.1180 - mse: 1678.1180 - mae: 30.9211 - val_loss: 1593.3563 - val_mse: 1593.3563 - val_mae: 29.5794\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1508.4209 - mse: 1508.4209 - mae: 29.7388 - val_loss: 1420.3787 - val_mse: 1420.3787 - val_mae: 28.3569\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1357.6788 - mse: 1357.6788 - mae: 28.6030 - val_loss: 1270.4639 - val_mse: 1270.4639 - val_mae: 27.2182\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1229.0787 - mse: 1229.0787 - mae: 27.5628 - val_loss: 1143.0096 - val_mse: 1143.0096 - val_mae: 26.1826\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1119.1488 - mse: 1119.1488 - mae: 26.6157 - val_loss: 1034.0864 - val_mse: 1034.0864 - val_mae: 25.2362\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1024.7120 - mse: 1024.7120 - mae: 25.7490 - val_loss: 939.3663 - val_mse: 939.3663 - val_mae: 24.3575\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 943.3193 - mse: 943.3193 - mae: 24.9436 - val_loss: 856.9167 - val_mse: 856.9167 - val_mae: 23.5409\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 873.2953 - mse: 873.2953 - mae: 24.2001 - val_loss: 785.6516 - val_mse: 785.6516 - val_mae: 22.7884\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 813.2099 - mse: 813.2099 - mae: 23.5326 - val_loss: 725.3796 - val_mse: 725.3796 - val_mae: 22.1108\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 762.1553 - mse: 762.1553 - mae: 22.9181 - val_loss: 673.1114 - val_mse: 673.1114 - val_mae: 21.4845\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 718.1314 - mse: 718.1314 - mae: 22.3625 - val_loss: 627.6700 - val_mse: 627.6700 - val_mae: 20.9076\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 680.2430 - mse: 680.2430 - mae: 21.8553 - val_loss: 588.6901 - val_mse: 588.6901 - val_mae: 20.3804\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 647.5421 - mse: 647.5421 - mae: 21.3941 - val_loss: 554.6265 - val_mse: 554.6265 - val_mae: 19.8932\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 619.4288 - mse: 619.4288 - mae: 20.9687 - val_loss: 524.7801 - val_mse: 524.7801 - val_mae: 19.4430\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 594.8962 - mse: 594.8962 - mae: 20.5853 - val_loss: 499.0891 - val_mse: 499.0891 - val_mae: 19.0301\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 573.7617 - mse: 573.7617 - mae: 20.2402 - val_loss: 476.9216 - val_mse: 476.9216 - val_mae: 18.6536\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 555.5238 - mse: 555.5238 - mae: 19.9240 - val_loss: 457.1616 - val_mse: 457.1616 - val_mae: 18.3039\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 539.5601 - mse: 539.5601 - mae: 19.6354 - val_loss: 439.9439 - val_mse: 439.9439 - val_mae: 17.9843\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 525.6979 - mse: 525.6979 - mae: 19.3773 - val_loss: 424.8491 - val_mse: 424.8491 - val_mae: 17.6931\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 513.5138 - mse: 513.5138 - mae: 19.1419 - val_loss: 411.5383 - val_mse: 411.5383 - val_mae: 17.4241\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 502.8153 - mse: 502.8153 - mae: 18.9272 - val_loss: 399.7368 - val_mse: 399.7368 - val_mae: 17.1760\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 493.3950 - mse: 493.3950 - mae: 18.7332 - val_loss: 389.1897 - val_mse: 389.1897 - val_mae: 16.9462\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 485.0640 - mse: 485.0640 - mae: 18.5556 - val_loss: 379.9367 - val_mse: 379.9367 - val_mae: 16.7379\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 477.8454 - mse: 477.8454 - mae: 18.3949 - val_loss: 371.7658 - val_mse: 371.7658 - val_mae: 16.5506\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 471.4756 - mse: 471.4756 - mae: 18.2518 - val_loss: 364.5710 - val_mse: 364.5710 - val_mae: 16.3796\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 465.7701 - mse: 465.7701 - mae: 18.1186 - val_loss: 357.9413 - val_mse: 357.9413 - val_mae: 16.2197\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 460.6336 - mse: 460.6336 - mae: 17.9984 - val_loss: 352.1112 - val_mse: 352.1112 - val_mae: 16.0746\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 456.0641 - mse: 456.0641 - mae: 17.8881 - val_loss: 346.5567 - val_mse: 346.5567 - val_mae: 15.9374\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 451.8401 - mse: 451.8401 - mae: 17.7833 - val_loss: 341.7339 - val_mse: 341.7339 - val_mae: 15.8130\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 448.0934 - mse: 448.0934 - mae: 17.6888 - val_loss: 337.3131 - val_mse: 337.3131 - val_mae: 15.6978\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 444.7101 - mse: 444.7101 - mae: 17.5997 - val_loss: 333.3554 - val_mse: 333.3554 - val_mae: 15.5909\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 441.6113 - mse: 441.6113 - mae: 17.5186 - val_loss: 329.6461 - val_mse: 329.6461 - val_mae: 15.4911\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 438.7722 - mse: 438.7722 - mae: 17.4424 - val_loss: 326.3761 - val_mse: 326.3761 - val_mae: 15.4031\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 436.1349 - mse: 436.1349 - mae: 17.3728 - val_loss: 323.2360 - val_mse: 323.2360 - val_mae: 15.3184\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 433.7108 - mse: 433.7108 - mae: 17.3085 - val_loss: 320.2487 - val_mse: 320.2487 - val_mae: 15.2389\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 431.4307 - mse: 431.4307 - mae: 17.2455 - val_loss: 317.7242 - val_mse: 317.7242 - val_mae: 15.1663\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 429.3224 - mse: 429.3224 - mae: 17.1875 - val_loss: 315.2117 - val_mse: 315.2117 - val_mae: 15.0972\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 427.3469 - mse: 427.3469 - mae: 17.1336 - val_loss: 312.8401 - val_mse: 312.8401 - val_mae: 15.0322\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 425.5061 - mse: 425.5061 - mae: 17.0827 - val_loss: 310.6552 - val_mse: 310.6552 - val_mae: 14.9717\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 423.7820 - mse: 423.7820 - mae: 17.0344 - val_loss: 308.5207 - val_mse: 308.5207 - val_mae: 14.9139\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 422.1193 - mse: 422.1193 - mae: 16.9894 - val_loss: 306.6377 - val_mse: 306.6377 - val_mae: 14.8623\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 420.5694 - mse: 420.5694 - mae: 16.9469 - val_loss: 304.8333 - val_mse: 304.8333 - val_mae: 14.8116\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 419.1389 - mse: 419.1389 - mae: 16.9062 - val_loss: 303.1105 - val_mse: 303.1105 - val_mae: 14.7631\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 417.7912 - mse: 417.7912 - mae: 16.8697 - val_loss: 301.5020 - val_mse: 301.5020 - val_mae: 14.7181\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 416.5027 - mse: 416.5027 - mae: 16.8344 - val_loss: 300.0304 - val_mse: 300.0304 - val_mae: 14.6761\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 415.1971 - mse: 415.1971 - mae: 16.7978 - val_loss: 298.5119 - val_mse: 298.5119 - val_mae: 14.6331\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 413.9718 - mse: 413.9718 - mae: 16.7638 - val_loss: 297.0902 - val_mse: 297.0902 - val_mae: 14.5933\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 412.8053 - mse: 412.8053 - mae: 16.7334 - val_loss: 295.6872 - val_mse: 295.6872 - val_mae: 14.5555\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 411.6855 - mse: 411.6855 - mae: 16.7031 - val_loss: 294.4114 - val_mse: 294.4114 - val_mae: 14.5207\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 410.5803 - mse: 410.5803 - mae: 16.6746 - val_loss: 293.2364 - val_mse: 293.2364 - val_mae: 14.4890\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 409.4984 - mse: 409.4984 - mae: 16.6469 - val_loss: 291.9608 - val_mse: 291.9608 - val_mae: 14.4543\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 408.4272 - mse: 408.4272 - mae: 16.6191 - val_loss: 290.7175 - val_mse: 290.7175 - val_mae: 14.4208\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 407.3539 - mse: 407.3539 - mae: 16.5924 - val_loss: 289.5092 - val_mse: 289.5092 - val_mae: 14.3882\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 406.3223 - mse: 406.3223 - mae: 16.5661 - val_loss: 288.3956 - val_mse: 288.3956 - val_mae: 14.3569\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 405.2789 - mse: 405.2789 - mae: 16.5389 - val_loss: 287.2339 - val_mse: 287.2339 - val_mae: 14.3253\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 404.2761 - mse: 404.2761 - mae: 16.5138 - val_loss: 286.1345 - val_mse: 286.1345 - val_mae: 14.2955\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 403.2592 - mse: 403.2592 - mae: 16.4874 - val_loss: 285.0527 - val_mse: 285.0527 - val_mae: 14.2648\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 402.2325 - mse: 402.2325 - mae: 16.4615 - val_loss: 283.9757 - val_mse: 283.9757 - val_mae: 14.2346\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 401.2119 - mse: 401.2119 - mae: 16.4354 - val_loss: 282.9287 - val_mse: 282.9287 - val_mae: 14.2045\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 400.1944 - mse: 400.1944 - mae: 16.4089 - val_loss: 281.9403 - val_mse: 281.9403 - val_mae: 14.1755\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 399.1936 - mse: 399.1936 - mae: 16.3834 - val_loss: 280.9131 - val_mse: 280.9131 - val_mae: 14.1454\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 398.1394 - mse: 398.1394 - mae: 16.3562 - val_loss: 279.8893 - val_mse: 279.8893 - val_mae: 14.1153\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 397.1094 - mse: 397.1094 - mae: 16.3290 - val_loss: 278.8383 - val_mse: 278.8383 - val_mae: 14.0840\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 396.0738 - mse: 396.0738 - mae: 16.3015 - val_loss: 277.8516 - val_mse: 277.8516 - val_mae: 14.0539\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 395.0724 - mse: 395.0724 - mae: 16.2735 - val_loss: 276.8300 - val_mse: 276.8300 - val_mae: 14.0237\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 394.0500 - mse: 394.0500 - mae: 16.2465 - val_loss: 275.8365 - val_mse: 275.8365 - val_mae: 13.9942\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 393.0825 - mse: 393.0825 - mae: 16.2192 - val_loss: 274.9836 - val_mse: 274.9836 - val_mae: 13.9669\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 392.1555 - mse: 392.1555 - mae: 16.1932 - val_loss: 274.0828 - val_mse: 274.0828 - val_mae: 13.9384\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 391.2649 - mse: 391.2649 - mae: 16.1673 - val_loss: 273.2610 - val_mse: 273.2610 - val_mae: 13.9115\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 390.3913 - mse: 390.3913 - mae: 16.1415 - val_loss: 272.4427 - val_mse: 272.4427 - val_mae: 13.8846\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 389.5583 - mse: 389.5583 - mae: 16.1178 - val_loss: 271.6598 - val_mse: 271.6598 - val_mae: 13.8588\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 388.7432 - mse: 388.7432 - mae: 16.0934 - val_loss: 270.8297 - val_mse: 270.8297 - val_mae: 13.8317\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 387.9279 - mse: 387.9279 - mae: 16.0691 - val_loss: 270.0863 - val_mse: 270.0863 - val_mae: 13.8059\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 387.1156 - mse: 387.1156 - mae: 16.0451 - val_loss: 269.3323 - val_mse: 269.3323 - val_mae: 13.7808\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 386.3368 - mse: 386.3368 - mae: 16.0230 - val_loss: 268.6349 - val_mse: 268.6349 - val_mae: 13.7576\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 385.6101 - mse: 385.6101 - mae: 16.0008 - val_loss: 267.9018 - val_mse: 267.9018 - val_mae: 13.7334\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 384.8679 - mse: 384.8679 - mae: 15.9797 - val_loss: 267.2177 - val_mse: 267.2177 - val_mae: 13.7119\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 384.1869 - mse: 384.1869 - mae: 15.9601 - val_loss: 266.6607 - val_mse: 266.6607 - val_mae: 13.6939\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 383.4906 - mse: 383.4906 - mae: 15.9399 - val_loss: 265.9832 - val_mse: 265.9832 - val_mae: 13.6730\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 382.8668 - mse: 382.8668 - mae: 15.9207 - val_loss: 265.4169 - val_mse: 265.4169 - val_mae: 13.6544\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 382.2357 - mse: 382.2357 - mae: 15.9037 - val_loss: 264.8477 - val_mse: 264.8477 - val_mae: 13.6363\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 381.6096 - mse: 381.6096 - mae: 15.8851 - val_loss: 264.2194 - val_mse: 264.2194 - val_mae: 13.6164\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 380.9950 - mse: 380.9950 - mae: 15.8687 - val_loss: 263.6606 - val_mse: 263.6606 - val_mae: 13.5989\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 380.3976 - mse: 380.3976 - mae: 15.8521 - val_loss: 263.1288 - val_mse: 263.1288 - val_mae: 13.5826\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 379.8098 - mse: 379.8098 - mae: 15.8358 - val_loss: 262.5750 - val_mse: 262.5750 - val_mae: 13.5654\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 379.2334 - mse: 379.2334 - mae: 15.8207 - val_loss: 262.0420 - val_mse: 262.0420 - val_mae: 13.5500\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 378.6734 - mse: 378.6734 - mae: 15.8064 - val_loss: 261.5026 - val_mse: 261.5026 - val_mae: 13.5341\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 378.1049 - mse: 378.1049 - mae: 15.7916 - val_loss: 261.0074 - val_mse: 261.0074 - val_mae: 13.5191\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 377.5530 - mse: 377.5530 - mae: 15.7782 - val_loss: 260.4856 - val_mse: 260.4856 - val_mae: 13.5038\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 376.9745 - mse: 376.9745 - mae: 15.7629 - val_loss: 259.9877 - val_mse: 259.9877 - val_mae: 13.4884\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 376.4097 - mse: 376.4097 - mae: 15.7473 - val_loss: 259.4141 - val_mse: 259.4141 - val_mae: 13.4714\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 375.8186 - mse: 375.8186 - mae: 15.7314 - val_loss: 258.8825 - val_mse: 258.8825 - val_mae: 13.4554\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 375.2624 - mse: 375.2624 - mae: 15.7164 - val_loss: 258.3541 - val_mse: 258.3541 - val_mae: 13.4394\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 374.6908 - mse: 374.6908 - mae: 15.7007 - val_loss: 257.8861 - val_mse: 257.8861 - val_mae: 13.4250\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 374.1154 - mse: 374.1154 - mae: 15.6871 - val_loss: 257.3692 - val_mse: 257.3692 - val_mae: 13.4098\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 373.5365 - mse: 373.5365 - mae: 15.6718 - val_loss: 256.7973 - val_mse: 256.7973 - val_mae: 13.3933\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 372.9716 - mse: 372.9716 - mae: 15.6562 - val_loss: 256.3160 - val_mse: 256.3160 - val_mae: 13.3783\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 372.4337 - mse: 372.4337 - mae: 15.6431 - val_loss: 255.8465 - val_mse: 255.8465 - val_mae: 13.3650\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 371.8866 - mse: 371.8866 - mae: 15.6287 - val_loss: 255.3306 - val_mse: 255.3306 - val_mae: 13.3500\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 371.3459 - mse: 371.3459 - mae: 15.6135 - val_loss: 254.8431 - val_mse: 254.8431 - val_mae: 13.3351\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmps42wbpqt/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 1692.5212 - mse: 1692.5212 - mae: 30.9364"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 1757.8635 - mse: 1757.8635 - mae: 30.9067 - val_loss: 1698.8835 - val_mse: 1698.8835 - val_mae: 30.4489\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1576.3945 - mse: 1576.3945 - mae: 29.6676 - val_loss: 1516.4299 - val_mse: 1516.4299 - val_mae: 29.1884\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1415.1876 - mse: 1415.1876 - mae: 28.4739 - val_loss: 1355.9962 - val_mse: 1355.9962 - val_mae: 28.0006\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1275.7986 - mse: 1275.7986 - mae: 27.3763 - val_loss: 1220.3293 - val_mse: 1220.3293 - val_mae: 26.9229\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1156.5338 - mse: 1156.5338 - mae: 26.3682 - val_loss: 1103.1154 - val_mse: 1103.1154 - val_mae: 25.9305\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1053.8441 - mse: 1053.8441 - mae: 25.4563 - val_loss: 1003.3868 - val_mse: 1003.3868 - val_mae: 25.0243\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 966.0760 - mse: 966.0760 - mae: 24.6162 - val_loss: 917.4717 - val_mse: 917.4717 - val_mae: 24.1999\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 890.5617 - mse: 890.5617 - mae: 23.8508 - val_loss: 844.0629 - val_mse: 844.0629 - val_mae: 23.4528\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 825.7064 - mse: 825.7064 - mae: 23.1531 - val_loss: 780.2659 - val_mse: 780.2659 - val_mae: 22.7605\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 769.3536 - mse: 769.3536 - mae: 22.5078 - val_loss: 724.9153 - val_mse: 724.9153 - val_mae: 22.1271\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 721.1388 - mse: 721.1388 - mae: 21.9290 - val_loss: 677.9886 - val_mse: 677.9886 - val_mae: 21.5613\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 679.6462 - mse: 679.6462 - mae: 21.4022 - val_loss: 637.6508 - val_mse: 637.6508 - val_mae: 21.0493\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 643.7507 - mse: 643.7507 - mae: 20.9151 - val_loss: 602.0048 - val_mse: 602.0048 - val_mae: 20.5743\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 612.4786 - mse: 612.4786 - mae: 20.4819 - val_loss: 571.9260 - val_mse: 571.9260 - val_mae: 20.1537\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 585.7205 - mse: 585.7205 - mae: 20.0938 - val_loss: 545.9967 - val_mse: 545.9967 - val_mae: 19.7717\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 562.5720 - mse: 562.5720 - mae: 19.7438 - val_loss: 523.7058 - val_mse: 523.7058 - val_mae: 19.4253\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 542.3763 - mse: 542.3763 - mae: 19.4191 - val_loss: 504.2051 - val_mse: 504.2051 - val_mae: 19.1064\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 524.6747 - mse: 524.6747 - mae: 19.1227 - val_loss: 486.9306 - val_mse: 486.9306 - val_mae: 18.8120\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 509.1538 - mse: 509.1538 - mae: 18.8559 - val_loss: 472.0213 - val_mse: 472.0213 - val_mae: 18.5502\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 495.6268 - mse: 495.6268 - mae: 18.6167 - val_loss: 458.9926 - val_mse: 458.9926 - val_mae: 18.3122\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 483.8781 - mse: 483.8781 - mae: 18.4019 - val_loss: 447.9908 - val_mse: 447.9908 - val_mae: 18.1015\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 473.6837 - mse: 473.6837 - mae: 18.2082 - val_loss: 438.1408 - val_mse: 438.1408 - val_mae: 17.9078\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 464.6458 - mse: 464.6458 - mae: 18.0351 - val_loss: 429.7586 - val_mse: 429.7586 - val_mae: 17.7330\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 456.7619 - mse: 456.7619 - mae: 17.8793 - val_loss: 422.4232 - val_mse: 422.4232 - val_mae: 17.5741\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 449.8139 - mse: 449.8139 - mae: 17.7386 - val_loss: 416.0236 - val_mse: 416.0236 - val_mae: 17.4303\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 443.6191 - mse: 443.6191 - mae: 17.6121 - val_loss: 410.3707 - val_mse: 410.3707 - val_mae: 17.2995\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 438.0732 - mse: 438.0732 - mae: 17.4971 - val_loss: 405.2537 - val_mse: 405.2537 - val_mae: 17.1790\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 433.0918 - mse: 433.0918 - mae: 17.3937 - val_loss: 400.7480 - val_mse: 400.7480 - val_mae: 17.0704\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 428.6822 - mse: 428.6822 - mae: 17.2984 - val_loss: 396.8990 - val_mse: 396.8990 - val_mae: 16.9752\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 424.7717 - mse: 424.7717 - mae: 17.2128 - val_loss: 393.2933 - val_mse: 393.2933 - val_mae: 16.8857\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 421.1606 - mse: 421.1606 - mae: 17.1335 - val_loss: 390.1035 - val_mse: 390.1035 - val_mae: 16.8049\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 417.8922 - mse: 417.8922 - mae: 17.0611 - val_loss: 387.2669 - val_mse: 387.2669 - val_mae: 16.7305\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 414.9243 - mse: 414.9243 - mae: 16.9936 - val_loss: 384.7637 - val_mse: 384.7637 - val_mae: 16.6636\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 412.2249 - mse: 412.2249 - mae: 16.9321 - val_loss: 382.4222 - val_mse: 382.4222 - val_mae: 16.6008\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 409.7773 - mse: 409.7773 - mae: 16.8766 - val_loss: 380.3624 - val_mse: 380.3624 - val_mae: 16.5448\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 407.5107 - mse: 407.5107 - mae: 16.8246 - val_loss: 378.5433 - val_mse: 378.5433 - val_mae: 16.4942\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 405.3986 - mse: 405.3986 - mae: 16.7767 - val_loss: 376.7789 - val_mse: 376.7789 - val_mae: 16.4452\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 403.3889 - mse: 403.3889 - mae: 16.7328 - val_loss: 375.1556 - val_mse: 375.1556 - val_mae: 16.3994\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 401.5667 - mse: 401.5667 - mae: 16.6921 - val_loss: 373.6622 - val_mse: 373.6622 - val_mae: 16.3566\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 399.8504 - mse: 399.8504 - mae: 16.6547 - val_loss: 372.3347 - val_mse: 372.3347 - val_mae: 16.3181\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 398.2696 - mse: 398.2696 - mae: 16.6186 - val_loss: 371.1672 - val_mse: 371.1672 - val_mae: 16.2847\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 396.7993 - mse: 396.7993 - mae: 16.5853 - val_loss: 370.0373 - val_mse: 370.0373 - val_mae: 16.2521\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 395.4216 - mse: 395.4216 - mae: 16.5557 - val_loss: 369.0294 - val_mse: 369.0294 - val_mae: 16.2239\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 394.1141 - mse: 394.1141 - mae: 16.5271 - val_loss: 368.0610 - val_mse: 368.0610 - val_mae: 16.1959\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 392.8648 - mse: 392.8648 - mae: 16.4987 - val_loss: 367.1505 - val_mse: 367.1505 - val_mae: 16.1690\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 391.6917 - mse: 391.6917 - mae: 16.4726 - val_loss: 366.3176 - val_mse: 366.3176 - val_mae: 16.1443\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 390.5699 - mse: 390.5699 - mae: 16.4491 - val_loss: 365.4932 - val_mse: 365.4932 - val_mae: 16.1202\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 389.4603 - mse: 389.4603 - mae: 16.4239 - val_loss: 364.6811 - val_mse: 364.6811 - val_mae: 16.0957\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 388.4165 - mse: 388.4165 - mae: 16.4014 - val_loss: 363.9408 - val_mse: 363.9408 - val_mae: 16.0737\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 387.4062 - mse: 387.4062 - mae: 16.3777 - val_loss: 363.2401 - val_mse: 363.2401 - val_mae: 16.0536\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 386.4071 - mse: 386.4071 - mae: 16.3559 - val_loss: 362.5707 - val_mse: 362.5707 - val_mae: 16.0339\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 385.4815 - mse: 385.4815 - mae: 16.3348 - val_loss: 361.8805 - val_mse: 361.8805 - val_mae: 16.0133\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 384.5093 - mse: 384.5093 - mae: 16.3132 - val_loss: 361.2382 - val_mse: 361.2382 - val_mae: 15.9943\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 383.5722 - mse: 383.5722 - mae: 16.2927 - val_loss: 360.6224 - val_mse: 360.6224 - val_mae: 15.9774\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 382.6555 - mse: 382.6555 - mae: 16.2717 - val_loss: 360.0380 - val_mse: 360.0380 - val_mae: 15.9610\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 381.7858 - mse: 381.7858 - mae: 16.2524 - val_loss: 359.4653 - val_mse: 359.4653 - val_mae: 15.9448\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 380.9264 - mse: 380.9264 - mae: 16.2322 - val_loss: 358.8953 - val_mse: 358.8953 - val_mae: 15.9282\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 380.0875 - mse: 380.0875 - mae: 16.2124 - val_loss: 358.3486 - val_mse: 358.3486 - val_mae: 15.9125\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 379.2801 - mse: 379.2801 - mae: 16.1925 - val_loss: 357.8139 - val_mse: 357.8139 - val_mae: 15.8972\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 378.4485 - mse: 378.4485 - mae: 16.1729 - val_loss: 357.2800 - val_mse: 357.2800 - val_mae: 15.8823\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 377.6660 - mse: 377.6660 - mae: 16.1549 - val_loss: 356.7436 - val_mse: 356.7436 - val_mae: 15.8668\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 376.8435 - mse: 376.8435 - mae: 16.1343 - val_loss: 356.2559 - val_mse: 356.2559 - val_mae: 15.8525\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 376.0489 - mse: 376.0489 - mae: 16.1152 - val_loss: 355.7269 - val_mse: 355.7269 - val_mae: 15.8373\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 375.2821 - mse: 375.2821 - mae: 16.0978 - val_loss: 355.1802 - val_mse: 355.1802 - val_mae: 15.8214\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 374.5031 - mse: 374.5031 - mae: 16.0787 - val_loss: 354.6692 - val_mse: 354.6692 - val_mae: 15.8074\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 373.7218 - mse: 373.7218 - mae: 16.0597 - val_loss: 354.1567 - val_mse: 354.1567 - val_mae: 15.7925\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 372.9824 - mse: 372.9824 - mae: 16.0417 - val_loss: 353.6707 - val_mse: 353.6707 - val_mae: 15.7787\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 372.2403 - mse: 372.2403 - mae: 16.0236 - val_loss: 353.1725 - val_mse: 353.1725 - val_mae: 15.7646\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 371.5109 - mse: 371.5109 - mae: 16.0053 - val_loss: 352.6763 - val_mse: 352.6763 - val_mae: 15.7496\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 370.7675 - mse: 370.7675 - mae: 15.9869 - val_loss: 352.1926 - val_mse: 352.1926 - val_mae: 15.7353\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 370.0439 - mse: 370.0439 - mae: 15.9685 - val_loss: 351.7147 - val_mse: 351.7147 - val_mae: 15.7206\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 369.3051 - mse: 369.3051 - mae: 15.9494 - val_loss: 351.2599 - val_mse: 351.2599 - val_mae: 15.7070\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 368.6051 - mse: 368.6051 - mae: 15.9313 - val_loss: 350.7887 - val_mse: 350.7887 - val_mae: 15.6922\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 367.8694 - mse: 367.8694 - mae: 15.9126 - val_loss: 350.3156 - val_mse: 350.3156 - val_mae: 15.6786\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 367.1589 - mse: 367.1589 - mae: 15.8946 - val_loss: 349.8396 - val_mse: 349.8396 - val_mae: 15.6640\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 366.4535 - mse: 366.4535 - mae: 15.8756 - val_loss: 349.3877 - val_mse: 349.3877 - val_mae: 15.6502\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 365.7366 - mse: 365.7366 - mae: 15.8568 - val_loss: 348.9236 - val_mse: 348.9236 - val_mae: 15.6366\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 365.0230 - mse: 365.0230 - mae: 15.8387 - val_loss: 348.4500 - val_mse: 348.4500 - val_mae: 15.6225\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 364.3253 - mse: 364.3253 - mae: 15.8203 - val_loss: 347.9720 - val_mse: 347.9720 - val_mae: 15.6082\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 363.5901 - mse: 363.5901 - mae: 15.8012 - val_loss: 347.4708 - val_mse: 347.4708 - val_mae: 15.5928\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 362.8912 - mse: 362.8912 - mae: 15.7829 - val_loss: 346.9860 - val_mse: 346.9860 - val_mae: 15.5780\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 362.1736 - mse: 362.1736 - mae: 15.7634 - val_loss: 346.5208 - val_mse: 346.5208 - val_mae: 15.5645\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 361.4477 - mse: 361.4477 - mae: 15.7435 - val_loss: 346.0274 - val_mse: 346.0274 - val_mae: 15.5501\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 360.7291 - mse: 360.7291 - mae: 15.7240 - val_loss: 345.5361 - val_mse: 345.5361 - val_mae: 15.5357\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 360.0487 - mse: 360.0487 - mae: 15.7053 - val_loss: 345.0311 - val_mse: 345.0311 - val_mae: 15.5208\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 359.3254 - mse: 359.3254 - mae: 15.6857 - val_loss: 344.5168 - val_mse: 344.5168 - val_mae: 15.5051\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 358.6165 - mse: 358.6165 - mae: 15.6660 - val_loss: 343.9914 - val_mse: 343.9914 - val_mae: 15.4886\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 357.8871 - mse: 357.8871 - mae: 15.6456 - val_loss: 343.4879 - val_mse: 343.4879 - val_mae: 15.4731\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 357.1636 - mse: 357.1636 - mae: 15.6257 - val_loss: 342.9561 - val_mse: 342.9561 - val_mae: 15.4569\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 356.4236 - mse: 356.4236 - mae: 15.6052 - val_loss: 342.3994 - val_mse: 342.3994 - val_mae: 15.4396\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 355.6703 - mse: 355.6703 - mae: 15.5841 - val_loss: 341.8456 - val_mse: 341.8456 - val_mae: 15.4231\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 354.9380 - mse: 354.9380 - mae: 15.5635 - val_loss: 341.3168 - val_mse: 341.3168 - val_mae: 15.4069\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 354.2372 - mse: 354.2372 - mae: 15.5429 - val_loss: 340.7881 - val_mse: 340.7881 - val_mae: 15.3907\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 353.5687 - mse: 353.5687 - mae: 15.5223 - val_loss: 340.2750 - val_mse: 340.2750 - val_mae: 15.3739\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 352.8959 - mse: 352.8959 - mae: 15.5024 - val_loss: 339.7509 - val_mse: 339.7509 - val_mae: 15.3563\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 352.2464 - mse: 352.2464 - mae: 15.4821 - val_loss: 339.2554 - val_mse: 339.2554 - val_mae: 15.3401\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 351.6122 - mse: 351.6122 - mae: 15.4638 - val_loss: 338.7887 - val_mse: 338.7887 - val_mae: 15.3244\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 351.0141 - mse: 351.0141 - mae: 15.4444 - val_loss: 338.3363 - val_mse: 338.3363 - val_mae: 15.3090\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 350.4254 - mse: 350.4254 - mae: 15.4259 - val_loss: 337.8882 - val_mse: 337.8882 - val_mae: 15.2933\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 349.8705 - mse: 349.8705 - mae: 15.4087 - val_loss: 337.4786 - val_mse: 337.4786 - val_mae: 15.2788\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpv0hh6h7l/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp6db51f49/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 23560.8262 - mse: 23560.8262 - mae: 73.5918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpbzok_t86/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpps2vf0x_/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 491.7212 - mse: 491.7212 - mae: 19.1721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 398.2989 - mse: 398.2989 - mae: 16.6180 - val_loss: 244.5097 - val_mse: 244.5097 - val_mae: 13.0452\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 326.8260 - mse: 326.8260 - mae: 14.3934 - val_loss: 218.1825 - val_mse: 218.1825 - val_mae: 11.9653\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 300.5413 - mse: 300.5413 - mae: 13.5274 - val_loss: 202.7513 - val_mse: 202.7513 - val_mae: 11.2557\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.6072 - mse: 289.6072 - mae: 13.0791 - val_loss: 201.0031 - val_mse: 201.0031 - val_mae: 11.1786\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 277.0369 - mse: 277.0369 - mae: 12.6970 - val_loss: 180.6614 - val_mse: 180.6614 - val_mae: 10.2430\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 264.3648 - mse: 264.3648 - mae: 12.2561 - val_loss: 174.0012 - val_mse: 174.0012 - val_mae: 9.9499\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 258.3103 - mse: 258.3103 - mae: 11.9858 - val_loss: 170.5130 - val_mse: 170.5130 - val_mae: 9.7437\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 255.4835 - mse: 255.4835 - mae: 11.9546 - val_loss: 170.0457 - val_mse: 170.0457 - val_mae: 9.9262\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 244.6149 - mse: 244.6149 - mae: 11.5648 - val_loss: 173.4147 - val_mse: 173.4147 - val_mae: 10.2546\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 242.2166 - mse: 242.2166 - mae: 11.5754 - val_loss: 159.9606 - val_mse: 159.9606 - val_mae: 9.4162\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 239.1008 - mse: 239.1008 - mae: 11.4233 - val_loss: 157.5915 - val_mse: 157.5915 - val_mae: 9.2314\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 237.4974 - mse: 237.4974 - mae: 11.3646 - val_loss: 153.2047 - val_mse: 153.2047 - val_mae: 9.0461\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 232.9458 - mse: 232.9458 - mae: 11.1840 - val_loss: 168.4380 - val_mse: 168.4380 - val_mae: 10.1372\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 236.9328 - mse: 236.9328 - mae: 11.5003 - val_loss: 150.8835 - val_mse: 150.8835 - val_mae: 8.8698\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 230.1633 - mse: 230.1633 - mae: 11.1157 - val_loss: 151.7017 - val_mse: 151.7017 - val_mae: 8.9792\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 231.9660 - mse: 231.9660 - mae: 11.2058 - val_loss: 150.0790 - val_mse: 150.0790 - val_mae: 8.7708\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 231.5852 - mse: 231.5852 - mae: 11.0589 - val_loss: 153.4012 - val_mse: 153.4012 - val_mae: 9.1355\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 230.9111 - mse: 230.9111 - mae: 11.1308 - val_loss: 149.5518 - val_mse: 149.5518 - val_mae: 8.7736\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 226.8690 - mse: 226.8690 - mae: 10.9296 - val_loss: 150.2152 - val_mse: 150.2152 - val_mae: 9.0196\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 225.7252 - mse: 225.7252 - mae: 10.9328 - val_loss: 156.0465 - val_mse: 156.0465 - val_mae: 9.4770\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 224.1910 - mse: 224.1910 - mae: 10.9690 - val_loss: 147.0988 - val_mse: 147.0988 - val_mae: 8.7691\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 225.7951 - mse: 225.7951 - mae: 10.9484 - val_loss: 145.8627 - val_mse: 145.8627 - val_mae: 8.5865\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 223.6655 - mse: 223.6655 - mae: 10.7898 - val_loss: 150.5398 - val_mse: 150.5398 - val_mae: 9.0706\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 223.8709 - mse: 223.8709 - mae: 10.8543 - val_loss: 147.9520 - val_mse: 147.9520 - val_mae: 8.8556\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 223.7972 - mse: 223.7972 - mae: 10.8900 - val_loss: 146.0548 - val_mse: 146.0548 - val_mae: 8.6693\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 223.1954 - mse: 223.1954 - mae: 10.8008 - val_loss: 151.5301 - val_mse: 151.5301 - val_mae: 9.1571\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 225.0099 - mse: 225.0099 - mae: 10.9448 - val_loss: 144.2456 - val_mse: 144.2456 - val_mae: 8.4754\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 226.7514 - mse: 226.7514 - mae: 10.9572 - val_loss: 144.1542 - val_mse: 144.1542 - val_mae: 8.4828\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 222.8581 - mse: 222.8581 - mae: 10.7681 - val_loss: 148.8619 - val_mse: 148.8619 - val_mae: 8.9447\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 222.5049 - mse: 222.5049 - mae: 10.8307 - val_loss: 143.5895 - val_mse: 143.5895 - val_mae: 8.4169\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 222.1582 - mse: 222.1582 - mae: 10.7791 - val_loss: 144.0419 - val_mse: 144.0419 - val_mae: 8.4284\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 221.3289 - mse: 221.3289 - mae: 10.6344 - val_loss: 149.3671 - val_mse: 149.3671 - val_mae: 9.1334\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 220.6343 - mse: 220.6343 - mae: 10.7904 - val_loss: 144.3214 - val_mse: 144.3214 - val_mae: 8.7216\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 214.7125 - mse: 214.7125 - mae: 10.5567 - val_loss: 141.9926 - val_mse: 141.9926 - val_mae: 8.6870\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.6752 - mse: 213.6752 - mae: 10.5375 - val_loss: 139.0970 - val_mse: 139.0970 - val_mae: 8.4184\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 214.1870 - mse: 214.1870 - mae: 10.5313 - val_loss: 137.6333 - val_mse: 137.6333 - val_mae: 8.2670\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.0509 - mse: 213.0509 - mae: 10.4942 - val_loss: 136.3166 - val_mse: 136.3166 - val_mae: 8.1288\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.9419 - mse: 213.9419 - mae: 10.4830 - val_loss: 138.6817 - val_mse: 138.6817 - val_mae: 8.2832\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.8092 - mse: 213.8092 - mae: 10.4674 - val_loss: 138.7404 - val_mse: 138.7404 - val_mae: 8.4287\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.2378 - mse: 213.2378 - mae: 10.5586 - val_loss: 140.3309 - val_mse: 140.3309 - val_mae: 8.5734\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.0320 - mse: 213.0320 - mae: 10.5401 - val_loss: 136.0902 - val_mse: 136.0902 - val_mae: 8.1686\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 211.6626 - mse: 211.6626 - mae: 10.4152 - val_loss: 136.0693 - val_mse: 136.0693 - val_mae: 8.1804\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 211.9027 - mse: 211.9027 - mae: 10.4153 - val_loss: 134.3875 - val_mse: 134.3875 - val_mae: 8.0768\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 209.0641 - mse: 209.0641 - mae: 10.3164 - val_loss: 133.7255 - val_mse: 133.7255 - val_mae: 8.0998\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 207.6998 - mse: 207.6998 - mae: 10.3693 - val_loss: 134.2056 - val_mse: 134.2056 - val_mae: 8.1616\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 207.5346 - mse: 207.5346 - mae: 10.3169 - val_loss: 132.8733 - val_mse: 132.8733 - val_mae: 7.9752\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 209.7036 - mse: 209.7036 - mae: 10.3314 - val_loss: 133.0312 - val_mse: 133.0312 - val_mae: 8.0472\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 207.2383 - mse: 207.2383 - mae: 10.2389 - val_loss: 140.0173 - val_mse: 140.0173 - val_mae: 8.7208\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.4612 - mse: 206.4612 - mae: 10.2654 - val_loss: 145.1836 - val_mse: 145.1836 - val_mae: 9.1448\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.8262 - mse: 206.8262 - mae: 10.3217 - val_loss: 129.6029 - val_mse: 129.6029 - val_mae: 7.9277\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 201.7729 - mse: 201.7729 - mae: 10.0655 - val_loss: 131.9049 - val_mse: 131.9049 - val_mae: 8.2955\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 203.5165 - mse: 203.5165 - mae: 10.2599 - val_loss: 126.2925 - val_mse: 126.2925 - val_mae: 7.7766\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 198.8886 - mse: 198.8886 - mae: 10.0205 - val_loss: 129.8503 - val_mse: 129.8503 - val_mae: 8.2637\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 194.8446 - mse: 194.8446 - mae: 10.0460 - val_loss: 122.8573 - val_mse: 122.8573 - val_mae: 7.6635\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 194.3749 - mse: 194.3749 - mae: 9.8365 - val_loss: 127.0889 - val_mse: 127.0889 - val_mae: 8.2013\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.9880 - mse: 191.9880 - mae: 9.8342 - val_loss: 119.9349 - val_mse: 119.9349 - val_mae: 7.5744\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.6500 - mse: 190.6500 - mae: 9.7203 - val_loss: 119.3659 - val_mse: 119.3659 - val_mae: 7.5121\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 192.7287 - mse: 192.7287 - mae: 9.7648 - val_loss: 127.8213 - val_mse: 127.8213 - val_mae: 8.2789\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 192.3530 - mse: 192.3530 - mae: 9.8489 - val_loss: 119.8306 - val_mse: 119.8306 - val_mae: 7.4775\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.2908 - mse: 191.2908 - mae: 9.7715 - val_loss: 120.2539 - val_mse: 120.2539 - val_mae: 7.6192\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 195.1129 - mse: 195.1129 - mae: 9.8617 - val_loss: 119.7370 - val_mse: 119.7370 - val_mae: 7.4826\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.3791 - mse: 190.3791 - mae: 9.7138 - val_loss: 118.4248 - val_mse: 118.4248 - val_mae: 7.3677\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.7466 - mse: 190.7466 - mae: 9.7354 - val_loss: 118.6367 - val_mse: 118.6367 - val_mae: 7.4677\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.8588 - mse: 191.8588 - mae: 9.6777 - val_loss: 129.8674 - val_mse: 129.8674 - val_mae: 8.4481\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 194.5200 - mse: 194.5200 - mae: 10.0166 - val_loss: 118.3149 - val_mse: 118.3149 - val_mae: 7.3865\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.8553 - mse: 191.8553 - mae: 9.6844 - val_loss: 117.7044 - val_mse: 117.7044 - val_mae: 7.3730\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.6404 - mse: 191.6404 - mae: 9.7228 - val_loss: 118.5195 - val_mse: 118.5195 - val_mae: 7.4774\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 189.3729 - mse: 189.3729 - mae: 9.6803 - val_loss: 121.5541 - val_mse: 121.5541 - val_mae: 7.7885\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.2855 - mse: 191.2855 - mae: 9.8456 - val_loss: 119.4870 - val_mse: 119.4870 - val_mae: 7.5898\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 187.5145 - mse: 187.5145 - mae: 9.6466 - val_loss: 116.8735 - val_mse: 116.8735 - val_mae: 7.3429\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 188.7643 - mse: 188.7643 - mae: 9.6748 - val_loss: 117.0280 - val_mse: 117.0280 - val_mae: 7.3488\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 186.9338 - mse: 186.9338 - mae: 9.5492 - val_loss: 115.5742 - val_mse: 115.5742 - val_mae: 7.3011\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 186.4257 - mse: 186.4257 - mae: 9.6295 - val_loss: 115.5805 - val_mse: 115.5805 - val_mae: 7.2687\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 186.5485 - mse: 186.5485 - mae: 9.5263 - val_loss: 131.0623 - val_mse: 131.0623 - val_mae: 8.6402\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 189.9646 - mse: 189.9646 - mae: 9.8062 - val_loss: 114.3781 - val_mse: 114.3781 - val_mae: 7.2097\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 186.8667 - mse: 186.8667 - mae: 9.5626 - val_loss: 118.0342 - val_mse: 118.0342 - val_mae: 7.4709\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 189.8323 - mse: 189.8323 - mae: 9.6495 - val_loss: 118.0263 - val_mse: 118.0263 - val_mae: 7.6141\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 184.6730 - mse: 184.6730 - mae: 9.5140 - val_loss: 116.7474 - val_mse: 116.7474 - val_mae: 7.4972\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 185.7038 - mse: 185.7038 - mae: 9.6033 - val_loss: 114.0992 - val_mse: 114.0992 - val_mae: 7.1927\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 187.5744 - mse: 187.5744 - mae: 9.6237 - val_loss: 125.3671 - val_mse: 125.3671 - val_mae: 8.2472\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 186.6460 - mse: 186.6460 - mae: 9.7100 - val_loss: 113.4831 - val_mse: 113.4831 - val_mae: 7.1187\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 185.0433 - mse: 185.0433 - mae: 9.4537 - val_loss: 116.4505 - val_mse: 116.4505 - val_mae: 7.4650\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 185.4100 - mse: 185.4100 - mae: 9.5002 - val_loss: 114.8147 - val_mse: 114.8147 - val_mae: 7.2978\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 184.8330 - mse: 184.8330 - mae: 9.5471 - val_loss: 115.9231 - val_mse: 115.9231 - val_mae: 7.4134\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 184.2173 - mse: 184.2173 - mae: 9.4981 - val_loss: 114.6184 - val_mse: 114.6184 - val_mae: 7.2431\n",
      "Epoch 85: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpjgye7xb0/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 633.0361 - mse: 633.0361 - mae: 21.1581"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 413.6639 - mse: 413.6639 - mae: 16.5251 - val_loss: 219.7115 - val_mse: 219.7115 - val_mae: 11.9776\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 327.0015 - mse: 327.0015 - mae: 14.1950 - val_loss: 201.4756 - val_mse: 201.4756 - val_mae: 11.1142\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 309.7898 - mse: 309.7898 - mae: 13.6190 - val_loss: 192.9888 - val_mse: 192.9888 - val_mae: 10.9096\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.2460 - mse: 292.2460 - mae: 13.1195 - val_loss: 181.1324 - val_mse: 181.1324 - val_mae: 10.4571\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 279.7018 - mse: 279.7018 - mae: 12.7396 - val_loss: 168.7302 - val_mse: 168.7302 - val_mae: 9.8021\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 271.8239 - mse: 271.8239 - mae: 12.4571 - val_loss: 159.2719 - val_mse: 159.2719 - val_mae: 9.2759\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 263.7213 - mse: 263.7213 - mae: 12.1341 - val_loss: 156.9282 - val_mse: 156.9282 - val_mae: 9.2750\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 260.7734 - mse: 260.7734 - mae: 12.0602 - val_loss: 154.8630 - val_mse: 154.8630 - val_mae: 9.3847\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 245.6343 - mse: 245.6343 - mae: 11.6361 - val_loss: 160.2881 - val_mse: 160.2881 - val_mae: 9.7964\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 244.2972 - mse: 244.2972 - mae: 11.6262 - val_loss: 161.9114 - val_mse: 161.9114 - val_mae: 9.8875\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 245.5214 - mse: 245.5214 - mae: 11.7208 - val_loss: 143.9904 - val_mse: 143.9904 - val_mae: 8.6134\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 243.3059 - mse: 243.3059 - mae: 11.5045 - val_loss: 152.8152 - val_mse: 152.8152 - val_mae: 9.3265\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 242.3449 - mse: 242.3449 - mae: 11.5485 - val_loss: 151.7726 - val_mse: 151.7726 - val_mae: 9.2434\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 244.2971 - mse: 244.2971 - mae: 11.6571 - val_loss: 143.7964 - val_mse: 143.7964 - val_mae: 8.6134\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 239.9658 - mse: 239.9658 - mae: 11.4064 - val_loss: 144.3406 - val_mse: 144.3406 - val_mae: 8.6869\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 240.1591 - mse: 240.1591 - mae: 11.3960 - val_loss: 143.0898 - val_mse: 143.0898 - val_mae: 8.5963\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 241.2696 - mse: 241.2696 - mae: 11.3873 - val_loss: 154.6160 - val_mse: 154.6160 - val_mae: 9.4841\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 242.6085 - mse: 242.6085 - mae: 11.5062 - val_loss: 143.4119 - val_mse: 143.4119 - val_mae: 8.6311\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 238.0856 - mse: 238.0856 - mae: 11.2726 - val_loss: 146.0777 - val_mse: 146.0777 - val_mae: 8.8697\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 239.0547 - mse: 239.0547 - mae: 11.3588 - val_loss: 148.3870 - val_mse: 148.3870 - val_mae: 9.0689\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 234.6467 - mse: 234.6467 - mae: 11.2786 - val_loss: 139.1964 - val_mse: 139.1964 - val_mae: 8.4445\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 234.7885 - mse: 234.7885 - mae: 11.1671 - val_loss: 137.3345 - val_mse: 137.3345 - val_mae: 8.2635\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 232.4434 - mse: 232.4434 - mae: 11.0481 - val_loss: 142.7292 - val_mse: 142.7292 - val_mae: 8.7590\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 233.4886 - mse: 233.4886 - mae: 11.1014 - val_loss: 144.3639 - val_mse: 144.3639 - val_mae: 8.8909\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 232.2557 - mse: 232.2557 - mae: 11.1466 - val_loss: 138.3941 - val_mse: 138.3941 - val_mae: 8.4035\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 233.8925 - mse: 233.8925 - mae: 11.1331 - val_loss: 146.4604 - val_mse: 146.4604 - val_mae: 9.0806\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 230.0815 - mse: 230.0815 - mae: 11.0536 - val_loss: 131.7269 - val_mse: 131.7269 - val_mae: 7.9975\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 227.2747 - mse: 227.2747 - mae: 10.9612 - val_loss: 127.1878 - val_mse: 127.1878 - val_mae: 7.8136\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 220.5242 - mse: 220.5242 - mae: 10.7109 - val_loss: 126.9329 - val_mse: 126.9329 - val_mae: 7.8298\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 219.3737 - mse: 219.3737 - mae: 10.6785 - val_loss: 126.7742 - val_mse: 126.7742 - val_mae: 7.8457\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 220.4992 - mse: 220.4992 - mae: 10.7749 - val_loss: 128.1899 - val_mse: 128.1899 - val_mae: 7.8543\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 223.0650 - mse: 223.0650 - mae: 10.6086 - val_loss: 143.0718 - val_mse: 143.0718 - val_mae: 9.1971\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 215.8413 - mse: 215.8413 - mae: 10.6618 - val_loss: 128.5214 - val_mse: 128.5214 - val_mae: 8.2115\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 211.0233 - mse: 211.0233 - mae: 10.4436 - val_loss: 129.6196 - val_mse: 129.6196 - val_mae: 8.3940\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 208.5515 - mse: 208.5515 - mae: 10.4092 - val_loss: 123.1451 - val_mse: 123.1451 - val_mae: 7.9134\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 207.7570 - mse: 207.7570 - mae: 10.3606 - val_loss: 118.8542 - val_mse: 118.8542 - val_mae: 7.7111\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 204.1690 - mse: 204.1690 - mae: 10.3062 - val_loss: 112.0818 - val_mse: 112.0818 - val_mae: 7.1737\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 203.0910 - mse: 203.0910 - mae: 10.1700 - val_loss: 110.7668 - val_mse: 110.7668 - val_mae: 7.0988\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 195.9894 - mse: 195.9894 - mae: 10.0159 - val_loss: 107.4210 - val_mse: 107.4210 - val_mae: 7.1292\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.2438 - mse: 191.2438 - mae: 9.8377 - val_loss: 113.2426 - val_mse: 113.2426 - val_mae: 7.6618\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 192.8694 - mse: 192.8694 - mae: 9.9272 - val_loss: 105.2499 - val_mse: 105.2499 - val_mae: 6.9665\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.9229 - mse: 190.9229 - mae: 9.8559 - val_loss: 103.8304 - val_mse: 103.8304 - val_mae: 6.8032\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.9771 - mse: 190.9771 - mae: 9.8462 - val_loss: 105.1743 - val_mse: 105.1743 - val_mae: 6.9475\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 189.7781 - mse: 189.7781 - mae: 9.7308 - val_loss: 107.7365 - val_mse: 107.7365 - val_mae: 7.2185\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 189.5310 - mse: 189.5310 - mae: 9.8994 - val_loss: 103.3204 - val_mse: 103.3204 - val_mae: 6.7594\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.4975 - mse: 190.4975 - mae: 9.8077 - val_loss: 104.6762 - val_mse: 104.6762 - val_mae: 6.8246\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.8106 - mse: 191.8106 - mae: 9.7984 - val_loss: 103.4809 - val_mse: 103.4809 - val_mae: 6.7708\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.9573 - mse: 190.9573 - mae: 9.7253 - val_loss: 111.0685 - val_mse: 111.0685 - val_mae: 7.5043\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 188.3820 - mse: 188.3820 - mae: 9.6201 - val_loss: 144.8516 - val_mse: 144.8516 - val_mae: 9.7839\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 193.9598 - mse: 193.9598 - mae: 10.0506 - val_loss: 102.6864 - val_mse: 102.6864 - val_mae: 6.6778\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.3855 - mse: 191.3855 - mae: 9.7547 - val_loss: 106.0584 - val_mse: 106.0584 - val_mae: 7.1615\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 193.8707 - mse: 193.8707 - mae: 9.9881 - val_loss: 103.7221 - val_mse: 103.7221 - val_mae: 7.0352\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 183.8073 - mse: 183.8073 - mae: 9.5343 - val_loss: 103.4710 - val_mse: 103.4710 - val_mae: 7.0168\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 184.8084 - mse: 184.8084 - mae: 9.6520 - val_loss: 99.3910 - val_mse: 99.3910 - val_mae: 6.6030\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 183.8949 - mse: 183.8949 - mae: 9.4838 - val_loss: 115.1224 - val_mse: 115.1224 - val_mae: 7.9500\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 187.7328 - mse: 187.7328 - mae: 9.7421 - val_loss: 102.0062 - val_mse: 102.0062 - val_mae: 6.8829\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 183.5994 - mse: 183.5994 - mae: 9.5134 - val_loss: 100.8773 - val_mse: 100.8773 - val_mae: 6.7770\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 182.9346 - mse: 182.9346 - mae: 9.4478 - val_loss: 103.6689 - val_mse: 103.6689 - val_mae: 7.0626\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 184.5136 - mse: 184.5136 - mae: 9.5263 - val_loss: 106.8414 - val_mse: 106.8414 - val_mae: 7.3171\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 186.2162 - mse: 186.2162 - mae: 9.5578 - val_loss: 108.6476 - val_mse: 108.6476 - val_mae: 7.4763\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 187.9159 - mse: 187.9159 - mae: 9.7414 - val_loss: 98.6498 - val_mse: 98.6498 - val_mae: 6.5288\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 181.6717 - mse: 181.6717 - mae: 9.4418 - val_loss: 98.0160 - val_mse: 98.0160 - val_mae: 6.4655\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 180.7344 - mse: 180.7344 - mae: 9.4698 - val_loss: 95.6197 - val_mse: 95.6197 - val_mae: 6.4497\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.7746 - mse: 177.7746 - mae: 9.3240 - val_loss: 97.5403 - val_mse: 97.5403 - val_mae: 6.6834\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.5122 - mse: 177.5122 - mae: 9.2918 - val_loss: 91.9284 - val_mse: 91.9284 - val_mae: 6.3902\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 171.6734 - mse: 171.6734 - mae: 9.0836 - val_loss: 95.9973 - val_mse: 95.9973 - val_mae: 6.8783\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 170.2919 - mse: 170.2919 - mae: 9.1906 - val_loss: 88.8743 - val_mse: 88.8743 - val_mae: 6.1086\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 172.8812 - mse: 172.8812 - mae: 9.1595 - val_loss: 97.4402 - val_mse: 97.4402 - val_mae: 6.9713\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 173.0398 - mse: 173.0398 - mae: 9.2765 - val_loss: 93.1074 - val_mse: 93.1074 - val_mae: 6.6119\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 169.0188 - mse: 169.0188 - mae: 9.0716 - val_loss: 88.3367 - val_mse: 88.3367 - val_mae: 6.0798\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 170.4220 - mse: 170.4220 - mae: 9.1441 - val_loss: 88.9904 - val_mse: 88.9904 - val_mae: 6.1022\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 170.1707 - mse: 170.1707 - mae: 9.0122 - val_loss: 88.3800 - val_mse: 88.3800 - val_mae: 6.1248\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 171.7705 - mse: 171.7705 - mae: 9.1424 - val_loss: 88.7297 - val_mse: 88.7297 - val_mae: 6.1691\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 167.9848 - mse: 167.9848 - mae: 8.9522 - val_loss: 114.8672 - val_mse: 114.8672 - val_mae: 8.3219\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 175.1379 - mse: 175.1379 - mae: 9.4478 - val_loss: 93.6078 - val_mse: 93.6078 - val_mae: 6.6663\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 170.3079 - mse: 170.3079 - mae: 9.1284 - val_loss: 90.7263 - val_mse: 90.7263 - val_mae: 6.2733\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 171.1923 - mse: 171.1923 - mae: 9.0107 - val_loss: 115.4708 - val_mse: 115.4708 - val_mae: 8.3429\n",
      "Epoch 77: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpkj6mv2ax/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 564.5652 - mse: 564.5652 - mae: 19.8666"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 362.0409 - mse: 362.0409 - mae: 15.6303 - val_loss: 285.2763 - val_mse: 285.2763 - val_mae: 13.6539\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 271.3849 - mse: 271.3849 - mae: 13.0041 - val_loss: 249.7887 - val_mse: 249.7887 - val_mae: 12.3109\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 248.1537 - mse: 248.1537 - mae: 12.1122 - val_loss: 233.5656 - val_mse: 233.5656 - val_mae: 11.7236\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 232.9130 - mse: 232.9130 - mae: 11.4879 - val_loss: 224.3523 - val_mse: 224.3523 - val_mae: 11.3798\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 227.4507 - mse: 227.4507 - mae: 11.2558 - val_loss: 220.2210 - val_mse: 220.2210 - val_mae: 11.1134\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 224.5702 - mse: 224.5702 - mae: 11.1098 - val_loss: 215.9381 - val_mse: 215.9381 - val_mae: 10.9636\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 223.1787 - mse: 223.1787 - mae: 11.1277 - val_loss: 212.4510 - val_mse: 212.4510 - val_mae: 10.9834\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 217.9292 - mse: 217.9292 - mae: 10.9732 - val_loss: 209.4935 - val_mse: 209.4935 - val_mae: 10.6844\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.7543 - mse: 213.7543 - mae: 10.7294 - val_loss: 204.8408 - val_mse: 204.8408 - val_mae: 10.5930\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 211.6960 - mse: 211.6960 - mae: 10.6820 - val_loss: 203.2677 - val_mse: 203.2677 - val_mae: 10.4753\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 210.3657 - mse: 210.3657 - mae: 10.5960 - val_loss: 201.1768 - val_mse: 201.1768 - val_mae: 10.4542\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 210.6522 - mse: 210.6522 - mae: 10.6270 - val_loss: 203.4982 - val_mse: 203.4982 - val_mae: 10.4981\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 209.3850 - mse: 209.3850 - mae: 10.5045 - val_loss: 199.0717 - val_mse: 199.0717 - val_mae: 10.3491\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 208.5502 - mse: 208.5502 - mae: 10.4835 - val_loss: 197.3714 - val_mse: 197.3714 - val_mae: 10.3110\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 205.3069 - mse: 205.3069 - mae: 10.4184 - val_loss: 194.1073 - val_mse: 194.1073 - val_mae: 10.1311\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 202.7166 - mse: 202.7166 - mae: 10.2706 - val_loss: 190.6660 - val_mse: 190.6660 - val_mae: 10.1476\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 198.3380 - mse: 198.3380 - mae: 10.2349 - val_loss: 189.6071 - val_mse: 189.6071 - val_mae: 9.9650\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 198.4194 - mse: 198.4194 - mae: 10.1262 - val_loss: 185.9859 - val_mse: 185.9859 - val_mae: 9.8580\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 196.8274 - mse: 196.8274 - mae: 10.1261 - val_loss: 185.6962 - val_mse: 185.6962 - val_mae: 9.8303\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 194.5310 - mse: 194.5310 - mae: 9.9816 - val_loss: 182.5018 - val_mse: 182.5018 - val_mae: 9.7378\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 194.5561 - mse: 194.5561 - mae: 10.0487 - val_loss: 182.4893 - val_mse: 182.4893 - val_mae: 9.7341\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 192.0755 - mse: 192.0755 - mae: 9.8423 - val_loss: 180.7077 - val_mse: 180.7077 - val_mae: 9.7405\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.0263 - mse: 191.0263 - mae: 9.9340 - val_loss: 180.3087 - val_mse: 180.3087 - val_mae: 9.6576\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 188.8614 - mse: 188.8614 - mae: 9.7261 - val_loss: 179.1151 - val_mse: 179.1151 - val_mae: 9.8130\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 187.4385 - mse: 187.4385 - mae: 9.7879 - val_loss: 173.7824 - val_mse: 173.7824 - val_mae: 9.4489\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 185.1685 - mse: 185.1685 - mae: 9.6181 - val_loss: 174.7237 - val_mse: 174.7237 - val_mae: 9.5847\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 181.8681 - mse: 181.8681 - mae: 9.6511 - val_loss: 166.7640 - val_mse: 166.7640 - val_mae: 9.2003\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 178.0405 - mse: 178.0405 - mae: 9.4795 - val_loss: 164.7834 - val_mse: 164.7834 - val_mae: 9.1535\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 175.5239 - mse: 175.5239 - mae: 9.3284 - val_loss: 164.0898 - val_mse: 164.0898 - val_mae: 9.0939\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 175.3606 - mse: 175.3606 - mae: 9.2970 - val_loss: 164.3412 - val_mse: 164.3412 - val_mae: 9.1767\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.2822 - mse: 174.2822 - mae: 9.2850 - val_loss: 162.1459 - val_mse: 162.1459 - val_mae: 9.0347\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.5078 - mse: 174.5078 - mae: 9.2632 - val_loss: 164.9366 - val_mse: 164.9366 - val_mae: 9.2816\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.8902 - mse: 174.8902 - mae: 9.3795 - val_loss: 165.5876 - val_mse: 165.5876 - val_mae: 9.1753\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 175.1367 - mse: 175.1367 - mae: 9.3092 - val_loss: 160.6046 - val_mse: 160.6046 - val_mae: 8.9702\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 172.4206 - mse: 172.4206 - mae: 9.1786 - val_loss: 160.2574 - val_mse: 160.2574 - val_mae: 8.9496\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 173.7360 - mse: 173.7360 - mae: 9.2282 - val_loss: 162.8897 - val_mse: 162.8897 - val_mae: 9.0514\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.6541 - mse: 174.6541 - mae: 9.3315 - val_loss: 163.3012 - val_mse: 163.3012 - val_mae: 9.0601\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.5786 - mse: 174.5786 - mae: 9.2702 - val_loss: 162.3164 - val_mse: 162.3164 - val_mae: 9.0295\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 173.2095 - mse: 173.2095 - mae: 9.1930 - val_loss: 158.9086 - val_mse: 158.9086 - val_mae: 8.9436\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 171.3026 - mse: 171.3026 - mae: 9.1634 - val_loss: 158.9302 - val_mse: 158.9302 - val_mae: 8.8465\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 172.0974 - mse: 172.0974 - mae: 9.2013 - val_loss: 160.4977 - val_mse: 160.4977 - val_mae: 8.9277\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 170.5327 - mse: 170.5327 - mae: 8.9562 - val_loss: 186.3574 - val_mse: 186.3574 - val_mae: 10.5259\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.8956 - mse: 177.8956 - mae: 9.5322 - val_loss: 158.0989 - val_mse: 158.0989 - val_mae: 8.8296\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 171.6494 - mse: 171.6494 - mae: 9.1443 - val_loss: 159.1361 - val_mse: 159.1361 - val_mae: 8.8552\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 171.1454 - mse: 171.1454 - mae: 9.0546 - val_loss: 156.9206 - val_mse: 156.9206 - val_mae: 8.8256\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 168.1048 - mse: 168.1048 - mae: 9.0023 - val_loss: 156.4019 - val_mse: 156.4019 - val_mae: 8.8000\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 167.7873 - mse: 167.7873 - mae: 9.0145 - val_loss: 158.1042 - val_mse: 158.1042 - val_mae: 8.8721\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 167.5656 - mse: 167.5656 - mae: 9.0260 - val_loss: 156.6199 - val_mse: 156.6199 - val_mae: 8.7894\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 167.1522 - mse: 167.1522 - mae: 8.9871 - val_loss: 153.7654 - val_mse: 153.7654 - val_mae: 8.6657\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 166.8450 - mse: 166.8450 - mae: 8.9739 - val_loss: 156.0268 - val_mse: 156.0268 - val_mae: 8.8155\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 165.2726 - mse: 165.2726 - mae: 8.9647 - val_loss: 153.2108 - val_mse: 153.2108 - val_mae: 8.6425\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 166.5556 - mse: 166.5556 - mae: 8.8884 - val_loss: 151.4319 - val_mse: 151.4319 - val_mae: 8.5772\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 163.6253 - mse: 163.6253 - mae: 8.8256 - val_loss: 154.5112 - val_mse: 154.5112 - val_mae: 8.8304\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 162.9617 - mse: 162.9617 - mae: 8.8360 - val_loss: 154.4801 - val_mse: 154.4801 - val_mae: 8.8340\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 165.3621 - mse: 165.3621 - mae: 9.0090 - val_loss: 151.2812 - val_mse: 151.2812 - val_mae: 8.5769\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 165.0360 - mse: 165.0360 - mae: 8.8963 - val_loss: 151.8775 - val_mse: 151.8775 - val_mae: 8.6112\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 163.3899 - mse: 163.3899 - mae: 8.8188 - val_loss: 149.5244 - val_mse: 149.5244 - val_mae: 8.4889\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 163.2858 - mse: 163.2858 - mae: 8.8176 - val_loss: 149.5041 - val_mse: 149.5041 - val_mae: 8.4852\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 162.2483 - mse: 162.2483 - mae: 8.8163 - val_loss: 150.0056 - val_mse: 150.0056 - val_mae: 8.6500\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 160.4100 - mse: 160.4100 - mae: 8.8689 - val_loss: 152.3033 - val_mse: 152.3033 - val_mae: 8.7832\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 159.0145 - mse: 159.0145 - mae: 8.5792 - val_loss: 145.6422 - val_mse: 145.6422 - val_mae: 8.3866\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 157.7063 - mse: 157.7063 - mae: 8.6943 - val_loss: 144.4501 - val_mse: 144.4501 - val_mae: 8.2955\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 157.9020 - mse: 157.9020 - mae: 8.6503 - val_loss: 147.1847 - val_mse: 147.1847 - val_mae: 8.4617\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 158.1971 - mse: 158.1971 - mae: 8.6027 - val_loss: 143.9677 - val_mse: 143.9677 - val_mae: 8.3069\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 157.9412 - mse: 157.9412 - mae: 8.6175 - val_loss: 144.8599 - val_mse: 144.8599 - val_mae: 8.2860\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 159.9490 - mse: 159.9490 - mae: 8.6627 - val_loss: 145.4227 - val_mse: 145.4227 - val_mae: 8.3569\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 157.9456 - mse: 157.9456 - mae: 8.6363 - val_loss: 145.9267 - val_mse: 145.9267 - val_mae: 8.3939\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 159.5818 - mse: 159.5818 - mae: 8.7574 - val_loss: 146.7213 - val_mse: 146.7213 - val_mae: 8.4342\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 156.1975 - mse: 156.1975 - mae: 8.5113 - val_loss: 144.7631 - val_mse: 144.7631 - val_mae: 8.3661\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 156.6275 - mse: 156.6275 - mae: 8.6312 - val_loss: 148.8770 - val_mse: 148.8770 - val_mae: 8.5916\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 156.8904 - mse: 156.8904 - mae: 8.5570 - val_loss: 143.8822 - val_mse: 143.8822 - val_mae: 8.2249\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 157.0543 - mse: 157.0543 - mae: 8.5634 - val_loss: 144.7795 - val_mse: 144.7795 - val_mae: 8.3121\n",
      "Epoch 72: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp6da3t6s5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 6282.3159 - mse: 6282.3159 - mae: 48.8201 - val_loss: 6145.1851 - val_mse: 6145.1851 - val_mae: 47.1226\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6281.2695 - mse: 6281.2695 - mae: 48.8171 - val_loss: 6144.1460 - val_mse: 6144.1460 - val_mae: 47.1195\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6280.2163 - mse: 6280.2163 - mae: 48.8140 - val_loss: 6143.1089 - val_mse: 6143.1089 - val_mae: 47.1165\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6279.1685 - mse: 6279.1685 - mae: 48.8110 - val_loss: 6142.0728 - val_mse: 6142.0728 - val_mae: 47.1135\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6278.1299 - mse: 6278.1299 - mae: 48.8079 - val_loss: 6141.0361 - val_mse: 6141.0361 - val_mae: 47.1104\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6277.0723 - mse: 6277.0723 - mae: 48.8049 - val_loss: 6140.0049 - val_mse: 6140.0049 - val_mae: 47.1074\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6276.0337 - mse: 6276.0337 - mae: 48.8018 - val_loss: 6138.9678 - val_mse: 6138.9678 - val_mae: 47.1044\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6274.9766 - mse: 6274.9766 - mae: 48.7988 - val_loss: 6137.9365 - val_mse: 6137.9365 - val_mae: 47.1014\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6273.9380 - mse: 6273.9380 - mae: 48.7958 - val_loss: 6136.8994 - val_mse: 6136.8994 - val_mae: 47.0983\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6272.8892 - mse: 6272.8892 - mae: 48.7927 - val_loss: 6135.8691 - val_mse: 6135.8691 - val_mae: 47.0953\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6271.8525 - mse: 6271.8525 - mae: 48.7897 - val_loss: 6134.8369 - val_mse: 6134.8369 - val_mae: 47.0923\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6270.8071 - mse: 6270.8066 - mae: 48.7867 - val_loss: 6133.8125 - val_mse: 6133.8125 - val_mae: 47.0893\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6269.7656 - mse: 6269.7656 - mae: 48.7836 - val_loss: 6132.7861 - val_mse: 6132.7861 - val_mae: 47.0863\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6268.7305 - mse: 6268.7305 - mae: 48.7806 - val_loss: 6131.7539 - val_mse: 6131.7539 - val_mae: 47.0833\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6267.6782 - mse: 6267.6782 - mae: 48.7776 - val_loss: 6130.7300 - val_mse: 6130.7300 - val_mae: 47.0803\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6266.6353 - mse: 6266.6353 - mae: 48.7745 - val_loss: 6129.6973 - val_mse: 6129.6973 - val_mae: 47.0772\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6265.5986 - mse: 6265.5986 - mae: 48.7715 - val_loss: 6128.6597 - val_mse: 6128.6597 - val_mae: 47.0742\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6264.5552 - mse: 6264.5552 - mae: 48.7685 - val_loss: 6127.6245 - val_mse: 6127.6245 - val_mae: 47.0712\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6263.5088 - mse: 6263.5088 - mae: 48.7654 - val_loss: 6126.6006 - val_mse: 6126.6006 - val_mae: 47.0682\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6262.4683 - mse: 6262.4683 - mae: 48.7624 - val_loss: 6125.5801 - val_mse: 6125.5801 - val_mae: 47.0652\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6261.4277 - mse: 6261.4277 - mae: 48.7594 - val_loss: 6124.5532 - val_mse: 6124.5532 - val_mae: 47.0622\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6260.3911 - mse: 6260.3911 - mae: 48.7564 - val_loss: 6123.5176 - val_mse: 6123.5176 - val_mae: 47.0592\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6259.3535 - mse: 6259.3535 - mae: 48.7533 - val_loss: 6122.4814 - val_mse: 6122.4814 - val_mae: 47.0561\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6258.3032 - mse: 6258.3032 - mae: 48.7503 - val_loss: 6121.4570 - val_mse: 6121.4570 - val_mae: 47.0531\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6257.2632 - mse: 6257.2632 - mae: 48.7472 - val_loss: 6120.4282 - val_mse: 6120.4282 - val_mae: 47.0501\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6256.2227 - mse: 6256.2227 - mae: 48.7442 - val_loss: 6119.4004 - val_mse: 6119.4004 - val_mae: 47.0471\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6255.1812 - mse: 6255.1812 - mae: 48.7412 - val_loss: 6118.3750 - val_mse: 6118.3750 - val_mae: 47.0441\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6254.1426 - mse: 6254.1426 - mae: 48.7382 - val_loss: 6117.3496 - val_mse: 6117.3496 - val_mae: 47.0411\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6253.1167 - mse: 6253.1167 - mae: 48.7352 - val_loss: 6116.3154 - val_mse: 6116.3154 - val_mae: 47.0381\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6252.0596 - mse: 6252.0596 - mae: 48.7321 - val_loss: 6115.2979 - val_mse: 6115.2979 - val_mae: 47.0351\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6251.0283 - mse: 6251.0283 - mae: 48.7291 - val_loss: 6114.2710 - val_mse: 6114.2710 - val_mae: 47.0321\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6249.9912 - mse: 6249.9912 - mae: 48.7261 - val_loss: 6113.2349 - val_mse: 6113.2349 - val_mae: 47.0290\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6248.9536 - mse: 6248.9536 - mae: 48.7230 - val_loss: 6112.2031 - val_mse: 6112.2031 - val_mae: 47.0260\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6247.9053 - mse: 6247.9053 - mae: 48.7200 - val_loss: 6111.1924 - val_mse: 6111.1924 - val_mae: 47.0230\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6246.8779 - mse: 6246.8779 - mae: 48.7170 - val_loss: 6110.1738 - val_mse: 6110.1738 - val_mae: 47.0201\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6245.8398 - mse: 6245.8398 - mae: 48.7140 - val_loss: 6109.1562 - val_mse: 6109.1562 - val_mae: 47.0171\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6244.8198 - mse: 6244.8198 - mae: 48.7110 - val_loss: 6108.1206 - val_mse: 6108.1206 - val_mae: 47.0140\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6243.7656 - mse: 6243.7656 - mae: 48.7079 - val_loss: 6107.0962 - val_mse: 6107.0962 - val_mae: 47.0110\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6242.7363 - mse: 6242.7363 - mae: 48.7049 - val_loss: 6106.0649 - val_mse: 6106.0649 - val_mae: 47.0080\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6241.6914 - mse: 6241.6914 - mae: 48.7019 - val_loss: 6105.0415 - val_mse: 6105.0415 - val_mae: 47.0050\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6240.6514 - mse: 6240.6514 - mae: 48.6988 - val_loss: 6104.0161 - val_mse: 6104.0161 - val_mae: 47.0020\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6239.6094 - mse: 6239.6094 - mae: 48.6958 - val_loss: 6103.0000 - val_mse: 6103.0000 - val_mae: 46.9990\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6238.5757 - mse: 6238.5757 - mae: 48.6928 - val_loss: 6101.9810 - val_mse: 6101.9810 - val_mae: 46.9960\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6237.5493 - mse: 6237.5493 - mae: 48.6898 - val_loss: 6100.9478 - val_mse: 6100.9478 - val_mae: 46.9930\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6236.5015 - mse: 6236.5015 - mae: 48.6868 - val_loss: 6099.9312 - val_mse: 6099.9312 - val_mae: 46.9900\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6235.4741 - mse: 6235.4741 - mae: 48.6837 - val_loss: 6098.9038 - val_mse: 6098.9038 - val_mae: 46.9870\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6234.4395 - mse: 6234.4395 - mae: 48.6807 - val_loss: 6097.8726 - val_mse: 6097.8726 - val_mae: 46.9840\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6233.3931 - mse: 6233.3931 - mae: 48.6777 - val_loss: 6096.8564 - val_mse: 6096.8564 - val_mae: 46.9810\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6232.3643 - mse: 6232.3643 - mae: 48.6747 - val_loss: 6095.8330 - val_mse: 6095.8330 - val_mae: 46.9780\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6231.3267 - mse: 6231.3267 - mae: 48.6716 - val_loss: 6094.8091 - val_mse: 6094.8091 - val_mae: 46.9750\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6230.2905 - mse: 6230.2905 - mae: 48.6686 - val_loss: 6093.7881 - val_mse: 6093.7881 - val_mae: 46.9720\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6229.2568 - mse: 6229.2568 - mae: 48.6656 - val_loss: 6092.7603 - val_mse: 6092.7603 - val_mae: 46.9690\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6228.2183 - mse: 6228.2183 - mae: 48.6626 - val_loss: 6091.7373 - val_mse: 6091.7373 - val_mae: 46.9660\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6227.1797 - mse: 6227.1797 - mae: 48.6596 - val_loss: 6090.7158 - val_mse: 6090.7158 - val_mae: 46.9630\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6226.1528 - mse: 6226.1528 - mae: 48.6565 - val_loss: 6089.6831 - val_mse: 6089.6831 - val_mae: 46.9599\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6225.1016 - mse: 6225.1016 - mae: 48.6535 - val_loss: 6088.6646 - val_mse: 6088.6646 - val_mae: 46.9569\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6224.0723 - mse: 6224.0723 - mae: 48.6505 - val_loss: 6087.6343 - val_mse: 6087.6343 - val_mae: 46.9539\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6223.0308 - mse: 6223.0308 - mae: 48.6474 - val_loss: 6086.6104 - val_mse: 6086.6104 - val_mae: 46.9509\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6221.9897 - mse: 6221.9897 - mae: 48.6444 - val_loss: 6085.5962 - val_mse: 6085.5962 - val_mae: 46.9479\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6220.9741 - mse: 6220.9741 - mae: 48.6414 - val_loss: 6084.5630 - val_mse: 6084.5630 - val_mae: 46.9449\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6219.9224 - mse: 6219.9224 - mae: 48.6383 - val_loss: 6083.5430 - val_mse: 6083.5430 - val_mae: 46.9419\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6218.8882 - mse: 6218.8882 - mae: 48.6353 - val_loss: 6082.5225 - val_mse: 6082.5225 - val_mae: 46.9389\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6217.8525 - mse: 6217.8525 - mae: 48.6323 - val_loss: 6081.4976 - val_mse: 6081.4976 - val_mae: 46.9359\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6216.8140 - mse: 6216.8140 - mae: 48.6293 - val_loss: 6080.4736 - val_mse: 6080.4736 - val_mae: 46.9329\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6215.7783 - mse: 6215.7783 - mae: 48.6262 - val_loss: 6079.4468 - val_mse: 6079.4468 - val_mae: 46.9299\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6214.7441 - mse: 6214.7441 - mae: 48.6232 - val_loss: 6078.4185 - val_mse: 6078.4185 - val_mae: 46.9268\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6213.7075 - mse: 6213.7075 - mae: 48.6202 - val_loss: 6077.3877 - val_mse: 6077.3877 - val_mae: 46.9238\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6212.6626 - mse: 6212.6626 - mae: 48.6171 - val_loss: 6076.3647 - val_mse: 6076.3647 - val_mae: 46.9208\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6211.6353 - mse: 6211.6353 - mae: 48.6141 - val_loss: 6075.3394 - val_mse: 6075.3394 - val_mae: 46.9178\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6210.5947 - mse: 6210.5947 - mae: 48.6111 - val_loss: 6074.3242 - val_mse: 6074.3242 - val_mae: 46.9148\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6209.5605 - mse: 6209.5605 - mae: 48.6081 - val_loss: 6073.3149 - val_mse: 6073.3149 - val_mae: 46.9118\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6208.5391 - mse: 6208.5391 - mae: 48.6051 - val_loss: 6072.2847 - val_mse: 6072.2847 - val_mae: 46.9088\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6207.4995 - mse: 6207.4995 - mae: 48.6020 - val_loss: 6071.2681 - val_mse: 6071.2681 - val_mae: 46.9058\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6206.4634 - mse: 6206.4634 - mae: 48.5990 - val_loss: 6070.2437 - val_mse: 6070.2437 - val_mae: 46.9028\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6205.4385 - mse: 6205.4385 - mae: 48.5960 - val_loss: 6069.2158 - val_mse: 6069.2158 - val_mae: 46.8998\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6204.3960 - mse: 6204.3960 - mae: 48.5929 - val_loss: 6068.1968 - val_mse: 6068.1968 - val_mae: 46.8968\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6203.3564 - mse: 6203.3564 - mae: 48.5899 - val_loss: 6067.1772 - val_mse: 6067.1772 - val_mae: 46.8938\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6202.3262 - mse: 6202.3262 - mae: 48.5869 - val_loss: 6066.1528 - val_mse: 6066.1528 - val_mae: 46.8907\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6201.2896 - mse: 6201.2896 - mae: 48.5839 - val_loss: 6065.1270 - val_mse: 6065.1270 - val_mae: 46.8877\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6200.2510 - mse: 6200.2510 - mae: 48.5808 - val_loss: 6064.1025 - val_mse: 6064.1025 - val_mae: 46.8847\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6199.2178 - mse: 6199.2178 - mae: 48.5778 - val_loss: 6063.0820 - val_mse: 6063.0820 - val_mae: 46.8817\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6198.1821 - mse: 6198.1821 - mae: 48.5748 - val_loss: 6062.0601 - val_mse: 6062.0601 - val_mae: 46.8787\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6197.1558 - mse: 6197.1558 - mae: 48.5718 - val_loss: 6061.0347 - val_mse: 6061.0347 - val_mae: 46.8757\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6196.1182 - mse: 6196.1182 - mae: 48.5687 - val_loss: 6060.0176 - val_mse: 6060.0176 - val_mae: 46.8727\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6195.0923 - mse: 6195.0923 - mae: 48.5657 - val_loss: 6059.0024 - val_mse: 6059.0024 - val_mae: 46.8697\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6194.0605 - mse: 6194.0605 - mae: 48.5627 - val_loss: 6057.9883 - val_mse: 6057.9883 - val_mae: 46.8667\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6193.0303 - mse: 6193.0303 - mae: 48.5597 - val_loss: 6056.9771 - val_mse: 6056.9771 - val_mae: 46.8637\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6192.0142 - mse: 6192.0142 - mae: 48.5567 - val_loss: 6055.9551 - val_mse: 6055.9551 - val_mae: 46.8607\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6190.9839 - mse: 6190.9839 - mae: 48.5537 - val_loss: 6054.9351 - val_mse: 6054.9351 - val_mae: 46.8577\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6189.9507 - mse: 6189.9507 - mae: 48.5507 - val_loss: 6053.9277 - val_mse: 6053.9277 - val_mae: 46.8548\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6188.9321 - mse: 6188.9321 - mae: 48.5477 - val_loss: 6052.9072 - val_mse: 6052.9072 - val_mae: 46.8517\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6187.8916 - mse: 6187.8916 - mae: 48.5447 - val_loss: 6051.9053 - val_mse: 6051.9053 - val_mae: 46.8488\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6186.8701 - mse: 6186.8701 - mae: 48.5417 - val_loss: 6050.8867 - val_mse: 6050.8867 - val_mae: 46.8458\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6185.8423 - mse: 6185.8423 - mae: 48.5387 - val_loss: 6049.8633 - val_mse: 6049.8633 - val_mae: 46.8428\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6184.8115 - mse: 6184.8115 - mae: 48.5356 - val_loss: 6048.8394 - val_mse: 6048.8394 - val_mae: 46.8398\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6183.7720 - mse: 6183.7720 - mae: 48.5326 - val_loss: 6047.8218 - val_mse: 6047.8218 - val_mae: 46.8368\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6182.7490 - mse: 6182.7490 - mae: 48.5296 - val_loss: 6046.7974 - val_mse: 6046.7974 - val_mae: 46.8337\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6181.7124 - mse: 6181.7124 - mae: 48.5265 - val_loss: 6045.7778 - val_mse: 6045.7778 - val_mae: 46.8307\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 6180.6777 - mse: 6180.6777 - mae: 48.5235 - val_loss: 6044.7598 - val_mse: 6044.7598 - val_mae: 46.8277\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 6179.6533 - mse: 6179.6533 - mae: 48.5205 - val_loss: 6043.7432 - val_mse: 6043.7432 - val_mae: 46.8247\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp5qmrhzqz/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 20ms/step - loss: 1017.2266 - mse: 1017.2266 - mae: 26.5546 - val_loss: 967.0067 - val_mse: 967.0067 - val_mae: 25.7085\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1017.0073 - mse: 1017.0073 - mae: 26.5518 - val_loss: 966.7745 - val_mse: 966.7744 - val_mae: 25.7057\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1016.7924 - mse: 1016.7924 - mae: 26.5492 - val_loss: 966.5403 - val_mse: 966.5403 - val_mae: 25.7028\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1016.5742 - mse: 1016.5742 - mae: 26.5464 - val_loss: 966.3083 - val_mse: 966.3083 - val_mae: 25.7000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1016.3563 - mse: 1016.3563 - mae: 26.5437 - val_loss: 966.0778 - val_mse: 966.0778 - val_mae: 25.6972\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1016.1385 - mse: 1016.1385 - mae: 26.5410 - val_loss: 965.8469 - val_mse: 965.8469 - val_mae: 25.6944\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1015.9227 - mse: 1015.9227 - mae: 26.5383 - val_loss: 965.6146 - val_mse: 965.6146 - val_mae: 25.6916\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1015.7064 - mse: 1015.7064 - mae: 26.5355 - val_loss: 965.3811 - val_mse: 965.3811 - val_mae: 25.6887\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1015.4861 - mse: 1015.4861 - mae: 26.5328 - val_loss: 965.1493 - val_mse: 965.1493 - val_mae: 25.6859\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1015.2688 - mse: 1015.2688 - mae: 26.5300 - val_loss: 964.9174 - val_mse: 964.9174 - val_mae: 25.6831\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1015.0524 - mse: 1015.0524 - mae: 26.5273 - val_loss: 964.6852 - val_mse: 964.6852 - val_mae: 25.6802\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1014.8334 - mse: 1014.8334 - mae: 26.5245 - val_loss: 964.4561 - val_mse: 964.4561 - val_mae: 25.6775\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1014.6186 - mse: 1014.6186 - mae: 26.5219 - val_loss: 964.2242 - val_mse: 964.2242 - val_mae: 25.6746\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1014.4055 - mse: 1014.4055 - mae: 26.5191 - val_loss: 963.9910 - val_mse: 963.9910 - val_mae: 25.6718\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1014.1866 - mse: 1014.1866 - mae: 26.5164 - val_loss: 963.7596 - val_mse: 963.7596 - val_mae: 25.6689\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1013.9678 - mse: 1013.9678 - mae: 26.5136 - val_loss: 963.5321 - val_mse: 963.5321 - val_mae: 25.6662\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1013.7556 - mse: 1013.7556 - mae: 26.5110 - val_loss: 963.3007 - val_mse: 963.3007 - val_mae: 25.6634\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1013.5385 - mse: 1013.5385 - mae: 26.5082 - val_loss: 963.0695 - val_mse: 963.0695 - val_mae: 25.6606\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1013.3223 - mse: 1013.3223 - mae: 26.5055 - val_loss: 962.8389 - val_mse: 962.8389 - val_mae: 25.6577\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1013.1046 - mse: 1013.1046 - mae: 26.5028 - val_loss: 962.6097 - val_mse: 962.6097 - val_mae: 25.6549\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1012.8882 - mse: 1012.8882 - mae: 26.5000 - val_loss: 962.3790 - val_mse: 962.3790 - val_mae: 25.6521\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1012.6738 - mse: 1012.6738 - mae: 26.4974 - val_loss: 962.1461 - val_mse: 962.1461 - val_mae: 25.6493\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1012.4548 - mse: 1012.4548 - mae: 26.4946 - val_loss: 961.9147 - val_mse: 961.9147 - val_mae: 25.6465\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1012.2392 - mse: 1012.2392 - mae: 26.4919 - val_loss: 961.6840 - val_mse: 961.6840 - val_mae: 25.6436\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1012.0248 - mse: 1012.0248 - mae: 26.4892 - val_loss: 961.4536 - val_mse: 961.4536 - val_mae: 25.6408\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1011.8098 - mse: 1011.8098 - mae: 26.4865 - val_loss: 961.2231 - val_mse: 961.2231 - val_mae: 25.6380\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1011.5930 - mse: 1011.5930 - mae: 26.4838 - val_loss: 960.9955 - val_mse: 960.9955 - val_mae: 25.6352\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1011.3819 - mse: 1011.3819 - mae: 26.4811 - val_loss: 960.7637 - val_mse: 960.7637 - val_mae: 25.6324\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1011.1650 - mse: 1011.1650 - mae: 26.4784 - val_loss: 960.5349 - val_mse: 960.5349 - val_mae: 25.6296\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1010.9518 - mse: 1010.9518 - mae: 26.4757 - val_loss: 960.3046 - val_mse: 960.3046 - val_mae: 25.6268\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1010.7356 - mse: 1010.7356 - mae: 26.4730 - val_loss: 960.0781 - val_mse: 960.0781 - val_mae: 25.6240\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1010.5239 - mse: 1010.5239 - mae: 26.4703 - val_loss: 959.8496 - val_mse: 959.8496 - val_mae: 25.6212\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1010.3098 - mse: 1010.3098 - mae: 26.4676 - val_loss: 959.6223 - val_mse: 959.6223 - val_mae: 25.6185\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1010.0966 - mse: 1010.0966 - mae: 26.4650 - val_loss: 959.3961 - val_mse: 959.3961 - val_mae: 25.6157\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1009.8876 - mse: 1009.8876 - mae: 26.4623 - val_loss: 959.1655 - val_mse: 959.1655 - val_mae: 25.6129\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1009.6693 - mse: 1009.6693 - mae: 26.4596 - val_loss: 958.9406 - val_mse: 958.9406 - val_mae: 25.6101\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1009.4575 - mse: 1009.4575 - mae: 26.4569 - val_loss: 958.7115 - val_mse: 958.7115 - val_mae: 25.6073\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1009.2447 - mse: 1009.2447 - mae: 26.4543 - val_loss: 958.4822 - val_mse: 958.4822 - val_mae: 25.6045\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1009.0314 - mse: 1009.0314 - mae: 26.4517 - val_loss: 958.2531 - val_mse: 958.2531 - val_mae: 25.6017\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1008.8168 - mse: 1008.8168 - mae: 26.4489 - val_loss: 958.0242 - val_mse: 958.0242 - val_mae: 25.5989\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1008.6006 - mse: 1008.6006 - mae: 26.4462 - val_loss: 957.7985 - val_mse: 957.7985 - val_mae: 25.5961\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1008.3896 - mse: 1008.3896 - mae: 26.4436 - val_loss: 957.5696 - val_mse: 957.5696 - val_mae: 25.5933\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1008.1750 - mse: 1008.1750 - mae: 26.4409 - val_loss: 957.3416 - val_mse: 957.3416 - val_mae: 25.5905\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1007.9645 - mse: 1007.9645 - mae: 26.4383 - val_loss: 957.1118 - val_mse: 957.1118 - val_mae: 25.5877\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1007.7466 - mse: 1007.7466 - mae: 26.4356 - val_loss: 956.8879 - val_mse: 956.8879 - val_mae: 25.5850\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1007.5388 - mse: 1007.5388 - mae: 26.4330 - val_loss: 956.6589 - val_mse: 956.6589 - val_mae: 25.5822\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1007.3246 - mse: 1007.3246 - mae: 26.4303 - val_loss: 956.4301 - val_mse: 956.4301 - val_mae: 25.5794\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1007.1111 - mse: 1007.1111 - mae: 26.4276 - val_loss: 956.2020 - val_mse: 956.2020 - val_mae: 25.5766\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1006.8992 - mse: 1006.8992 - mae: 26.4249 - val_loss: 955.9739 - val_mse: 955.9739 - val_mae: 25.5738\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1006.6849 - mse: 1006.6849 - mae: 26.4222 - val_loss: 955.7479 - val_mse: 955.7479 - val_mae: 25.5711\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1006.4707 - mse: 1006.4707 - mae: 26.4195 - val_loss: 955.5208 - val_mse: 955.5208 - val_mae: 25.5683\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1006.2592 - mse: 1006.2592 - mae: 26.4169 - val_loss: 955.2906 - val_mse: 955.2906 - val_mae: 25.5654\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1006.0424 - mse: 1006.0424 - mae: 26.4142 - val_loss: 955.0624 - val_mse: 955.0624 - val_mae: 25.5626\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1005.8273 - mse: 1005.8273 - mae: 26.4115 - val_loss: 954.8342 - val_mse: 954.8342 - val_mae: 25.5598\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1005.6141 - mse: 1005.6141 - mae: 26.4089 - val_loss: 954.6046 - val_mse: 954.6046 - val_mae: 25.5570\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1005.4015 - mse: 1005.4015 - mae: 26.4062 - val_loss: 954.3754 - val_mse: 954.3754 - val_mae: 25.5542\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1005.1880 - mse: 1005.1880 - mae: 26.4035 - val_loss: 954.1477 - val_mse: 954.1477 - val_mae: 25.5514\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1004.9755 - mse: 1004.9755 - mae: 26.4009 - val_loss: 953.9199 - val_mse: 953.9199 - val_mae: 25.5486\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1004.7603 - mse: 1004.7603 - mae: 26.3982 - val_loss: 953.6940 - val_mse: 953.6940 - val_mae: 25.5458\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1004.5488 - mse: 1004.5488 - mae: 26.3956 - val_loss: 953.4673 - val_mse: 953.4673 - val_mae: 25.5431\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1004.3356 - mse: 1004.3356 - mae: 26.3930 - val_loss: 953.2426 - val_mse: 953.2426 - val_mae: 25.5403\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1004.1250 - mse: 1004.1250 - mae: 26.3903 - val_loss: 953.0174 - val_mse: 953.0174 - val_mae: 25.5375\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1003.9125 - mse: 1003.9125 - mae: 26.3877 - val_loss: 952.7910 - val_mse: 952.7910 - val_mae: 25.5347\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1003.6983 - mse: 1003.6983 - mae: 26.3850 - val_loss: 952.5657 - val_mse: 952.5657 - val_mae: 25.5319\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1003.4885 - mse: 1003.4885 - mae: 26.3824 - val_loss: 952.3361 - val_mse: 952.3361 - val_mae: 25.5291\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1003.2767 - mse: 1003.2767 - mae: 26.3797 - val_loss: 952.1067 - val_mse: 952.1067 - val_mae: 25.5263\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1003.0585 - mse: 1003.0585 - mae: 26.3770 - val_loss: 951.8831 - val_mse: 951.8831 - val_mae: 25.5235\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1002.8491 - mse: 1002.8491 - mae: 26.3743 - val_loss: 951.6550 - val_mse: 951.6550 - val_mae: 25.5207\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1002.6357 - mse: 1002.6357 - mae: 26.3717 - val_loss: 951.4276 - val_mse: 951.4276 - val_mae: 25.5179\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1002.4218 - mse: 1002.4218 - mae: 26.3690 - val_loss: 951.2034 - val_mse: 951.2034 - val_mae: 25.5151\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1002.2101 - mse: 1002.2101 - mae: 26.3664 - val_loss: 950.9777 - val_mse: 950.9777 - val_mae: 25.5123\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1001.9986 - mse: 1001.9986 - mae: 26.3637 - val_loss: 950.7492 - val_mse: 950.7492 - val_mae: 25.5095\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1001.7839 - mse: 1001.7839 - mae: 26.3610 - val_loss: 950.5243 - val_mse: 950.5243 - val_mae: 25.5067\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1001.5710 - mse: 1001.5710 - mae: 26.3584 - val_loss: 950.2993 - val_mse: 950.2993 - val_mae: 25.5039\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1001.3605 - mse: 1001.3605 - mae: 26.3557 - val_loss: 950.0714 - val_mse: 950.0713 - val_mae: 25.5011\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1001.1473 - mse: 1001.1473 - mae: 26.3531 - val_loss: 949.8438 - val_mse: 949.8438 - val_mae: 25.4983\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1000.9353 - mse: 1000.9353 - mae: 26.3504 - val_loss: 949.6175 - val_mse: 949.6175 - val_mae: 25.4955\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1000.7206 - mse: 1000.7206 - mae: 26.3477 - val_loss: 949.3960 - val_mse: 949.3960 - val_mae: 25.4927\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1000.5131 - mse: 1000.5131 - mae: 26.3451 - val_loss: 949.1677 - val_mse: 949.1677 - val_mae: 25.4899\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1000.2997 - mse: 1000.2997 - mae: 26.3424 - val_loss: 948.9417 - val_mse: 948.9417 - val_mae: 25.4871\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1000.0876 - mse: 1000.0876 - mae: 26.3398 - val_loss: 948.7164 - val_mse: 948.7164 - val_mae: 25.4843\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 999.8762 - mse: 999.8762 - mae: 26.3371 - val_loss: 948.4906 - val_mse: 948.4906 - val_mae: 25.4815\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 999.6641 - mse: 999.6641 - mae: 26.3345 - val_loss: 948.2647 - val_mse: 948.2647 - val_mae: 25.4787\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 999.4527 - mse: 999.4527 - mae: 26.3319 - val_loss: 948.0422 - val_mse: 948.0422 - val_mae: 25.4760\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 999.2435 - mse: 999.2435 - mae: 26.3292 - val_loss: 947.8181 - val_mse: 947.8181 - val_mae: 25.4732\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 999.0355 - mse: 999.0355 - mae: 26.3266 - val_loss: 947.5911 - val_mse: 947.5911 - val_mae: 25.4704\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 998.8228 - mse: 998.8228 - mae: 26.3239 - val_loss: 947.3659 - val_mse: 947.3659 - val_mae: 25.4675\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 998.6123 - mse: 998.6123 - mae: 26.3213 - val_loss: 947.1409 - val_mse: 947.1409 - val_mae: 25.4647\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 998.4025 - mse: 998.4025 - mae: 26.3187 - val_loss: 946.9160 - val_mse: 946.9160 - val_mae: 25.4619\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 998.1917 - mse: 998.1917 - mae: 26.3160 - val_loss: 946.6918 - val_mse: 946.6918 - val_mae: 25.4591\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 997.9835 - mse: 997.9835 - mae: 26.3134 - val_loss: 946.4632 - val_mse: 946.4632 - val_mae: 25.4562\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 997.7690 - mse: 997.7690 - mae: 26.3108 - val_loss: 946.2394 - val_mse: 946.2394 - val_mae: 25.4534\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 997.5609 - mse: 997.5609 - mae: 26.3081 - val_loss: 946.0140 - val_mse: 946.0140 - val_mae: 25.4506\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 997.3502 - mse: 997.3502 - mae: 26.3055 - val_loss: 945.7889 - val_mse: 945.7889 - val_mae: 25.4478\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 997.1381 - mse: 997.1381 - mae: 26.3028 - val_loss: 945.5659 - val_mse: 945.5659 - val_mae: 25.4450\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 996.9311 - mse: 996.9311 - mae: 26.3002 - val_loss: 945.3392 - val_mse: 945.3392 - val_mae: 25.4422\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 996.7180 - mse: 996.7180 - mae: 26.2975 - val_loss: 945.1163 - val_mse: 945.1163 - val_mae: 25.4394\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 996.5106 - mse: 996.5106 - mae: 26.2950 - val_loss: 944.8893 - val_mse: 944.8893 - val_mae: 25.4365\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 996.2976 - mse: 996.2976 - mae: 26.2923 - val_loss: 944.6628 - val_mse: 944.6628 - val_mae: 25.4337\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 996.0887 - mse: 996.0887 - mae: 26.2897 - val_loss: 944.4356 - val_mse: 944.4356 - val_mae: 25.4308\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpydyv9tus/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 36ms/step - loss: 553.7541 - mse: 553.7541 - mae: 21.8271 - val_loss: 545.4205 - val_mse: 545.4205 - val_mae: 21.7784\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 553.7430 - mse: 553.7430 - mae: 21.8269 - val_loss: 545.4076 - val_mse: 545.4076 - val_mae: 21.7781\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.7314 - mse: 553.7314 - mae: 21.8266 - val_loss: 545.3948 - val_mse: 545.3948 - val_mae: 21.7779\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.7203 - mse: 553.7203 - mae: 21.8264 - val_loss: 545.3818 - val_mse: 545.3818 - val_mae: 21.7777\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.7086 - mse: 553.7086 - mae: 21.8262 - val_loss: 545.3680 - val_mse: 545.3680 - val_mae: 21.7774\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6973 - mse: 553.6973 - mae: 21.8259 - val_loss: 545.3539 - val_mse: 545.3539 - val_mae: 21.7772\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6859 - mse: 553.6859 - mae: 21.8257 - val_loss: 545.3397 - val_mse: 545.3397 - val_mae: 21.7769\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6742 - mse: 553.6742 - mae: 21.8255 - val_loss: 545.3256 - val_mse: 545.3256 - val_mae: 21.7767\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6630 - mse: 553.6630 - mae: 21.8252 - val_loss: 545.3115 - val_mse: 545.3115 - val_mae: 21.7764\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6515 - mse: 553.6515 - mae: 21.8250 - val_loss: 545.2974 - val_mse: 545.2974 - val_mae: 21.7761\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 553.6400 - mse: 553.6400 - mae: 21.8248 - val_loss: 545.2833 - val_mse: 545.2833 - val_mae: 21.7759\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp0cpuo47c/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 1836.9862 - mse: 1836.9862 - mae: 30.6916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 952.9039 - mse: 952.9039 - mae: 23.9172 - val_loss: 484.2060 - val_mse: 484.2060 - val_mae: 18.1814\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 499.9503 - mse: 499.9503 - mae: 18.7977 - val_loss: 373.1352 - val_mse: 373.1352 - val_mae: 16.3023\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.3243 - mse: 440.3243 - mae: 17.5782 - val_loss: 326.2769 - val_mse: 326.2769 - val_mae: 15.2926\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 411.5687 - mse: 411.5687 - mae: 16.9117 - val_loss: 303.7065 - val_mse: 303.7065 - val_mae: 14.7651\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 391.8192 - mse: 391.8192 - mae: 16.4359 - val_loss: 287.8206 - val_mse: 287.8206 - val_mae: 14.3397\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 377.9874 - mse: 377.9874 - mae: 16.0836 - val_loss: 277.5381 - val_mse: 277.5381 - val_mae: 14.0501\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 368.8320 - mse: 368.8320 - mae: 15.8031 - val_loss: 270.3205 - val_mse: 270.3205 - val_mae: 13.8020\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 362.0765 - mse: 362.0765 - mae: 15.5972 - val_loss: 264.9989 - val_mse: 264.9989 - val_mae: 13.5861\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 356.8487 - mse: 356.8487 - mae: 15.4057 - val_loss: 260.3645 - val_mse: 260.3645 - val_mae: 13.4382\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 351.8831 - mse: 351.8830 - mae: 15.2478 - val_loss: 256.1045 - val_mse: 256.1045 - val_mae: 13.3085\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 347.9214 - mse: 347.9214 - mae: 15.1293 - val_loss: 253.1335 - val_mse: 253.1335 - val_mae: 13.1909\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 344.9096 - mse: 344.9096 - mae: 15.0191 - val_loss: 250.3487 - val_mse: 250.3487 - val_mae: 13.1020\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 342.0861 - mse: 342.0861 - mae: 14.9436 - val_loss: 247.8761 - val_mse: 247.8761 - val_mae: 13.0194\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 339.5713 - mse: 339.5713 - mae: 14.8878 - val_loss: 244.0570 - val_mse: 244.0570 - val_mae: 12.8875\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 335.7720 - mse: 335.7720 - mae: 14.7706 - val_loss: 239.1416 - val_mse: 239.1416 - val_mae: 12.7409\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 329.4143 - mse: 329.4143 - mae: 14.6116 - val_loss: 230.1093 - val_mse: 230.1093 - val_mae: 12.4997\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 319.8535 - mse: 319.8535 - mae: 14.3324 - val_loss: 223.4065 - val_mse: 223.4065 - val_mae: 12.2232\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 311.7367 - mse: 311.7367 - mae: 14.0451 - val_loss: 219.2350 - val_mse: 219.2350 - val_mae: 12.0247\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 306.9930 - mse: 306.9930 - mae: 13.8628 - val_loss: 216.8387 - val_mse: 216.8387 - val_mae: 11.9156\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 303.9573 - mse: 303.9573 - mae: 13.7649 - val_loss: 214.4949 - val_mse: 214.4949 - val_mae: 11.7781\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 301.7996 - mse: 301.7996 - mae: 13.6971 - val_loss: 211.8633 - val_mse: 211.8633 - val_mae: 11.6379\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 300.1635 - mse: 300.1635 - mae: 13.6072 - val_loss: 210.2159 - val_mse: 210.2159 - val_mae: 11.5650\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 298.8220 - mse: 298.8220 - mae: 13.5466 - val_loss: 210.4729 - val_mse: 210.4729 - val_mae: 11.5856\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 297.8038 - mse: 297.8038 - mae: 13.5355 - val_loss: 208.7113 - val_mse: 208.7113 - val_mae: 11.5153\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 296.8954 - mse: 296.8954 - mae: 13.5175 - val_loss: 207.2440 - val_mse: 207.2440 - val_mae: 11.4335\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 296.0096 - mse: 296.0096 - mae: 13.4557 - val_loss: 206.9730 - val_mse: 206.9730 - val_mae: 11.4430\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.5310 - mse: 295.5310 - mae: 13.4563 - val_loss: 205.9367 - val_mse: 205.9367 - val_mae: 11.3864\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.9707 - mse: 294.9707 - mae: 13.4309 - val_loss: 204.8588 - val_mse: 204.8588 - val_mae: 11.3449\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.1911 - mse: 294.1911 - mae: 13.3942 - val_loss: 205.1528 - val_mse: 205.1528 - val_mae: 11.3657\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.4902 - mse: 293.4902 - mae: 13.4019 - val_loss: 203.5175 - val_mse: 203.5175 - val_mae: 11.2975\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.0392 - mse: 293.0392 - mae: 13.3642 - val_loss: 202.3070 - val_mse: 202.3070 - val_mae: 11.2277\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.6286 - mse: 292.6286 - mae: 13.3180 - val_loss: 202.6901 - val_mse: 202.6901 - val_mae: 11.2672\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.3733 - mse: 292.3733 - mae: 13.3467 - val_loss: 202.6461 - val_mse: 202.6461 - val_mae: 11.2575\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.7193 - mse: 291.7193 - mae: 13.3187 - val_loss: 202.1886 - val_mse: 202.1886 - val_mae: 11.2397\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.3455 - mse: 291.3455 - mae: 13.3043 - val_loss: 201.2760 - val_mse: 201.2760 - val_mae: 11.2043\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.1234 - mse: 291.1234 - mae: 13.3033 - val_loss: 200.5121 - val_mse: 200.5121 - val_mae: 11.1645\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.6981 - mse: 290.6981 - mae: 13.2819 - val_loss: 199.8568 - val_mse: 199.8568 - val_mae: 11.1273\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.3834 - mse: 290.3834 - mae: 13.2540 - val_loss: 199.2085 - val_mse: 199.2085 - val_mae: 11.1026\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.9044 - mse: 289.9044 - mae: 13.2291 - val_loss: 199.1802 - val_mse: 199.1802 - val_mae: 11.1020\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.7451 - mse: 289.7451 - mae: 13.2157 - val_loss: 199.5484 - val_mse: 199.5484 - val_mae: 11.1233\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.2636 - mse: 289.2636 - mae: 13.2391 - val_loss: 198.4624 - val_mse: 198.4624 - val_mae: 11.0752\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.9093 - mse: 288.9093 - mae: 13.2063 - val_loss: 198.3244 - val_mse: 198.3244 - val_mae: 11.0705\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.6553 - mse: 288.6553 - mae: 13.1985 - val_loss: 197.5961 - val_mse: 197.5961 - val_mae: 11.0444\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.3599 - mse: 288.3599 - mae: 13.1919 - val_loss: 197.3761 - val_mse: 197.3761 - val_mae: 11.0236\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.1704 - mse: 288.1704 - mae: 13.1925 - val_loss: 196.9401 - val_mse: 196.9401 - val_mae: 11.0031\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.7818 - mse: 287.7818 - mae: 13.1644 - val_loss: 196.4119 - val_mse: 196.4119 - val_mae: 10.9689\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.0035 - mse: 288.0035 - mae: 13.1496 - val_loss: 196.4586 - val_mse: 196.4586 - val_mae: 10.9667\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.3355 - mse: 287.3355 - mae: 13.1330 - val_loss: 196.9647 - val_mse: 196.9647 - val_mae: 11.0062\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.1826 - mse: 287.1826 - mae: 13.1370 - val_loss: 197.1811 - val_mse: 197.1811 - val_mae: 11.0014\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 286.9840 - mse: 286.9840 - mae: 13.1394 - val_loss: 196.2772 - val_mse: 196.2772 - val_mae: 10.9556\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 286.6984 - mse: 286.6984 - mae: 13.1070 - val_loss: 196.7983 - val_mse: 196.7983 - val_mae: 10.9880\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 286.6474 - mse: 286.6474 - mae: 13.1427 - val_loss: 195.8575 - val_mse: 195.8575 - val_mae: 10.9374\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 286.3329 - mse: 286.3329 - mae: 13.1003 - val_loss: 196.0182 - val_mse: 196.0182 - val_mae: 10.9515\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 286.1937 - mse: 286.1937 - mae: 13.1316 - val_loss: 194.5278 - val_mse: 194.5278 - val_mae: 10.8665\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 286.0046 - mse: 286.0046 - mae: 13.0643 - val_loss: 195.9832 - val_mse: 195.9832 - val_mae: 10.9543\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.8040 - mse: 285.8040 - mae: 13.0968 - val_loss: 194.8990 - val_mse: 194.8990 - val_mae: 10.8845\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.5041 - mse: 285.5041 - mae: 13.0627 - val_loss: 194.8330 - val_mse: 194.8330 - val_mae: 10.8784\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.5214 - mse: 285.5214 - mae: 13.0670 - val_loss: 195.2058 - val_mse: 195.2058 - val_mae: 10.9040\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.4012 - mse: 285.4012 - mae: 13.0922 - val_loss: 193.6928 - val_mse: 193.6928 - val_mae: 10.8247\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.1327 - mse: 285.1327 - mae: 13.0490 - val_loss: 194.1180 - val_mse: 194.1180 - val_mae: 10.8347\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.3701 - mse: 285.3701 - mae: 13.0476 - val_loss: 194.0717 - val_mse: 194.0717 - val_mae: 10.8330\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.7761 - mse: 284.7761 - mae: 13.0526 - val_loss: 193.2212 - val_mse: 193.2212 - val_mae: 10.7886\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.6872 - mse: 284.6872 - mae: 13.0174 - val_loss: 193.5310 - val_mse: 193.5310 - val_mae: 10.8064\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.5868 - mse: 284.5868 - mae: 13.0176 - val_loss: 194.2467 - val_mse: 194.2467 - val_mae: 10.8541\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 284.5777 - mse: 284.5777 - mae: 13.0365 - val_loss: 193.5100 - val_mse: 193.5100 - val_mae: 10.8069\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.2751 - mse: 284.2751 - mae: 13.0167 - val_loss: 192.9588 - val_mse: 192.9588 - val_mae: 10.7773\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.2618 - mse: 284.2618 - mae: 13.0056 - val_loss: 192.8711 - val_mse: 192.8711 - val_mae: 10.7776\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.9240 - mse: 283.9240 - mae: 13.0023 - val_loss: 193.0701 - val_mse: 193.0701 - val_mae: 10.7854\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.9719 - mse: 283.9719 - mae: 13.0250 - val_loss: 192.6232 - val_mse: 192.6232 - val_mae: 10.7576\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.6613 - mse: 283.6613 - mae: 12.9834 - val_loss: 192.4503 - val_mse: 192.4503 - val_mae: 10.7445\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.6548 - mse: 283.6548 - mae: 13.0074 - val_loss: 191.5604 - val_mse: 191.5604 - val_mae: 10.7145\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.5788 - mse: 283.5788 - mae: 12.9468 - val_loss: 192.3400 - val_mse: 192.3400 - val_mae: 10.7386\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.3177 - mse: 283.3177 - mae: 12.9809 - val_loss: 191.6832 - val_mse: 191.6832 - val_mae: 10.7046\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 283.2626 - mse: 283.2626 - mae: 12.9481 - val_loss: 193.1769 - val_mse: 193.1769 - val_mae: 10.7836\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.3251 - mse: 283.3251 - mae: 12.9897 - val_loss: 192.1420 - val_mse: 192.1420 - val_mae: 10.7204\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.2335 - mse: 283.2335 - mae: 12.9553 - val_loss: 191.7768 - val_mse: 191.7768 - val_mae: 10.7049\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 282.9824 - mse: 282.9824 - mae: 12.9587 - val_loss: 191.9130 - val_mse: 191.9130 - val_mae: 10.7031\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 282.8379 - mse: 282.8379 - mae: 12.9585 - val_loss: 191.6900 - val_mse: 191.6900 - val_mae: 10.6940\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 282.7018 - mse: 282.7018 - mae: 12.9421 - val_loss: 191.6241 - val_mse: 191.6241 - val_mae: 10.6873\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 282.8790 - mse: 282.8790 - mae: 12.9564 - val_loss: 192.1353 - val_mse: 192.1353 - val_mae: 10.7231\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 282.6211 - mse: 282.6211 - mae: 12.9723 - val_loss: 191.0555 - val_mse: 191.0555 - val_mae: 10.6656\n",
      "Epoch 81: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp3k6qs4zp/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 2243.2649 - mse: 2243.2649 - mae: 33.5671"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 1034.0267 - mse: 1034.0267 - mae: 23.8161 - val_loss: 458.8803 - val_mse: 458.8803 - val_mae: 17.2132\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 496.9819 - mse: 496.9818 - mae: 17.9637 - val_loss: 350.6255 - val_mse: 350.6255 - val_mae: 15.4191\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 438.7036 - mse: 438.7036 - mae: 16.8819 - val_loss: 303.9397 - val_mse: 303.9397 - val_mae: 14.4311\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 402.2175 - mse: 402.2175 - mae: 16.0795 - val_loss: 273.2162 - val_mse: 273.2162 - val_mae: 13.6342\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 375.2523 - mse: 375.2523 - mae: 15.4386 - val_loss: 253.6048 - val_mse: 253.6048 - val_mae: 13.1010\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 359.3770 - mse: 359.3770 - mae: 15.0772 - val_loss: 239.8334 - val_mse: 239.8334 - val_mae: 12.7120\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 346.7273 - mse: 346.7273 - mae: 14.7417 - val_loss: 230.0083 - val_mse: 230.0083 - val_mae: 12.4399\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 337.8925 - mse: 337.8925 - mae: 14.5345 - val_loss: 224.9287 - val_mse: 224.9287 - val_mae: 12.2721\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 328.3548 - mse: 328.3548 - mae: 14.3358 - val_loss: 217.7623 - val_mse: 217.7623 - val_mae: 12.0663\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 318.0660 - mse: 318.0660 - mae: 14.0752 - val_loss: 207.8774 - val_mse: 207.8774 - val_mae: 11.7791\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 311.2879 - mse: 311.2879 - mae: 13.8760 - val_loss: 202.4643 - val_mse: 202.4643 - val_mae: 11.5268\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 306.6922 - mse: 306.6922 - mae: 13.6826 - val_loss: 198.9195 - val_mse: 198.9195 - val_mae: 11.4676\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 303.2885 - mse: 303.2885 - mae: 13.6025 - val_loss: 195.9954 - val_mse: 195.9954 - val_mae: 11.3525\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 300.5107 - mse: 300.5107 - mae: 13.5253 - val_loss: 191.5700 - val_mse: 191.5700 - val_mae: 11.1394\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 297.5121 - mse: 297.5121 - mae: 13.3885 - val_loss: 190.6689 - val_mse: 190.6689 - val_mae: 11.0933\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 295.4606 - mse: 295.4606 - mae: 13.3506 - val_loss: 187.0966 - val_mse: 187.0966 - val_mae: 10.9559\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.8185 - mse: 293.8185 - mae: 13.2559 - val_loss: 187.9100 - val_mse: 187.9100 - val_mae: 10.9843\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.2651 - mse: 292.2651 - mae: 13.2202 - val_loss: 185.3895 - val_mse: 185.3895 - val_mae: 10.8637\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 290.4666 - mse: 290.4666 - mae: 13.1371 - val_loss: 185.5029 - val_mse: 185.5029 - val_mae: 10.8686\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.4264 - mse: 289.4264 - mae: 13.1221 - val_loss: 184.8523 - val_mse: 184.8523 - val_mae: 10.8225\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.8513 - mse: 287.8513 - mae: 13.1100 - val_loss: 180.6881 - val_mse: 180.6881 - val_mae: 10.6264\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.1806 - mse: 287.1806 - mae: 13.0063 - val_loss: 180.4777 - val_mse: 180.4777 - val_mae: 10.5880\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.2849 - mse: 285.2849 - mae: 12.9449 - val_loss: 181.6422 - val_mse: 181.6422 - val_mae: 10.6406\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.2494 - mse: 284.2494 - mae: 12.9585 - val_loss: 180.4215 - val_mse: 180.4215 - val_mae: 10.5718\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 282.8850 - mse: 282.8850 - mae: 12.8979 - val_loss: 178.9470 - val_mse: 178.9470 - val_mae: 10.4975\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 281.9372 - mse: 281.9372 - mae: 12.8505 - val_loss: 179.4252 - val_mse: 179.4252 - val_mae: 10.5362\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 280.5996 - mse: 280.5996 - mae: 12.8100 - val_loss: 175.8788 - val_mse: 175.8788 - val_mae: 10.3522\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 279.7803 - mse: 279.7803 - mae: 12.7685 - val_loss: 175.2384 - val_mse: 175.2384 - val_mae: 10.3284\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 277.7520 - mse: 277.7520 - mae: 12.7001 - val_loss: 174.3517 - val_mse: 174.3517 - val_mae: 10.2930\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 276.5216 - mse: 276.5216 - mae: 12.6612 - val_loss: 174.6201 - val_mse: 174.6201 - val_mae: 10.3116\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 275.7078 - mse: 275.7078 - mae: 12.6777 - val_loss: 171.2081 - val_mse: 171.2081 - val_mae: 10.1254\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 274.6234 - mse: 274.6234 - mae: 12.5698 - val_loss: 174.3864 - val_mse: 174.3864 - val_mae: 10.3476\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 273.2402 - mse: 273.2402 - mae: 12.6231 - val_loss: 172.2190 - val_mse: 172.2190 - val_mae: 10.2289\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 272.3235 - mse: 272.3235 - mae: 12.5589 - val_loss: 172.3146 - val_mse: 172.3146 - val_mae: 10.2766\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 271.4028 - mse: 271.4028 - mae: 12.5476 - val_loss: 170.5983 - val_mse: 170.5983 - val_mae: 10.1659\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 270.6995 - mse: 270.6995 - mae: 12.5534 - val_loss: 168.0322 - val_mse: 168.0322 - val_mae: 10.0190\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 270.1921 - mse: 270.1921 - mae: 12.4973 - val_loss: 166.9193 - val_mse: 166.9193 - val_mae: 9.9475\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 269.4295 - mse: 269.4295 - mae: 12.4463 - val_loss: 166.3440 - val_mse: 166.3440 - val_mae: 9.9304\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 268.4781 - mse: 268.4781 - mae: 12.4586 - val_loss: 165.4393 - val_mse: 165.4393 - val_mae: 9.8715\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 267.8605 - mse: 267.8605 - mae: 12.3737 - val_loss: 167.7869 - val_mse: 167.7869 - val_mae: 10.0433\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 267.4948 - mse: 267.4948 - mae: 12.4710 - val_loss: 164.5112 - val_mse: 164.5112 - val_mae: 9.8673\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 266.8589 - mse: 266.8589 - mae: 12.4003 - val_loss: 163.5043 - val_mse: 163.5043 - val_mae: 9.8022\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 266.3299 - mse: 266.3299 - mae: 12.3503 - val_loss: 163.9088 - val_mse: 163.9088 - val_mae: 9.8268\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 265.8241 - mse: 265.8241 - mae: 12.3570 - val_loss: 163.8635 - val_mse: 163.8635 - val_mae: 9.8430\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 265.6596 - mse: 265.6596 - mae: 12.4412 - val_loss: 161.5177 - val_mse: 161.5177 - val_mae: 9.6947\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 265.3968 - mse: 265.3968 - mae: 12.2847 - val_loss: 161.8683 - val_mse: 161.8683 - val_mae: 9.7154\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 264.8104 - mse: 264.8104 - mae: 12.2824 - val_loss: 162.7771 - val_mse: 162.7771 - val_mae: 9.7814\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 264.3375 - mse: 264.3375 - mae: 12.3074 - val_loss: 162.6941 - val_mse: 162.6941 - val_mae: 9.7807\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 263.7897 - mse: 263.7897 - mae: 12.2471 - val_loss: 167.0524 - val_mse: 167.0524 - val_mae: 10.0744\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 263.9544 - mse: 263.9544 - mae: 12.3742 - val_loss: 161.6315 - val_mse: 161.6315 - val_mae: 9.7138\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 263.3002 - mse: 263.3002 - mae: 12.2606 - val_loss: 161.6650 - val_mse: 161.6650 - val_mae: 9.7277\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 263.6652 - mse: 263.6652 - mae: 12.2870 - val_loss: 161.6705 - val_mse: 161.6705 - val_mae: 9.7229\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 262.4243 - mse: 262.4243 - mae: 12.2343 - val_loss: 161.7312 - val_mse: 161.7312 - val_mae: 9.7446\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 262.4157 - mse: 262.4157 - mae: 12.2721 - val_loss: 159.7682 - val_mse: 159.7682 - val_mae: 9.6175\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 261.7751 - mse: 261.7751 - mae: 12.1708 - val_loss: 161.9241 - val_mse: 161.9241 - val_mae: 9.7682\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 261.9102 - mse: 261.9102 - mae: 12.2527 - val_loss: 160.8978 - val_mse: 160.8978 - val_mae: 9.6998\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 261.1492 - mse: 261.1492 - mae: 12.1992 - val_loss: 160.8457 - val_mse: 160.8457 - val_mae: 9.7043\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 260.6499 - mse: 260.6499 - mae: 12.2069 - val_loss: 160.7468 - val_mse: 160.7468 - val_mae: 9.7203\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 260.2760 - mse: 260.2760 - mae: 12.1792 - val_loss: 161.0410 - val_mse: 161.0410 - val_mae: 9.7391\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 260.1757 - mse: 260.1757 - mae: 12.2107 - val_loss: 160.8654 - val_mse: 160.8654 - val_mae: 9.7380\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 259.9796 - mse: 259.9796 - mae: 12.2253 - val_loss: 158.7029 - val_mse: 158.7029 - val_mae: 9.5905\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 258.8520 - mse: 258.8520 - mae: 12.1664 - val_loss: 157.7026 - val_mse: 157.7026 - val_mae: 9.5385\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 258.6111 - mse: 258.6111 - mae: 12.1490 - val_loss: 157.2912 - val_mse: 157.2912 - val_mae: 9.5089\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 258.2197 - mse: 258.2197 - mae: 12.1267 - val_loss: 157.6406 - val_mse: 157.6406 - val_mae: 9.5307\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 258.2018 - mse: 258.2018 - mae: 12.0756 - val_loss: 158.4903 - val_mse: 158.4903 - val_mae: 9.5858\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 257.4892 - mse: 257.4892 - mae: 12.0957 - val_loss: 159.2150 - val_mse: 159.2150 - val_mae: 9.6369\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 257.2125 - mse: 257.2125 - mae: 12.1374 - val_loss: 157.0128 - val_mse: 157.0128 - val_mae: 9.4911\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 257.1181 - mse: 257.1181 - mae: 12.0771 - val_loss: 157.9791 - val_mse: 157.9791 - val_mae: 9.5555\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 257.2585 - mse: 257.2585 - mae: 12.1322 - val_loss: 157.1510 - val_mse: 157.1510 - val_mae: 9.5013\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 256.4668 - mse: 256.4668 - mae: 12.0435 - val_loss: 156.9759 - val_mse: 156.9759 - val_mae: 9.4927\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 256.2611 - mse: 256.2611 - mae: 12.1047 - val_loss: 155.0617 - val_mse: 155.0617 - val_mae: 9.3631\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 256.5550 - mse: 256.5550 - mae: 12.0091 - val_loss: 156.1855 - val_mse: 156.1855 - val_mae: 9.4368\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 256.0353 - mse: 256.0353 - mae: 12.0377 - val_loss: 156.3834 - val_mse: 156.3834 - val_mae: 9.4548\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 255.5717 - mse: 255.5717 - mae: 12.0128 - val_loss: 159.1288 - val_mse: 159.1288 - val_mae: 9.6636\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 255.8285 - mse: 255.8285 - mae: 12.0870 - val_loss: 156.8403 - val_mse: 156.8403 - val_mae: 9.4829\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 255.8513 - mse: 255.8513 - mae: 11.9942 - val_loss: 157.0767 - val_mse: 157.0767 - val_mae: 9.5090\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 255.1388 - mse: 255.1388 - mae: 12.0327 - val_loss: 157.5761 - val_mse: 157.5761 - val_mae: 9.5512\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 254.7239 - mse: 254.7239 - mae: 12.0839 - val_loss: 153.6439 - val_mse: 153.6439 - val_mae: 9.2646\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 255.0108 - mse: 255.0108 - mae: 11.9670 - val_loss: 154.7184 - val_mse: 154.7184 - val_mae: 9.3425\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 254.8715 - mse: 254.8715 - mae: 11.9989 - val_loss: 155.2380 - val_mse: 155.2380 - val_mae: 9.3802\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 254.6925 - mse: 254.6925 - mae: 12.0207 - val_loss: 153.5669 - val_mse: 153.5669 - val_mae: 9.2603\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 254.4172 - mse: 254.4172 - mae: 11.9237 - val_loss: 156.1800 - val_mse: 156.1800 - val_mae: 9.4582\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 254.3441 - mse: 254.3441 - mae: 11.9628 - val_loss: 157.9253 - val_mse: 157.9253 - val_mae: 9.5820\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 254.4245 - mse: 254.4245 - mae: 12.0241 - val_loss: 155.6442 - val_mse: 155.6442 - val_mae: 9.4120\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 254.0061 - mse: 254.0061 - mae: 11.9351 - val_loss: 156.6994 - val_mse: 156.6994 - val_mae: 9.4857\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 253.8975 - mse: 253.8975 - mae: 11.9928 - val_loss: 155.4218 - val_mse: 155.4218 - val_mae: 9.3901\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 253.6242 - mse: 253.6242 - mae: 11.9486 - val_loss: 155.5034 - val_mse: 155.5034 - val_mae: 9.4034\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 253.8961 - mse: 253.8961 - mae: 11.9437 - val_loss: 158.3766 - val_mse: 158.3766 - val_mae: 9.6073\n",
      "Epoch 88: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 3375]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp77mchaqh/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 2381.5725 - mse: 2381.5725 - mae: 35.5683"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 1146.6687 - mse: 1146.6687 - mae: 25.7808 - val_loss: 510.8493 - val_mse: 510.8493 - val_mae: 19.5769\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 483.7398 - mse: 483.7398 - mae: 18.8704 - val_loss: 414.0573 - val_mse: 414.0573 - val_mae: 17.5551\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 423.8484 - mse: 423.8484 - mae: 17.6256 - val_loss: 388.7452 - val_mse: 388.7452 - val_mae: 16.8193\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 400.1743 - mse: 400.1743 - mae: 16.9948 - val_loss: 375.8424 - val_mse: 375.8424 - val_mae: 16.3974\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 386.7537 - mse: 386.7537 - mae: 16.5962 - val_loss: 368.2192 - val_mse: 368.2192 - val_mae: 16.1256\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 377.9373 - mse: 377.9373 - mae: 16.3248 - val_loss: 363.1104 - val_mse: 363.1104 - val_mae: 15.9455\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 371.7918 - mse: 371.7918 - mae: 16.1378 - val_loss: 359.5457 - val_mse: 359.5457 - val_mae: 15.8428\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 367.1691 - mse: 367.1691 - mae: 16.0041 - val_loss: 356.7457 - val_mse: 356.7457 - val_mae: 15.7404\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 363.5912 - mse: 363.5912 - mae: 15.8752 - val_loss: 354.6055 - val_mse: 354.6055 - val_mae: 15.6660\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 360.4740 - mse: 360.4740 - mae: 15.7817 - val_loss: 352.8394 - val_mse: 352.8394 - val_mae: 15.5951\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 358.0247 - mse: 358.0247 - mae: 15.6953 - val_loss: 351.3094 - val_mse: 351.3094 - val_mae: 15.5511\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 355.9586 - mse: 355.9586 - mae: 15.6259 - val_loss: 349.7572 - val_mse: 349.7572 - val_mae: 15.4892\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 353.9933 - mse: 353.9933 - mae: 15.5511 - val_loss: 348.5517 - val_mse: 348.5517 - val_mae: 15.4484\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 352.3853 - mse: 352.3853 - mae: 15.4887 - val_loss: 347.3793 - val_mse: 347.3793 - val_mae: 15.4111\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 350.8754 - mse: 350.8754 - mae: 15.4441 - val_loss: 346.1514 - val_mse: 346.1514 - val_mae: 15.3609\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 349.5056 - mse: 349.5056 - mae: 15.3890 - val_loss: 345.1198 - val_mse: 345.1198 - val_mae: 15.3284\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 348.2503 - mse: 348.2503 - mae: 15.3575 - val_loss: 343.7681 - val_mse: 343.7681 - val_mae: 15.2629\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 347.0644 - mse: 347.0644 - mae: 15.3072 - val_loss: 342.7204 - val_mse: 342.7204 - val_mae: 15.2333\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 345.8230 - mse: 345.8230 - mae: 15.2611 - val_loss: 341.7697 - val_mse: 341.7697 - val_mae: 15.2109\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 344.3168 - mse: 344.3168 - mae: 15.2249 - val_loss: 340.2440 - val_mse: 340.2440 - val_mae: 15.1525\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 341.3210 - mse: 341.3210 - mae: 15.1562 - val_loss: 336.0543 - val_mse: 336.0543 - val_mae: 15.0562\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 336.0331 - mse: 336.0331 - mae: 15.0118 - val_loss: 329.7047 - val_mse: 329.7047 - val_mae: 14.8730\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 331.3073 - mse: 331.3073 - mae: 14.8411 - val_loss: 326.2316 - val_mse: 326.2316 - val_mae: 14.7498\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 328.4792 - mse: 328.4792 - mae: 14.7249 - val_loss: 324.3597 - val_mse: 324.3597 - val_mae: 14.6760\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 326.7986 - mse: 326.7986 - mae: 14.6562 - val_loss: 322.8878 - val_mse: 322.8878 - val_mae: 14.6157\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 325.5565 - mse: 325.5565 - mae: 14.6031 - val_loss: 322.0660 - val_mse: 322.0660 - val_mae: 14.5867\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 324.6998 - mse: 324.6998 - mae: 14.5744 - val_loss: 320.9052 - val_mse: 320.9052 - val_mae: 14.5395\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 323.9443 - mse: 323.9443 - mae: 14.5318 - val_loss: 320.2721 - val_mse: 320.2721 - val_mae: 14.5157\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 323.1964 - mse: 323.1964 - mae: 14.5029 - val_loss: 319.4781 - val_mse: 319.4781 - val_mae: 14.4785\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 322.6321 - mse: 322.6321 - mae: 14.4790 - val_loss: 318.8707 - val_mse: 318.8707 - val_mae: 14.4575\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 322.0525 - mse: 322.0525 - mae: 14.4586 - val_loss: 318.2181 - val_mse: 318.2181 - val_mae: 14.4374\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 321.4736 - mse: 321.4736 - mae: 14.4277 - val_loss: 317.8973 - val_mse: 317.8973 - val_mae: 14.4325\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 321.0041 - mse: 321.0041 - mae: 14.4227 - val_loss: 316.9806 - val_mse: 316.9806 - val_mae: 14.3874\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 320.4359 - mse: 320.4359 - mae: 14.3937 - val_loss: 316.4649 - val_mse: 316.4649 - val_mae: 14.3618\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 319.8345 - mse: 319.8345 - mae: 14.3674 - val_loss: 316.0085 - val_mse: 316.0085 - val_mae: 14.3524\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 319.5067 - mse: 319.5067 - mae: 14.3573 - val_loss: 315.3206 - val_mse: 315.3206 - val_mae: 14.3220\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 319.0140 - mse: 319.0140 - mae: 14.3472 - val_loss: 314.5892 - val_mse: 314.5892 - val_mae: 14.2953\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 318.7206 - mse: 318.7206 - mae: 14.3176 - val_loss: 314.0743 - val_mse: 314.0743 - val_mae: 14.2780\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 318.3902 - mse: 318.3902 - mae: 14.3014 - val_loss: 313.7625 - val_mse: 313.7625 - val_mae: 14.2651\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 317.6857 - mse: 317.6857 - mae: 14.2825 - val_loss: 313.4820 - val_mse: 313.4820 - val_mae: 14.2613\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 317.4323 - mse: 317.4323 - mae: 14.2714 - val_loss: 313.0111 - val_mse: 313.0111 - val_mae: 14.2375\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 317.0190 - mse: 317.0190 - mae: 14.2489 - val_loss: 313.1754 - val_mse: 313.1754 - val_mae: 14.2590\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 316.7084 - mse: 316.7084 - mae: 14.2637 - val_loss: 312.2298 - val_mse: 312.2298 - val_mae: 14.2198\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 316.3142 - mse: 316.3142 - mae: 14.2301 - val_loss: 311.6256 - val_mse: 311.6256 - val_mae: 14.1903\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 315.8701 - mse: 315.8701 - mae: 14.2069 - val_loss: 311.3039 - val_mse: 311.3039 - val_mae: 14.1812\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 315.4606 - mse: 315.4606 - mae: 14.1902 - val_loss: 310.9865 - val_mse: 310.9865 - val_mae: 14.1651\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 315.1414 - mse: 315.1414 - mae: 14.2007 - val_loss: 310.2250 - val_mse: 310.2250 - val_mae: 14.1331\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 314.8610 - mse: 314.8610 - mae: 14.1583 - val_loss: 309.8373 - val_mse: 309.8373 - val_mae: 14.1216\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 314.4526 - mse: 314.4526 - mae: 14.1537 - val_loss: 309.4755 - val_mse: 309.4755 - val_mae: 14.1148\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 314.0182 - mse: 314.0182 - mae: 14.1331 - val_loss: 309.1277 - val_mse: 309.1277 - val_mae: 14.0912\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 313.4295 - mse: 313.4295 - mae: 14.1239 - val_loss: 308.0961 - val_mse: 308.0961 - val_mae: 14.0558\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 312.7498 - mse: 312.7498 - mae: 14.0880 - val_loss: 307.0546 - val_mse: 307.0546 - val_mae: 14.0303\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 311.2701 - mse: 311.2701 - mae: 14.0517 - val_loss: 305.3911 - val_mse: 305.3911 - val_mae: 14.0042\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 308.8990 - mse: 308.8990 - mae: 13.9986 - val_loss: 302.9284 - val_mse: 302.9284 - val_mae: 13.9436\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 306.4805 - mse: 306.4805 - mae: 13.9210 - val_loss: 300.3112 - val_mse: 300.3112 - val_mae: 13.8649\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 303.6260 - mse: 303.6260 - mae: 13.8355 - val_loss: 297.3986 - val_mse: 297.3986 - val_mae: 13.7808\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 301.4364 - mse: 301.4364 - mae: 13.7386 - val_loss: 295.4746 - val_mse: 295.4746 - val_mae: 13.7141\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 300.1119 - mse: 300.1119 - mae: 13.6974 - val_loss: 294.1537 - val_mse: 294.1537 - val_mae: 13.6716\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 299.1087 - mse: 299.1087 - mae: 13.6591 - val_loss: 292.8757 - val_mse: 292.8757 - val_mae: 13.6273\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 298.3160 - mse: 298.3160 - mae: 13.6442 - val_loss: 291.9906 - val_mse: 291.9906 - val_mae: 13.5874\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 297.7015 - mse: 297.7015 - mae: 13.6076 - val_loss: 291.3959 - val_mse: 291.3959 - val_mae: 13.5682\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 297.2171 - mse: 297.2171 - mae: 13.6058 - val_loss: 290.6458 - val_mse: 290.6458 - val_mae: 13.5391\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 296.8214 - mse: 296.8214 - mae: 13.5838 - val_loss: 289.7095 - val_mse: 289.7095 - val_mae: 13.5034\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 296.1021 - mse: 296.1021 - mae: 13.5449 - val_loss: 288.9253 - val_mse: 288.9253 - val_mae: 13.4785\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.4808 - mse: 295.4808 - mae: 13.5457 - val_loss: 288.1631 - val_mse: 288.1631 - val_mae: 13.4425\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.9957 - mse: 294.9957 - mae: 13.5104 - val_loss: 287.5964 - val_mse: 287.5964 - val_mae: 13.4223\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.4808 - mse: 294.4808 - mae: 13.4960 - val_loss: 287.0508 - val_mse: 287.0508 - val_mae: 13.4048\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.1540 - mse: 294.1540 - mae: 13.4961 - val_loss: 286.4437 - val_mse: 286.4437 - val_mae: 13.3716\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.8150 - mse: 293.8150 - mae: 13.4490 - val_loss: 286.0052 - val_mse: 286.0052 - val_mae: 13.3627\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.4120 - mse: 293.4120 - mae: 13.4595 - val_loss: 285.6717 - val_mse: 285.6717 - val_mae: 13.3434\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.0702 - mse: 293.0702 - mae: 13.4240 - val_loss: 285.4236 - val_mse: 285.4236 - val_mae: 13.3371\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 292.7985 - mse: 292.7985 - mae: 13.4238 - val_loss: 284.9243 - val_mse: 284.9243 - val_mae: 13.3124\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.5421 - mse: 292.5421 - mae: 13.3922 - val_loss: 284.7451 - val_mse: 284.7451 - val_mae: 13.3123\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.3291 - mse: 292.3291 - mae: 13.4024 - val_loss: 284.4449 - val_mse: 284.4449 - val_mae: 13.2965\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.0417 - mse: 292.0417 - mae: 13.3884 - val_loss: 284.1924 - val_mse: 284.1924 - val_mae: 13.2867\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.8743 - mse: 291.8743 - mae: 13.3670 - val_loss: 283.9084 - val_mse: 283.9084 - val_mae: 13.2798\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.5959 - mse: 291.5959 - mae: 13.3733 - val_loss: 283.5242 - val_mse: 283.5242 - val_mae: 13.2612\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.4268 - mse: 291.4268 - mae: 13.3563 - val_loss: 283.3309 - val_mse: 283.3309 - val_mae: 13.2534\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.1028 - mse: 291.1028 - mae: 13.3680 - val_loss: 282.9449 - val_mse: 282.9449 - val_mae: 13.2252\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.9725 - mse: 290.9725 - mae: 13.3200 - val_loss: 282.7125 - val_mse: 282.7125 - val_mae: 13.2166\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.7675 - mse: 290.7675 - mae: 13.3079 - val_loss: 282.5363 - val_mse: 282.5363 - val_mae: 13.2174\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.5454 - mse: 290.5454 - mae: 13.3214 - val_loss: 282.3080 - val_mse: 282.3080 - val_mae: 13.2081\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.3666 - mse: 290.3666 - mae: 13.2998 - val_loss: 282.1541 - val_mse: 282.1541 - val_mae: 13.2072\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.1516 - mse: 290.1516 - mae: 13.3043 - val_loss: 281.8534 - val_mse: 281.8534 - val_mae: 13.1895\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.0013 - mse: 290.0013 - mae: 13.3089 - val_loss: 281.5079 - val_mse: 281.5079 - val_mae: 13.1669\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.9289 - mse: 289.9289 - mae: 13.2825 - val_loss: 281.3293 - val_mse: 281.3293 - val_mae: 13.1568\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.6992 - mse: 289.6992 - mae: 13.2623 - val_loss: 281.2191 - val_mse: 281.2191 - val_mae: 13.1534\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.4199 - mse: 289.4199 - mae: 13.2557 - val_loss: 280.9427 - val_mse: 280.9427 - val_mae: 13.1415\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.2731 - mse: 289.2731 - mae: 13.2451 - val_loss: 280.7613 - val_mse: 280.7613 - val_mae: 13.1402\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.3144 - mse: 289.3144 - mae: 13.2527 - val_loss: 280.7772 - val_mse: 280.7772 - val_mae: 13.1422\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.9959 - mse: 288.9959 - mae: 13.2456 - val_loss: 280.5704 - val_mse: 280.5704 - val_mae: 13.1351\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.9149 - mse: 288.9149 - mae: 13.2422 - val_loss: 280.4603 - val_mse: 280.4603 - val_mae: 13.1353\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.6973 - mse: 288.6973 - mae: 13.2562 - val_loss: 280.0670 - val_mse: 280.0670 - val_mae: 13.1144\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.5930 - mse: 288.5930 - mae: 13.2453 - val_loss: 279.8702 - val_mse: 279.8702 - val_mae: 13.1005\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.4756 - mse: 288.4756 - mae: 13.2133 - val_loss: 279.7139 - val_mse: 279.7139 - val_mae: 13.0965\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.2646 - mse: 288.2646 - mae: 13.2148 - val_loss: 279.4783 - val_mse: 279.4783 - val_mae: 13.0863\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.1605 - mse: 288.1605 - mae: 13.2069 - val_loss: 279.3255 - val_mse: 279.3255 - val_mae: 13.0769\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.9927 - mse: 287.9927 - mae: 13.1946 - val_loss: 279.1603 - val_mse: 279.1603 - val_mae: 13.0699\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.8665 - mse: 287.8665 - mae: 13.1871 - val_loss: 279.0152 - val_mse: 279.0152 - val_mae: 13.0644\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.7698 - mse: 287.7698 - mae: 13.1985 - val_loss: 278.7930 - val_mse: 278.7930 - val_mae: 13.0561\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpq5h6bok7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 3350]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 461.1450 - mse: 461.1450 - mae: 18.3011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 348.4405 - mse: 348.4405 - mae: 15.1654 - val_loss: 184.0385 - val_mse: 184.0385 - val_mae: 10.6790\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 260.3710 - mse: 260.3710 - mae: 12.1774 - val_loss: 171.0919 - val_mse: 171.0919 - val_mae: 9.8865\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 252.3988 - mse: 252.3988 - mae: 11.8492 - val_loss: 164.2862 - val_mse: 164.2862 - val_mae: 9.5122\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 248.6192 - mse: 248.6192 - mae: 11.6679 - val_loss: 166.0399 - val_mse: 166.0399 - val_mae: 9.6335\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 246.3265 - mse: 246.3265 - mae: 11.6261 - val_loss: 160.2553 - val_mse: 160.2553 - val_mae: 9.2335\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 241.6175 - mse: 241.6175 - mae: 11.4144 - val_loss: 156.0632 - val_mse: 156.0632 - val_mae: 8.9984\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 239.8948 - mse: 239.8948 - mae: 11.3715 - val_loss: 154.8341 - val_mse: 154.8341 - val_mae: 8.9279\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 237.1994 - mse: 237.1994 - mae: 11.2596 - val_loss: 167.3692 - val_mse: 167.3692 - val_mae: 9.9926\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 234.6717 - mse: 234.6717 - mae: 11.1861 - val_loss: 168.2796 - val_mse: 168.2796 - val_mae: 10.0509\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 233.5122 - mse: 233.5122 - mae: 11.2053 - val_loss: 155.0728 - val_mse: 155.0728 - val_mae: 9.1140\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 232.2529 - mse: 232.2529 - mae: 11.1412 - val_loss: 150.2227 - val_mse: 150.2227 - val_mae: 8.7194\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 228.3012 - mse: 228.3012 - mae: 11.0119 - val_loss: 144.0159 - val_mse: 144.0159 - val_mae: 8.4333\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 221.8379 - mse: 221.8379 - mae: 10.7654 - val_loss: 149.4653 - val_mse: 149.4653 - val_mae: 9.1045\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 218.6009 - mse: 218.6009 - mae: 10.7819 - val_loss: 136.5348 - val_mse: 136.5348 - val_mae: 8.1549\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.3483 - mse: 213.3483 - mae: 10.5426 - val_loss: 135.5739 - val_mse: 135.5739 - val_mae: 8.0353\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.2305 - mse: 213.2305 - mae: 10.5544 - val_loss: 136.7796 - val_mse: 136.7796 - val_mae: 8.1212\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 214.8558 - mse: 214.8558 - mae: 10.4474 - val_loss: 138.1889 - val_mse: 138.1889 - val_mae: 8.3289\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.9776 - mse: 213.9776 - mae: 10.5503 - val_loss: 135.6771 - val_mse: 135.6771 - val_mae: 8.0396\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.8409 - mse: 213.8409 - mae: 10.4883 - val_loss: 140.0353 - val_mse: 140.0353 - val_mae: 8.4968\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 214.0070 - mse: 214.0070 - mae: 10.5271 - val_loss: 142.6847 - val_mse: 142.6847 - val_mae: 8.7350\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 212.5764 - mse: 212.5764 - mae: 10.5207 - val_loss: 139.9278 - val_mse: 139.9278 - val_mae: 8.4984\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 212.8437 - mse: 212.8437 - mae: 10.4903 - val_loss: 135.0250 - val_mse: 135.0250 - val_mae: 8.0095\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 213.3294 - mse: 213.3294 - mae: 10.4565 - val_loss: 138.5864 - val_mse: 138.5864 - val_mae: 8.3892\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 211.7279 - mse: 211.7279 - mae: 10.3924 - val_loss: 139.4507 - val_mse: 139.4507 - val_mae: 8.6102\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 207.5805 - mse: 207.5805 - mae: 10.3156 - val_loss: 132.3720 - val_mse: 132.3720 - val_mae: 7.9793\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.8515 - mse: 206.8515 - mae: 10.2034 - val_loss: 137.5507 - val_mse: 137.5507 - val_mae: 8.4837\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 209.0005 - mse: 209.0005 - mae: 10.4045 - val_loss: 131.6951 - val_mse: 131.6951 - val_mae: 7.8665\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 208.8195 - mse: 208.8195 - mae: 10.2556 - val_loss: 131.5398 - val_mse: 131.5398 - val_mae: 7.8780\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 207.3264 - mse: 207.3264 - mae: 10.2411 - val_loss: 133.4038 - val_mse: 133.4038 - val_mae: 8.0844\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 207.0839 - mse: 207.0839 - mae: 10.2604 - val_loss: 130.7727 - val_mse: 130.7727 - val_mae: 7.7995\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.4935 - mse: 206.4935 - mae: 10.2144 - val_loss: 132.1615 - val_mse: 132.1615 - val_mae: 7.8547\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 208.4228 - mse: 208.4228 - mae: 10.1131 - val_loss: 140.0209 - val_mse: 140.0209 - val_mae: 8.6947\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 208.0854 - mse: 208.0854 - mae: 10.3321 - val_loss: 132.4284 - val_mse: 132.4284 - val_mae: 7.9957\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 205.4155 - mse: 205.4155 - mae: 10.1639 - val_loss: 137.7970 - val_mse: 137.7970 - val_mae: 8.5199\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.6857 - mse: 206.6857 - mae: 10.2536 - val_loss: 136.7447 - val_mse: 136.7447 - val_mae: 8.4223\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.6426 - mse: 206.6426 - mae: 10.1791 - val_loss: 132.7925 - val_mse: 132.7925 - val_mae: 8.0314\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.4639 - mse: 206.4639 - mae: 10.2736 - val_loss: 130.8599 - val_mse: 130.8599 - val_mae: 7.7677\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 208.4791 - mse: 208.4791 - mae: 10.2500 - val_loss: 130.3986 - val_mse: 130.3986 - val_mae: 7.7939\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 205.7621 - mse: 205.7621 - mae: 10.1412 - val_loss: 131.5083 - val_mse: 131.5083 - val_mae: 7.9133\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 206.0411 - mse: 206.0411 - mae: 10.2269 - val_loss: 138.1079 - val_mse: 138.1079 - val_mae: 8.5539\n",
      "Epoch 40: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp8nbt_bw7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 552.4747 - mse: 552.4747 - mae: 20.2481"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: 400.9789 - mse: 400.9789 - mae: 16.3481 - val_loss: 212.2459 - val_mse: 212.2459 - val_mae: 11.6748\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 305.1158 - mse: 305.1158 - mae: 13.6054 - val_loss: 174.1400 - val_mse: 174.1400 - val_mae: 10.1155\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 274.2186 - mse: 274.2186 - mae: 12.5561 - val_loss: 166.6996 - val_mse: 166.6996 - val_mae: 9.8474\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 259.3647 - mse: 259.3647 - mae: 12.0970 - val_loss: 159.7293 - val_mse: 159.7293 - val_mae: 9.4958\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 256.2648 - mse: 256.2648 - mae: 11.9974 - val_loss: 154.1846 - val_mse: 154.1846 - val_mae: 9.0980\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 253.4474 - mse: 253.4474 - mae: 11.8364 - val_loss: 147.4987 - val_mse: 147.4987 - val_mae: 8.7288\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 249.4044 - mse: 249.4044 - mae: 11.7063 - val_loss: 144.2448 - val_mse: 144.2448 - val_mae: 8.6180\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 247.5130 - mse: 247.5130 - mae: 11.5543 - val_loss: 153.9665 - val_mse: 153.9665 - val_mae: 9.4009\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 238.3461 - mse: 238.3461 - mae: 11.3620 - val_loss: 151.4875 - val_mse: 151.4875 - val_mae: 9.3374\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 235.6439 - mse: 235.6439 - mae: 11.2928 - val_loss: 150.6468 - val_mse: 150.6468 - val_mae: 9.2785\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 236.5310 - mse: 236.5310 - mae: 11.3823 - val_loss: 139.0864 - val_mse: 139.0864 - val_mae: 8.3847\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 234.4436 - mse: 234.4436 - mae: 11.1936 - val_loss: 139.8201 - val_mse: 139.8201 - val_mae: 8.5952\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 230.8866 - mse: 230.8866 - mae: 11.1101 - val_loss: 138.0761 - val_mse: 138.0761 - val_mae: 8.4592\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 230.9603 - mse: 230.9603 - mae: 11.1145 - val_loss: 136.1691 - val_mse: 136.1691 - val_mae: 8.3111\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 229.3787 - mse: 229.3787 - mae: 11.0446 - val_loss: 133.2915 - val_mse: 133.2915 - val_mae: 8.0346\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 228.7293 - mse: 228.7293 - mae: 10.9666 - val_loss: 132.4636 - val_mse: 132.4636 - val_mae: 7.9935\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 229.0665 - mse: 229.0665 - mae: 10.9336 - val_loss: 139.8387 - val_mse: 139.8387 - val_mae: 8.7536\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 224.4807 - mse: 224.4807 - mae: 10.9027 - val_loss: 124.6228 - val_mse: 124.6228 - val_mae: 7.9054\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 212.0351 - mse: 212.0351 - mae: 10.4749 - val_loss: 123.1080 - val_mse: 123.1080 - val_mae: 7.8274\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 210.6059 - mse: 210.6059 - mae: 10.4248 - val_loss: 126.4937 - val_mse: 126.4937 - val_mae: 8.1694\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 205.9094 - mse: 205.9094 - mae: 10.3433 - val_loss: 114.1963 - val_mse: 114.1963 - val_mae: 7.3902\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 200.8355 - mse: 200.8355 - mae: 10.1055 - val_loss: 110.2871 - val_mse: 110.2871 - val_mae: 7.2162\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 196.4184 - mse: 196.4184 - mae: 9.9419 - val_loss: 110.9089 - val_mse: 110.9089 - val_mae: 7.2994\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 198.7199 - mse: 198.7199 - mae: 9.9619 - val_loss: 120.7988 - val_mse: 120.7988 - val_mae: 8.1054\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 196.5259 - mse: 196.5259 - mae: 10.0775 - val_loss: 111.7142 - val_mse: 111.7142 - val_mae: 7.4000\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 198.1816 - mse: 198.1816 - mae: 10.0612 - val_loss: 115.0370 - val_mse: 115.0370 - val_mae: 7.6785\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 197.4207 - mse: 197.4207 - mae: 10.0426 - val_loss: 107.9394 - val_mse: 107.9394 - val_mae: 6.9700\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 198.7235 - mse: 198.7235 - mae: 10.0167 - val_loss: 106.8082 - val_mse: 106.8082 - val_mae: 6.8545\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 191.2438 - mse: 191.2438 - mae: 9.7589 - val_loss: 100.8157 - val_mse: 100.8157 - val_mae: 6.7084\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 185.5685 - mse: 185.5685 - mae: 9.6234 - val_loss: 101.1598 - val_mse: 101.1598 - val_mae: 6.7734\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 188.3976 - mse: 188.3976 - mae: 9.7943 - val_loss: 104.4131 - val_mse: 104.4131 - val_mae: 6.9594\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 189.1555 - mse: 189.1555 - mae: 9.4121 - val_loss: 124.4227 - val_mse: 124.4227 - val_mae: 8.6541\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 181.2306 - mse: 181.2306 - mae: 9.6042 - val_loss: 97.9729 - val_mse: 97.9729 - val_mae: 6.8740\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 175.5229 - mse: 175.5229 - mae: 9.3183 - val_loss: 110.6416 - val_mse: 110.6416 - val_mae: 7.8773\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.2884 - mse: 176.2884 - mae: 9.4214 - val_loss: 99.1882 - val_mse: 99.1882 - val_mae: 6.9685\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 175.6313 - mse: 175.6313 - mae: 9.3196 - val_loss: 95.5094 - val_mse: 95.5094 - val_mae: 6.6346\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.5366 - mse: 177.5366 - mae: 9.4727 - val_loss: 92.3025 - val_mse: 92.3025 - val_mae: 6.2643\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.5492 - mse: 177.5492 - mae: 9.2892 - val_loss: 92.9324 - val_mse: 92.9324 - val_mae: 6.3682\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.9374 - mse: 174.9374 - mae: 9.2701 - val_loss: 92.1721 - val_mse: 92.1721 - val_mae: 6.2826\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.8086 - mse: 174.8086 - mae: 9.2747 - val_loss: 99.0229 - val_mse: 99.0229 - val_mae: 6.9583\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 175.9076 - mse: 175.9076 - mae: 9.3358 - val_loss: 91.5489 - val_mse: 91.5489 - val_mae: 6.2205\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.3595 - mse: 176.3595 - mae: 9.3549 - val_loss: 91.8401 - val_mse: 91.8401 - val_mae: 6.1921\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.2328 - mse: 176.2328 - mae: 9.3835 - val_loss: 91.4452 - val_mse: 91.4452 - val_mae: 6.1930\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.3711 - mse: 174.3711 - mae: 9.1719 - val_loss: 95.7052 - val_mse: 95.7052 - val_mae: 6.6660\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 175.8597 - mse: 175.8597 - mae: 9.4988 - val_loss: 91.6912 - val_mse: 91.6912 - val_mae: 6.1898\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 180.5510 - mse: 180.5510 - mae: 9.6440 - val_loss: 98.4848 - val_mse: 98.4848 - val_mae: 6.7589\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 182.3634 - mse: 182.3634 - mae: 9.4854 - val_loss: 95.1279 - val_mse: 95.1279 - val_mae: 6.6117\n",
      "Epoch 47: early stopping\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp1gjfhjdl/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 16875]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 540.6714 - mse: 540.6714 - mae: 19.8765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 19ms/step - loss: 349.4176 - mse: 349.4176 - mae: 15.3446 - val_loss: 279.3661 - val_mse: 279.3661 - val_mae: 13.3465\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 276.1696 - mse: 276.1696 - mae: 13.0191 - val_loss: 256.7347 - val_mse: 256.7347 - val_mae: 12.3938\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 264.6652 - mse: 264.6652 - mae: 12.5036 - val_loss: 245.8429 - val_mse: 245.8429 - val_mae: 11.9830\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 250.0318 - mse: 250.0318 - mae: 11.9227 - val_loss: 238.7755 - val_mse: 238.7755 - val_mae: 11.7562\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 246.9072 - mse: 246.9072 - mae: 11.7932 - val_loss: 237.3341 - val_mse: 237.3341 - val_mae: 11.6343\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 245.0816 - mse: 245.0816 - mae: 11.7101 - val_loss: 232.5558 - val_mse: 232.5558 - val_mae: 11.4384\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 247.7339 - mse: 247.7339 - mae: 11.8799 - val_loss: 230.5403 - val_mse: 230.5403 - val_mae: 11.3949\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 239.8322 - mse: 239.8322 - mae: 11.6221 - val_loss: 225.9808 - val_mse: 225.9808 - val_mae: 11.3224\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 229.3517 - mse: 229.3517 - mae: 11.1808 - val_loss: 212.7436 - val_mse: 212.7436 - val_mae: 10.7544\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 217.4094 - mse: 217.4094 - mae: 10.8363 - val_loss: 197.2092 - val_mse: 197.2092 - val_mae: 10.3174\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 203.0084 - mse: 203.0084 - mae: 10.3679 - val_loss: 189.9007 - val_mse: 189.9007 - val_mae: 10.1328\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 198.0169 - mse: 198.0169 - mae: 10.2221 - val_loss: 190.5768 - val_mse: 190.5768 - val_mae: 10.2024\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 195.2657 - mse: 195.2657 - mae: 10.0200 - val_loss: 179.9357 - val_mse: 179.9357 - val_mae: 9.6851\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 190.5752 - mse: 190.5752 - mae: 9.8398 - val_loss: 176.8460 - val_mse: 176.8460 - val_mae: 9.5288\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 188.0737 - mse: 188.0737 - mae: 9.7704 - val_loss: 178.2498 - val_mse: 178.2498 - val_mae: 9.5680\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 188.9287 - mse: 188.9287 - mae: 9.7851 - val_loss: 178.1825 - val_mse: 178.1825 - val_mae: 9.6486\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 186.9786 - mse: 186.9786 - mae: 9.7882 - val_loss: 176.4304 - val_mse: 176.4304 - val_mae: 9.5083\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 185.9419 - mse: 185.9419 - mae: 9.6987 - val_loss: 170.0460 - val_mse: 170.0460 - val_mae: 9.2455\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 181.1032 - mse: 181.1032 - mae: 9.5883 - val_loss: 169.3597 - val_mse: 169.3597 - val_mae: 9.2781\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 180.7399 - mse: 180.7399 - mae: 9.4882 - val_loss: 166.1163 - val_mse: 166.1163 - val_mae: 9.1537\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 182.8176 - mse: 182.8176 - mae: 9.6738 - val_loss: 166.4828 - val_mse: 166.4828 - val_mae: 9.1704\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 178.2577 - mse: 178.2577 - mae: 9.3044 - val_loss: 167.4448 - val_mse: 167.4448 - val_mae: 9.3101\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 179.2859 - mse: 179.2859 - mae: 9.6459 - val_loss: 167.5866 - val_mse: 167.5866 - val_mae: 9.2429\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.5891 - mse: 177.5891 - mae: 9.2797 - val_loss: 167.3939 - val_mse: 167.3939 - val_mae: 9.3229\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.0535 - mse: 177.0535 - mae: 9.3946 - val_loss: 163.1584 - val_mse: 163.1584 - val_mae: 9.0155\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.0854 - mse: 176.0854 - mae: 9.2657 - val_loss: 165.2799 - val_mse: 165.2799 - val_mae: 9.1708\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.0594 - mse: 177.0594 - mae: 9.4271 - val_loss: 165.7645 - val_mse: 165.7645 - val_mae: 9.1109\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 178.2470 - mse: 178.2470 - mae: 9.3906 - val_loss: 164.3146 - val_mse: 164.3146 - val_mae: 9.0808\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.1584 - mse: 176.1584 - mae: 9.3124 - val_loss: 162.5129 - val_mse: 162.5129 - val_mae: 8.9459\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.0123 - mse: 176.0123 - mae: 9.2421 - val_loss: 163.1578 - val_mse: 163.1578 - val_mae: 8.9833\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.8375 - mse: 174.8375 - mae: 9.2144 - val_loss: 161.4102 - val_mse: 161.4102 - val_mae: 8.9107\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.3844 - mse: 177.3844 - mae: 9.3326 - val_loss: 162.8104 - val_mse: 162.8104 - val_mae: 8.9837\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.8634 - mse: 174.8634 - mae: 9.2770 - val_loss: 165.3283 - val_mse: 165.3283 - val_mae: 9.1260\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.8463 - mse: 177.8463 - mae: 9.3445 - val_loss: 161.4559 - val_mse: 161.4559 - val_mae: 8.8741\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 173.8056 - mse: 173.8056 - mae: 9.1488 - val_loss: 161.0790 - val_mse: 161.0790 - val_mae: 8.8361\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.4540 - mse: 174.4540 - mae: 9.1556 - val_loss: 162.0363 - val_mse: 162.0363 - val_mae: 8.9255\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 176.3445 - mse: 176.3445 - mae: 9.3487 - val_loss: 165.4580 - val_mse: 165.4580 - val_mae: 9.1583\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.5600 - mse: 174.5600 - mae: 9.2222 - val_loss: 160.3936 - val_mse: 160.3936 - val_mae: 8.9495\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 174.8328 - mse: 174.8328 - mae: 9.2529 - val_loss: 160.8576 - val_mse: 160.8576 - val_mae: 9.0257\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 169.9238 - mse: 169.9238 - mae: 9.0993 - val_loss: 156.5032 - val_mse: 156.5032 - val_mae: 8.6643\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 170.3429 - mse: 170.3429 - mae: 9.0875 - val_loss: 162.1717 - val_mse: 162.1717 - val_mae: 9.0952\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 169.3403 - mse: 169.3403 - mae: 8.8420 - val_loss: 195.2739 - val_mse: 195.2739 - val_mae: 11.0694\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 177.2966 - mse: 177.2966 - mae: 9.4279 - val_loss: 155.9150 - val_mse: 155.9150 - val_mae: 8.6609\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 170.8949 - mse: 170.8949 - mae: 9.0918 - val_loss: 159.3475 - val_mse: 159.3475 - val_mae: 8.8591\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 169.8793 - mse: 169.8793 - mae: 8.9372 - val_loss: 155.4558 - val_mse: 155.4558 - val_mae: 8.6226\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 168.1765 - mse: 168.1765 - mae: 8.9091 - val_loss: 159.0271 - val_mse: 159.0271 - val_mae: 8.8463\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 168.5644 - mse: 168.5644 - mae: 8.9495 - val_loss: 158.2161 - val_mse: 158.2161 - val_mae: 8.8031\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 169.3129 - mse: 169.3129 - mae: 9.0612 - val_loss: 161.4103 - val_mse: 161.4103 - val_mae: 9.0499\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 171.4218 - mse: 171.4218 - mae: 9.1052 - val_loss: 156.4568 - val_mse: 156.4568 - val_mae: 8.6863\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 168.9093 - mse: 168.9093 - mae: 8.9905 - val_loss: 158.7872 - val_mse: 158.7872 - val_mae: 8.8541\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 169.0360 - mse: 169.0360 - mae: 9.0819 - val_loss: 155.4285 - val_mse: 155.4285 - val_mae: 8.6617\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 169.2005 - mse: 169.2005 - mae: 8.9365 - val_loss: 152.8597 - val_mse: 152.8597 - val_mae: 8.5683\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 164.0872 - mse: 164.0872 - mae: 8.7696 - val_loss: 153.3680 - val_mse: 153.3680 - val_mae: 8.7128\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 160.7944 - mse: 160.7944 - mae: 8.6831 - val_loss: 151.3774 - val_mse: 151.3774 - val_mae: 8.6494\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 158.0295 - mse: 158.0295 - mae: 8.6969 - val_loss: 135.7480 - val_mse: 135.7480 - val_mae: 7.9649\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 148.1597 - mse: 148.1597 - mae: 8.3443 - val_loss: 135.3927 - val_mse: 135.3927 - val_mae: 8.0638\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 147.3446 - mse: 147.3446 - mae: 8.2826 - val_loss: 133.0815 - val_mse: 133.0815 - val_mae: 7.9428\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 145.0033 - mse: 145.0033 - mae: 8.1908 - val_loss: 130.8008 - val_mse: 130.8008 - val_mae: 7.9051\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 143.5617 - mse: 143.5617 - mae: 8.2587 - val_loss: 127.9574 - val_mse: 127.9574 - val_mae: 7.6943\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 142.0375 - mse: 142.0375 - mae: 8.2271 - val_loss: 141.0650 - val_mse: 141.0650 - val_mae: 8.6375\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 142.5243 - mse: 142.5243 - mae: 7.9477 - val_loss: 130.9012 - val_mse: 130.9012 - val_mae: 7.9077\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 139.8079 - mse: 139.8079 - mae: 8.0483 - val_loss: 125.5142 - val_mse: 125.5142 - val_mae: 7.5344\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 139.7203 - mse: 139.7203 - mae: 7.9816 - val_loss: 130.9409 - val_mse: 130.9409 - val_mae: 7.9696\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 141.0685 - mse: 141.0685 - mae: 8.0121 - val_loss: 125.4479 - val_mse: 125.4479 - val_mae: 7.5911\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 139.9366 - mse: 139.9366 - mae: 7.9357 - val_loss: 125.9418 - val_mse: 125.9418 - val_mae: 7.5234\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 142.6804 - mse: 142.6804 - mae: 8.0846 - val_loss: 128.9695 - val_mse: 128.9695 - val_mae: 7.7519\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 142.2897 - mse: 142.2897 - mae: 8.0636 - val_loss: 131.8937 - val_mse: 131.8937 - val_mae: 7.9826\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 141.3267 - mse: 141.3267 - mae: 8.1070 - val_loss: 129.1131 - val_mse: 129.1131 - val_mae: 7.8389\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 139.3155 - mse: 139.3155 - mae: 7.9311 - val_loss: 125.1022 - val_mse: 125.1022 - val_mae: 7.5592\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 140.4511 - mse: 140.4511 - mae: 8.1463 - val_loss: 137.5854 - val_mse: 137.5854 - val_mae: 8.4413\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 141.6881 - mse: 141.6881 - mae: 8.0797 - val_loss: 126.9838 - val_mse: 126.9838 - val_mae: 7.5556\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 139.5060 - mse: 139.5060 - mae: 7.9149 - val_loss: 126.2630 - val_mse: 126.2630 - val_mae: 7.6139\n",
      "Epoch 72: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpab9dx9ow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 16750]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 1848.4435 - mse: 1848.4435 - mae: 29.9756"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 18ms/step - loss: 1741.5387 - mse: 1741.5387 - mae: 29.7394 - val_loss: 1700.0107 - val_mse: 1700.0107 - val_mae: 29.4952\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1152.5437 - mse: 1152.5437 - mae: 26.0926 - val_loss: 1242.8890 - val_mse: 1242.8890 - val_mae: 26.5935\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 910.2637 - mse: 910.2637 - mae: 24.1428 - val_loss: 995.2198 - val_mse: 995.2198 - val_mae: 24.4074\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 785.5444 - mse: 785.5444 - mae: 22.9348 - val_loss: 866.2850 - val_mse: 866.2850 - val_mae: 23.3660\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 721.2790 - mse: 721.2790 - mae: 22.3218 - val_loss: 782.3762 - val_mse: 782.3762 - val_mae: 22.7692\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 681.3708 - mse: 681.3708 - mae: 22.0170 - val_loss: 725.0934 - val_mse: 725.0934 - val_mae: 22.3994\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 652.7878 - mse: 652.7878 - mae: 21.8062 - val_loss: 676.5671 - val_mse: 676.5671 - val_mae: 22.0026\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 627.7003 - mse: 627.7003 - mae: 21.5364 - val_loss: 635.5756 - val_mse: 635.5756 - val_mae: 21.5648\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 607.2039 - mse: 607.2039 - mae: 21.3114 - val_loss: 601.7837 - val_mse: 601.7837 - val_mae: 21.1455\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 589.7664 - mse: 589.7664 - mae: 21.0806 - val_loss: 573.3514 - val_mse: 573.3514 - val_mae: 20.8164\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 575.2552 - mse: 575.2552 - mae: 20.8775 - val_loss: 551.7175 - val_mse: 551.7175 - val_mae: 20.5805\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 563.8223 - mse: 563.8223 - mae: 20.6934 - val_loss: 533.2669 - val_mse: 533.2669 - val_mae: 20.3471\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 554.2849 - mse: 554.2849 - mae: 20.5469 - val_loss: 518.3990 - val_mse: 518.3990 - val_mae: 20.1480\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 546.2852 - mse: 546.2852 - mae: 20.4382 - val_loss: 505.5182 - val_mse: 505.5182 - val_mae: 19.9866\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 538.8893 - mse: 538.8893 - mae: 20.3165 - val_loss: 493.4887 - val_mse: 493.4887 - val_mae: 19.8253\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 532.1269 - mse: 532.1269 - mae: 20.2089 - val_loss: 481.6784 - val_mse: 481.6784 - val_mae: 19.6576\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 525.8510 - mse: 525.8510 - mae: 20.1150 - val_loss: 471.1876 - val_mse: 471.1876 - val_mae: 19.4923\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 519.8857 - mse: 519.8857 - mae: 20.0140 - val_loss: 461.2890 - val_mse: 461.2890 - val_mae: 19.3255\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 514.2065 - mse: 514.2065 - mae: 19.9162 - val_loss: 452.1692 - val_mse: 452.1692 - val_mae: 19.1674\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 509.0985 - mse: 509.0985 - mae: 19.8166 - val_loss: 444.7818 - val_mse: 444.7818 - val_mae: 19.0299\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 504.5832 - mse: 504.5832 - mae: 19.7295 - val_loss: 437.7258 - val_mse: 437.7258 - val_mae: 18.8955\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 500.3893 - mse: 500.3893 - mae: 19.6482 - val_loss: 430.8575 - val_mse: 430.8575 - val_mae: 18.7634\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 496.1724 - mse: 496.1724 - mae: 19.5705 - val_loss: 424.4016 - val_mse: 424.4016 - val_mae: 18.6278\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 492.2511 - mse: 492.2511 - mae: 19.4906 - val_loss: 419.0487 - val_mse: 419.0487 - val_mae: 18.5087\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 488.7422 - mse: 488.7422 - mae: 19.4125 - val_loss: 413.6797 - val_mse: 413.6797 - val_mae: 18.3922\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 485.4065 - mse: 485.4065 - mae: 19.3392 - val_loss: 408.8368 - val_mse: 408.8368 - val_mae: 18.2830\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 482.2875 - mse: 482.2875 - mae: 19.2754 - val_loss: 404.4503 - val_mse: 404.4503 - val_mae: 18.1771\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 479.3928 - mse: 479.3928 - mae: 19.2099 - val_loss: 400.2798 - val_mse: 400.2798 - val_mae: 18.0866\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 476.5905 - mse: 476.5905 - mae: 19.1485 - val_loss: 395.9842 - val_mse: 395.9842 - val_mae: 17.9991\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 473.7582 - mse: 473.7582 - mae: 19.0801 - val_loss: 392.0305 - val_mse: 392.0305 - val_mae: 17.9178\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 471.1514 - mse: 471.1514 - mae: 19.0224 - val_loss: 388.4967 - val_mse: 388.4967 - val_mae: 17.8445\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 468.7095 - mse: 468.7095 - mae: 18.9660 - val_loss: 385.1898 - val_mse: 385.1898 - val_mae: 17.7724\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 466.3753 - mse: 466.3753 - mae: 18.9086 - val_loss: 381.9635 - val_mse: 381.9635 - val_mae: 17.6996\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 464.1155 - mse: 464.1155 - mae: 18.8512 - val_loss: 378.9703 - val_mse: 378.9703 - val_mae: 17.6306\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 462.0824 - mse: 462.0824 - mae: 18.7991 - val_loss: 376.0334 - val_mse: 376.0334 - val_mae: 17.5661\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 459.9391 - mse: 459.9391 - mae: 18.7393 - val_loss: 373.0898 - val_mse: 373.0898 - val_mae: 17.5030\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 457.9230 - mse: 457.9230 - mae: 18.6944 - val_loss: 370.3013 - val_mse: 370.3013 - val_mae: 17.4394\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 455.9543 - mse: 455.9543 - mae: 18.6426 - val_loss: 367.7373 - val_mse: 367.7373 - val_mae: 17.3793\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 454.1223 - mse: 454.1223 - mae: 18.5933 - val_loss: 365.2279 - val_mse: 365.2279 - val_mae: 17.3180\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 452.3121 - mse: 452.3121 - mae: 18.5469 - val_loss: 362.7693 - val_mse: 362.7693 - val_mae: 17.2550\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 450.4131 - mse: 450.4131 - mae: 18.4894 - val_loss: 360.4003 - val_mse: 360.4003 - val_mae: 17.1934\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 448.7187 - mse: 448.7187 - mae: 18.4406 - val_loss: 358.0125 - val_mse: 358.0125 - val_mae: 17.1307\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 447.0372 - mse: 447.0372 - mae: 18.3923 - val_loss: 355.6903 - val_mse: 355.6903 - val_mae: 17.0707\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 445.4776 - mse: 445.4776 - mae: 18.3507 - val_loss: 353.4910 - val_mse: 353.4910 - val_mae: 17.0092\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 443.9351 - mse: 443.9351 - mae: 18.3034 - val_loss: 351.4831 - val_mse: 351.4831 - val_mae: 16.9520\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 442.3690 - mse: 442.3690 - mae: 18.2561 - val_loss: 349.4227 - val_mse: 349.4227 - val_mae: 16.8938\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 440.9666 - mse: 440.9666 - mae: 18.2144 - val_loss: 347.3146 - val_mse: 347.3146 - val_mae: 16.8345\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 439.4345 - mse: 439.4345 - mae: 18.1682 - val_loss: 345.5328 - val_mse: 345.5328 - val_mae: 16.7792\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 438.0278 - mse: 438.0278 - mae: 18.1236 - val_loss: 343.7364 - val_mse: 343.7364 - val_mae: 16.7247\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 436.6729 - mse: 436.6729 - mae: 18.0806 - val_loss: 342.0039 - val_mse: 342.0039 - val_mae: 16.6709\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 435.3739 - mse: 435.3739 - mae: 18.0402 - val_loss: 340.2364 - val_mse: 340.2364 - val_mae: 16.6167\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 434.0886 - mse: 434.0886 - mae: 17.9968 - val_loss: 338.5651 - val_mse: 338.5651 - val_mae: 16.5645\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 432.8422 - mse: 432.8422 - mae: 17.9574 - val_loss: 336.9570 - val_mse: 336.9570 - val_mae: 16.5132\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 431.6336 - mse: 431.6336 - mae: 17.9163 - val_loss: 335.1971 - val_mse: 335.1971 - val_mae: 16.4585\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 430.3658 - mse: 430.3658 - mae: 17.8777 - val_loss: 333.6752 - val_mse: 333.6752 - val_mae: 16.4083\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 429.2723 - mse: 429.2723 - mae: 17.8400 - val_loss: 332.2413 - val_mse: 332.2413 - val_mae: 16.3600\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 428.1485 - mse: 428.1485 - mae: 17.8018 - val_loss: 330.6015 - val_mse: 330.6015 - val_mae: 16.3064\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 426.9320 - mse: 426.9320 - mae: 17.7610 - val_loss: 329.3420 - val_mse: 329.3420 - val_mae: 16.2663\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 425.8929 - mse: 425.8929 - mae: 17.7242 - val_loss: 327.9383 - val_mse: 327.9383 - val_mae: 16.2237\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 424.8456 - mse: 424.8456 - mae: 17.6884 - val_loss: 326.6401 - val_mse: 326.6401 - val_mae: 16.1821\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 423.8438 - mse: 423.8438 - mae: 17.6525 - val_loss: 325.4021 - val_mse: 325.4021 - val_mae: 16.1422\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 422.8361 - mse: 422.8361 - mae: 17.6162 - val_loss: 324.1589 - val_mse: 324.1589 - val_mae: 16.1020\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 421.8858 - mse: 421.8858 - mae: 17.5828 - val_loss: 322.8969 - val_mse: 322.8969 - val_mae: 16.0607\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 420.9179 - mse: 420.9179 - mae: 17.5485 - val_loss: 321.7845 - val_mse: 321.7845 - val_mae: 16.0223\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 420.0565 - mse: 420.0565 - mae: 17.5168 - val_loss: 320.5768 - val_mse: 320.5768 - val_mae: 15.9820\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 419.1289 - mse: 419.1289 - mae: 17.4837 - val_loss: 319.4555 - val_mse: 319.4555 - val_mae: 15.9432\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 418.2772 - mse: 418.2772 - mae: 17.4509 - val_loss: 318.3872 - val_mse: 318.3872 - val_mae: 15.9050\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 417.4366 - mse: 417.4366 - mae: 17.4193 - val_loss: 317.4398 - val_mse: 317.4398 - val_mae: 15.8703\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 416.6380 - mse: 416.6380 - mae: 17.3869 - val_loss: 316.4142 - val_mse: 316.4142 - val_mae: 15.8346\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 415.7907 - mse: 415.7907 - mae: 17.3575 - val_loss: 315.4384 - val_mse: 315.4384 - val_mae: 15.8020\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 415.0376 - mse: 415.0376 - mae: 17.3281 - val_loss: 314.4026 - val_mse: 314.4026 - val_mae: 15.7664\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 414.2985 - mse: 414.2985 - mae: 17.2988 - val_loss: 313.5793 - val_mse: 313.5793 - val_mae: 15.7380\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 413.5809 - mse: 413.5809 - mae: 17.2707 - val_loss: 312.6232 - val_mse: 312.6232 - val_mae: 15.7042\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 412.8777 - mse: 412.8777 - mae: 17.2426 - val_loss: 311.8310 - val_mse: 311.8310 - val_mae: 15.6760\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 412.1743 - mse: 412.1743 - mae: 17.2136 - val_loss: 310.9894 - val_mse: 310.9894 - val_mae: 15.6453\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 411.5527 - mse: 411.5527 - mae: 17.1903 - val_loss: 310.1114 - val_mse: 310.1114 - val_mae: 15.6128\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 410.8354 - mse: 410.8354 - mae: 17.1597 - val_loss: 309.3758 - val_mse: 309.3758 - val_mae: 15.5851\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 410.1964 - mse: 410.1964 - mae: 17.1335 - val_loss: 308.5907 - val_mse: 308.5907 - val_mae: 15.5552\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 409.5653 - mse: 409.5653 - mae: 17.1087 - val_loss: 307.8105 - val_mse: 307.8105 - val_mae: 15.5250\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 408.9947 - mse: 408.9947 - mae: 17.0839 - val_loss: 307.1051 - val_mse: 307.1051 - val_mae: 15.4970\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 408.4069 - mse: 408.4069 - mae: 17.0612 - val_loss: 306.2514 - val_mse: 306.2514 - val_mae: 15.4628\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 407.7702 - mse: 407.7702 - mae: 17.0341 - val_loss: 305.5907 - val_mse: 305.5907 - val_mae: 15.4355\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 407.2173 - mse: 407.2173 - mae: 17.0120 - val_loss: 304.9035 - val_mse: 304.9035 - val_mae: 15.4065\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 406.6645 - mse: 406.6645 - mae: 16.9881 - val_loss: 304.2676 - val_mse: 304.2676 - val_mae: 15.3799\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 406.1711 - mse: 406.1711 - mae: 16.9681 - val_loss: 303.6204 - val_mse: 303.6204 - val_mae: 15.3524\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 405.6364 - mse: 405.6364 - mae: 16.9465 - val_loss: 303.0649 - val_mse: 303.0649 - val_mae: 15.3282\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 405.1258 - mse: 405.1258 - mae: 16.9264 - val_loss: 302.3988 - val_mse: 302.3988 - val_mae: 15.2997\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 404.6465 - mse: 404.6465 - mae: 16.9044 - val_loss: 301.8399 - val_mse: 301.8399 - val_mae: 15.2748\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 404.2019 - mse: 404.2019 - mae: 16.8852 - val_loss: 301.2601 - val_mse: 301.2601 - val_mae: 15.2490\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 403.7125 - mse: 403.7125 - mae: 16.8666 - val_loss: 300.6714 - val_mse: 300.6714 - val_mae: 15.2227\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 403.2656 - mse: 403.2656 - mae: 16.8463 - val_loss: 300.1060 - val_mse: 300.1060 - val_mae: 15.1963\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 402.8292 - mse: 402.8292 - mae: 16.8300 - val_loss: 299.5320 - val_mse: 299.5320 - val_mae: 15.1701\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 402.3874 - mse: 402.3874 - mae: 16.8093 - val_loss: 299.0002 - val_mse: 299.0002 - val_mae: 15.1452\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 401.9760 - mse: 401.9760 - mae: 16.7923 - val_loss: 298.4886 - val_mse: 298.4886 - val_mae: 15.1209\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 401.5826 - mse: 401.5826 - mae: 16.7766 - val_loss: 297.9782 - val_mse: 297.9782 - val_mae: 15.0959\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 401.1687 - mse: 401.1687 - mae: 16.7581 - val_loss: 297.4447 - val_mse: 297.4447 - val_mae: 15.0703\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 400.7939 - mse: 400.7939 - mae: 16.7392 - val_loss: 297.0369 - val_mse: 297.0369 - val_mae: 15.0493\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 400.4070 - mse: 400.4070 - mae: 16.7256 - val_loss: 296.5428 - val_mse: 296.5428 - val_mae: 15.0247\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 400.0462 - mse: 400.0462 - mae: 16.7084 - val_loss: 296.1112 - val_mse: 296.1112 - val_mae: 15.0024\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 399.7194 - mse: 399.7194 - mae: 16.6931 - val_loss: 295.6890 - val_mse: 295.6890 - val_mae: 14.9801\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpv82dnpy_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 840.2097 - mse: 840.2097 - mae: 26.7043"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 32ms/step - loss: 790.7453 - mse: 790.7453 - mae: 25.5540 - val_loss: 584.4409 - val_mse: 584.4409 - val_mae: 22.6136\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 704.6283 - mse: 704.6283 - mae: 24.2967 - val_loss: 519.4584 - val_mse: 519.4584 - val_mae: 21.5628\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 652.5607 - mse: 652.5607 - mae: 23.4012 - val_loss: 478.6330 - val_mse: 478.6330 - val_mae: 20.7751\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 619.7462 - mse: 619.7462 - mae: 22.7421 - val_loss: 451.4644 - val_mse: 451.4644 - val_mae: 20.1600\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 597.7124 - mse: 597.7124 - mae: 22.2407 - val_loss: 432.4831 - val_mse: 432.4831 - val_mae: 19.6675\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 582.1782 - mse: 582.1782 - mae: 21.8505 - val_loss: 418.7722 - val_mse: 418.7722 - val_mae: 19.2829\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 570.7771 - mse: 570.7771 - mae: 21.5242 - val_loss: 408.8318 - val_mse: 408.8318 - val_mae: 19.0080\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 562.5161 - mse: 562.5161 - mae: 21.2807 - val_loss: 401.4568 - val_mse: 401.4568 - val_mae: 18.7873\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 556.2395 - mse: 556.2395 - mae: 21.0851 - val_loss: 395.7776 - val_mse: 395.7776 - val_mae: 18.6056\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 551.4496 - mse: 551.4496 - mae: 20.9227 - val_loss: 391.3232 - val_mse: 391.3232 - val_mae: 18.4527\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 547.7001 - mse: 547.7001 - mae: 20.7834 - val_loss: 387.5961 - val_mse: 387.5961 - val_mae: 18.3097\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 544.6045 - mse: 544.6045 - mae: 20.6614 - val_loss: 384.5313 - val_mse: 384.5313 - val_mae: 18.1804\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 542.1426 - mse: 542.1426 - mae: 20.5639 - val_loss: 381.9996 - val_mse: 381.9996 - val_mae: 18.0663\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 540.1255 - mse: 540.1255 - mae: 20.4830 - val_loss: 379.8798 - val_mse: 379.8798 - val_mae: 17.9633\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 538.4650 - mse: 538.4650 - mae: 20.4114 - val_loss: 378.1034 - val_mse: 378.1034 - val_mae: 17.8779\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 537.1081 - mse: 537.1081 - mae: 20.3499 - val_loss: 376.6471 - val_mse: 376.6471 - val_mae: 17.8143\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 536.0184 - mse: 536.0184 - mae: 20.3009 - val_loss: 375.4921 - val_mse: 375.4921 - val_mae: 17.7610\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 535.1460 - mse: 535.1460 - mae: 20.2598 - val_loss: 374.4157 - val_mse: 374.4157 - val_mae: 17.7080\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 534.3584 - mse: 534.3584 - mae: 20.2215 - val_loss: 373.5492 - val_mse: 373.5492 - val_mae: 17.6626\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 533.7383 - mse: 533.7383 - mae: 20.1884 - val_loss: 372.8339 - val_mse: 372.8339 - val_mae: 17.6230\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 533.2253 - mse: 533.2253 - mae: 20.1628 - val_loss: 372.1812 - val_mse: 372.1812 - val_mae: 17.5848\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 532.7785 - mse: 532.7785 - mae: 20.1380 - val_loss: 371.6062 - val_mse: 371.6062 - val_mae: 17.5493\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 532.3906 - mse: 532.3906 - mae: 20.1129 - val_loss: 371.1206 - val_mse: 371.1206 - val_mae: 17.5177\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 532.0736 - mse: 532.0736 - mae: 20.0928 - val_loss: 370.6970 - val_mse: 370.6970 - val_mae: 17.4887\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 531.8008 - mse: 531.8008 - mae: 20.0758 - val_loss: 370.3096 - val_mse: 370.3096 - val_mae: 17.4609\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 531.5591 - mse: 531.5591 - mae: 20.0557 - val_loss: 370.0033 - val_mse: 370.0033 - val_mae: 17.4379\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 531.3684 - mse: 531.3684 - mae: 20.0431 - val_loss: 369.7092 - val_mse: 369.7092 - val_mae: 17.4178\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 531.1967 - mse: 531.1967 - mae: 20.0304 - val_loss: 369.4556 - val_mse: 369.4556 - val_mae: 17.4000\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 531.0447 - mse: 531.0447 - mae: 20.0201 - val_loss: 369.2263 - val_mse: 369.2263 - val_mae: 17.3848\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.9186 - mse: 530.9186 - mae: 20.0090 - val_loss: 369.0240 - val_mse: 369.0240 - val_mae: 17.3721\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.8106 - mse: 530.8106 - mae: 20.0018 - val_loss: 368.8322 - val_mse: 368.8322 - val_mae: 17.3595\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.7094 - mse: 530.7094 - mae: 19.9935 - val_loss: 368.6988 - val_mse: 368.6988 - val_mae: 17.3505\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.6318 - mse: 530.6318 - mae: 19.9884 - val_loss: 368.5669 - val_mse: 368.5669 - val_mae: 17.3413\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.5668 - mse: 530.5668 - mae: 19.9823 - val_loss: 368.4554 - val_mse: 368.4554 - val_mae: 17.3334\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.5056 - mse: 530.5056 - mae: 19.9782 - val_loss: 368.3398 - val_mse: 368.3398 - val_mae: 17.3249\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.4503 - mse: 530.4503 - mae: 19.9743 - val_loss: 368.2339 - val_mse: 368.2339 - val_mae: 17.3168\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.3929 - mse: 530.3929 - mae: 19.9694 - val_loss: 368.1266 - val_mse: 368.1266 - val_mae: 17.3092\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.3424 - mse: 530.3424 - mae: 19.9657 - val_loss: 368.0254 - val_mse: 368.0254 - val_mae: 17.3024\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.2968 - mse: 530.2968 - mae: 19.9628 - val_loss: 367.9446 - val_mse: 367.9446 - val_mae: 17.2969\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.2570 - mse: 530.2570 - mae: 19.9585 - val_loss: 367.8786 - val_mse: 367.8786 - val_mae: 17.2921\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.2250 - mse: 530.2250 - mae: 19.9557 - val_loss: 367.8217 - val_mse: 367.8217 - val_mae: 17.2880\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.1968 - mse: 530.1968 - mae: 19.9540 - val_loss: 367.7585 - val_mse: 367.7585 - val_mae: 17.2832\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.1681 - mse: 530.1681 - mae: 19.9509 - val_loss: 367.6989 - val_mse: 367.6989 - val_mae: 17.2786\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.1357 - mse: 530.1357 - mae: 19.9487 - val_loss: 367.6571 - val_mse: 367.6571 - val_mae: 17.2754\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 530.1214 - mse: 530.1214 - mae: 19.9482 - val_loss: 367.6111 - val_mse: 367.6111 - val_mae: 17.2717\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.0925 - mse: 530.0925 - mae: 19.9455 - val_loss: 367.5582 - val_mse: 367.5582 - val_mae: 17.2674\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 530.0708 - mse: 530.0708 - mae: 19.9430 - val_loss: 367.5143 - val_mse: 367.5143 - val_mae: 17.2639\n",
      "Epoch 47: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpadonn0sr/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 675]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 1s - loss: 4292.5420 - mse: 4292.5420 - mae: 39.5734"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 21ms/step - loss: 3574.0984 - mse: 3574.0984 - mae: 34.8106 - val_loss: 1521.7480 - val_mse: 1521.7480 - val_mae: 26.3877\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1601.0530 - mse: 1601.0530 - mae: 25.3592 - val_loss: 796.6239 - val_mse: 796.6239 - val_mae: 21.1015\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1036.4062 - mse: 1036.4062 - mae: 21.7700 - val_loss: 560.6737 - val_mse: 560.6737 - val_mae: 18.9135\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 800.9456 - mse: 800.9456 - mae: 20.0051 - val_loss: 457.8560 - val_mse: 457.8560 - val_mae: 17.7780\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 676.7952 - mse: 676.7952 - mae: 18.9673 - val_loss: 404.4209 - val_mse: 404.4209 - val_mae: 16.9300\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 597.4321 - mse: 597.4321 - mae: 18.1082 - val_loss: 371.6214 - val_mse: 371.6214 - val_mae: 16.2980\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 541.0438 - mse: 541.0438 - mae: 17.4437 - val_loss: 350.0587 - val_mse: 350.0587 - val_mae: 15.8356\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 497.7442 - mse: 497.7442 - mae: 16.9287 - val_loss: 334.2846 - val_mse: 334.2846 - val_mae: 15.4514\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 465.3858 - mse: 465.3858 - mae: 16.4587 - val_loss: 322.6200 - val_mse: 322.6200 - val_mae: 15.1384\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 437.4883 - mse: 437.4883 - mae: 16.1079 - val_loss: 313.4984 - val_mse: 313.4984 - val_mae: 14.8361\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 416.3891 - mse: 416.3891 - mae: 15.7733 - val_loss: 306.8281 - val_mse: 306.8281 - val_mae: 14.5959\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 397.9076 - mse: 397.9076 - mae: 15.5262 - val_loss: 301.7782 - val_mse: 301.7782 - val_mae: 14.3797\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 382.4836 - mse: 382.4836 - mae: 15.3047 - val_loss: 298.3101 - val_mse: 298.3101 - val_mae: 14.2111\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 369.7823 - mse: 369.7823 - mae: 15.1082 - val_loss: 296.0318 - val_mse: 296.0318 - val_mae: 14.0960\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 359.0768 - mse: 359.0768 - mae: 14.9423 - val_loss: 294.1336 - val_mse: 294.1336 - val_mae: 13.9918\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 349.9505 - mse: 349.9505 - mae: 14.7876 - val_loss: 292.6299 - val_mse: 292.6299 - val_mae: 13.9109\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 342.0418 - mse: 342.0418 - mae: 14.6661 - val_loss: 291.3387 - val_mse: 291.3387 - val_mae: 13.8437\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 335.0443 - mse: 335.0443 - mae: 14.5421 - val_loss: 290.3225 - val_mse: 290.3225 - val_mae: 13.7970\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 328.6031 - mse: 328.6031 - mae: 14.4337 - val_loss: 289.4657 - val_mse: 289.4657 - val_mae: 13.7580\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 322.9350 - mse: 322.9350 - mae: 14.3466 - val_loss: 288.6493 - val_mse: 288.6493 - val_mae: 13.7044\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 318.5599 - mse: 318.5599 - mae: 14.2482 - val_loss: 287.9874 - val_mse: 287.9874 - val_mae: 13.6673\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 314.3490 - mse: 314.3490 - mae: 14.1718 - val_loss: 287.3705 - val_mse: 287.3705 - val_mae: 13.6227\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 310.8957 - mse: 310.8957 - mae: 14.0929 - val_loss: 286.8632 - val_mse: 286.8632 - val_mae: 13.5894\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 307.8823 - mse: 307.8823 - mae: 14.0302 - val_loss: 286.3981 - val_mse: 286.3981 - val_mae: 13.5590\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 305.0965 - mse: 305.0965 - mae: 13.9701 - val_loss: 285.9870 - val_mse: 285.9870 - val_mae: 13.5292\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 302.7660 - mse: 302.7660 - mae: 13.9080 - val_loss: 285.6324 - val_mse: 285.6324 - val_mae: 13.5152\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 300.5938 - mse: 300.5938 - mae: 13.8667 - val_loss: 285.2978 - val_mse: 285.2978 - val_mae: 13.4955\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 298.7089 - mse: 298.7089 - mae: 13.8176 - val_loss: 284.9834 - val_mse: 284.9834 - val_mae: 13.4663\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 297.0237 - mse: 297.0237 - mae: 13.7675 - val_loss: 284.6985 - val_mse: 284.6985 - val_mae: 13.4481\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.6127 - mse: 295.6127 - mae: 13.7270 - val_loss: 284.4533 - val_mse: 284.4533 - val_mae: 13.4376\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.2698 - mse: 294.2698 - mae: 13.6897 - val_loss: 284.2158 - val_mse: 284.2158 - val_mae: 13.4255\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.0458 - mse: 293.0458 - mae: 13.6470 - val_loss: 283.9971 - val_mse: 283.9971 - val_mae: 13.4165\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.0114 - mse: 292.0114 - mae: 13.6222 - val_loss: 283.7774 - val_mse: 283.7774 - val_mae: 13.3995\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.1696 - mse: 291.1696 - mae: 13.5929 - val_loss: 283.5830 - val_mse: 283.5830 - val_mae: 13.3861\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.3491 - mse: 290.3491 - mae: 13.5583 - val_loss: 283.4079 - val_mse: 283.4079 - val_mae: 13.3803\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 289.7490 - mse: 289.7490 - mae: 13.5405 - val_loss: 283.2134 - val_mse: 283.2134 - val_mae: 13.3671\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.0706 - mse: 289.0706 - mae: 13.5153 - val_loss: 283.0375 - val_mse: 283.0375 - val_mae: 13.3564\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 288.5024 - mse: 288.5024 - mae: 13.4945 - val_loss: 282.8675 - val_mse: 282.8675 - val_mae: 13.3487\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.9209 - mse: 287.9209 - mae: 13.4740 - val_loss: 282.7216 - val_mse: 282.7216 - val_mae: 13.3437\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.3223 - mse: 287.3223 - mae: 13.4512 - val_loss: 282.5847 - val_mse: 282.5847 - val_mae: 13.3382\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 286.8445 - mse: 286.8445 - mae: 13.4372 - val_loss: 282.4543 - val_mse: 282.4543 - val_mae: 13.3310\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 286.3794 - mse: 286.3794 - mae: 13.4124 - val_loss: 282.3554 - val_mse: 282.3554 - val_mae: 13.3375\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.9368 - mse: 285.9368 - mae: 13.4072 - val_loss: 282.2214 - val_mse: 282.2214 - val_mae: 13.3321\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.5497 - mse: 285.5497 - mae: 13.3934 - val_loss: 282.0906 - val_mse: 282.0906 - val_mae: 13.3238\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 285.1866 - mse: 285.1866 - mae: 13.3758 - val_loss: 281.9904 - val_mse: 281.9904 - val_mae: 13.3221\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.8957 - mse: 284.8957 - mae: 13.3674 - val_loss: 281.8787 - val_mse: 281.8787 - val_mae: 13.3154\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.6545 - mse: 284.6545 - mae: 13.3606 - val_loss: 281.7505 - val_mse: 281.7505 - val_mae: 13.3047\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.3718 - mse: 284.3718 - mae: 13.3374 - val_loss: 281.6433 - val_mse: 281.6433 - val_mae: 13.3010\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 284.1562 - mse: 284.1562 - mae: 13.3299 - val_loss: 281.5517 - val_mse: 281.5517 - val_mae: 13.2988\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.9619 - mse: 283.9619 - mae: 13.3227 - val_loss: 281.4570 - val_mse: 281.4570 - val_mae: 13.2927\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.7484 - mse: 283.7484 - mae: 13.3151 - val_loss: 281.3480 - val_mse: 281.3480 - val_mae: 13.2850\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.5782 - mse: 283.5782 - mae: 13.3023 - val_loss: 281.2580 - val_mse: 281.2580 - val_mae: 13.2818\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.3763 - mse: 283.3763 - mae: 13.2870 - val_loss: 281.1933 - val_mse: 281.1933 - val_mae: 13.2871\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.2036 - mse: 283.2036 - mae: 13.2851 - val_loss: 281.1244 - val_mse: 281.1244 - val_mae: 13.2884\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 283.0604 - mse: 283.0604 - mae: 13.2768 - val_loss: 281.0363 - val_mse: 281.0363 - val_mae: 13.2836\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 282.9291 - mse: 282.9291 - mae: 13.2719 - val_loss: 280.9296 - val_mse: 280.9296 - val_mae: 13.2769\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 282.7875 - mse: 282.7875 - mae: 13.2593 - val_loss: 280.8487 - val_mse: 280.8487 - val_mae: 13.2750\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 282.6789 - mse: 282.6789 - mae: 13.2560 - val_loss: 280.7775 - val_mse: 280.7775 - val_mae: 13.2722\n",
      "Epoch 58: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 670]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp2o7x1xe3/assets\n",
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 24492.2461 - mse: 24492.2461 - mae: 74.6546"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2246 - mse: 23399.2246 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23399.2227 - mse: 23399.2227 - mae: 72.8680 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmppxt6ehni/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 23560.8262 - mse: 23560.8262 - mae: 73.5918"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9805 - mse: 23112.9805 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9844 - mse: 23112.9844 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23112.9824 - mse: 23112.9824 - mae: 72.6047 - val_loss: 24142.5371 - val_mse: 24142.5371 - val_mae: 71.7670\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmp10xchbqj/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [135, 1755]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 21635.4844 - mse: 21635.4844 - mae: 71.4221"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 15ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8223 - mse: 23041.8223 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8262 - mse: 23041.8262 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 23041.8242 - mse: 23041.8242 - mae: 71.6140 - val_loss: 23629.0918 - val_mse: 23629.0918 - val_mae: 73.5344\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "INFO:tensorflow:Assets written to: ram:///var/folders/zz/zyxvpxvq6csfxvn_n00000s4000069/T/tmpjfomeuan/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/Guest/.local/lib/python3.9/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 789, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/metrics/_regression.py\", line 94, in _check_reg_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/sklearn/utils/validation.py\", line 332, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [134, 1742]\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ml/lib/python3.9/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 22ms/step - loss: 617.8920 - mse: 617.8920 - mae: 21.1290 - val_loss: 348.5784 - val_mse: 348.5784 - val_mae: 16.4508\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 418.8069 - mse: 418.8069 - mae: 17.3975 - val_loss: 291.3195 - val_mse: 291.3195 - val_mae: 14.7326\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 381.9932 - mse: 381.9932 - mae: 16.1968 - val_loss: 271.1105 - val_mse: 271.1105 - val_mae: 13.9825\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 366.9913 - mse: 366.9913 - mae: 15.6508 - val_loss: 259.9752 - val_mse: 259.9752 - val_mae: 13.5741\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 353.4204 - mse: 353.4204 - mae: 15.2211 - val_loss: 248.1113 - val_mse: 248.1113 - val_mae: 13.1732\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 341.5623 - mse: 341.5623 - mae: 14.8655 - val_loss: 240.7906 - val_mse: 240.7906 - val_mae: 12.9234\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 335.3733 - mse: 335.3733 - mae: 14.6564 - val_loss: 237.9510 - val_mse: 237.9510 - val_mae: 12.8025\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 332.4570 - mse: 332.4570 - mae: 14.5558 - val_loss: 236.1310 - val_mse: 236.1310 - val_mae: 12.7044\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 329.9995 - mse: 329.9995 - mae: 14.4645 - val_loss: 235.0709 - val_mse: 235.0709 - val_mae: 12.6411\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 328.2261 - mse: 328.2261 - mae: 14.4099 - val_loss: 233.2555 - val_mse: 233.2555 - val_mae: 12.5420\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 326.9040 - mse: 326.9040 - mae: 14.3412 - val_loss: 232.3158 - val_mse: 232.3158 - val_mae: 12.4762\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 325.7537 - mse: 325.7537 - mae: 14.2934 - val_loss: 231.1603 - val_mse: 231.1603 - val_mae: 12.4135\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 324.6336 - mse: 324.6336 - mae: 14.2372 - val_loss: 230.9931 - val_mse: 230.9931 - val_mae: 12.4203\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 324.0786 - mse: 324.0786 - mae: 14.2460 - val_loss: 229.2186 - val_mse: 229.2186 - val_mae: 12.3157\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 321.5492 - mse: 321.5492 - mae: 14.1424 - val_loss: 225.0750 - val_mse: 225.0750 - val_mae: 12.1589\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 312.7343 - mse: 312.7343 - mae: 13.9173 - val_loss: 217.4028 - val_mse: 217.4028 - val_mae: 11.8901\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 306.8797 - mse: 306.8797 - mae: 13.7094 - val_loss: 215.5056 - val_mse: 215.5056 - val_mae: 11.8239\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 304.6153 - mse: 304.6153 - mae: 13.6163 - val_loss: 214.6807 - val_mse: 214.6807 - val_mae: 11.7859\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 303.3480 - mse: 303.3480 - mae: 13.5357 - val_loss: 214.4098 - val_mse: 214.4098 - val_mae: 11.7802\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 302.7024 - mse: 302.7024 - mae: 13.5292 - val_loss: 213.4216 - val_mse: 213.4216 - val_mae: 11.7108\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 301.8848 - mse: 301.8848 - mae: 13.5232 - val_loss: 212.0055 - val_mse: 212.0055 - val_mae: 11.6469\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 301.3747 - mse: 301.3747 - mae: 13.4648 - val_loss: 210.9498 - val_mse: 210.9498 - val_mae: 11.5835\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 300.6013 - mse: 300.6013 - mae: 13.4033 - val_loss: 211.3198 - val_mse: 211.3198 - val_mae: 11.5938\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 300.1604 - mse: 300.1604 - mae: 13.4243 - val_loss: 210.5225 - val_mse: 210.5225 - val_mae: 11.5324\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 299.5292 - mse: 299.5292 - mae: 13.4127 - val_loss: 209.6685 - val_mse: 209.6685 - val_mae: 11.4960\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 299.0097 - mse: 299.0097 - mae: 13.3537 - val_loss: 209.5054 - val_mse: 209.5054 - val_mae: 11.4676\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 298.7063 - mse: 298.7063 - mae: 13.3531 - val_loss: 208.8905 - val_mse: 208.8905 - val_mae: 11.4267\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 298.4797 - mse: 298.4797 - mae: 13.3465 - val_loss: 208.1514 - val_mse: 208.1514 - val_mae: 11.3962\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 297.7880 - mse: 297.7880 - mae: 13.2930 - val_loss: 208.4973 - val_mse: 208.4973 - val_mae: 11.4255\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 297.3378 - mse: 297.3378 - mae: 13.3230 - val_loss: 207.0544 - val_mse: 207.0544 - val_mae: 11.3299\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 296.9958 - mse: 296.9958 - mae: 13.2710 - val_loss: 206.3150 - val_mse: 206.3150 - val_mae: 11.2883\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 296.6534 - mse: 296.6534 - mae: 13.2203 - val_loss: 207.1094 - val_mse: 207.1094 - val_mae: 11.3482\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 296.3914 - mse: 296.3914 - mae: 13.2632 - val_loss: 206.8224 - val_mse: 206.8224 - val_mae: 11.3216\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.8921 - mse: 295.8921 - mae: 13.2316 - val_loss: 206.6744 - val_mse: 206.6744 - val_mae: 11.3005\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.7484 - mse: 295.7484 - mae: 13.2234 - val_loss: 206.3169 - val_mse: 206.3169 - val_mae: 11.2925\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.4503 - mse: 295.4503 - mae: 13.2338 - val_loss: 205.1750 - val_mse: 205.1750 - val_mae: 11.2300\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 295.0650 - mse: 295.0650 - mae: 13.1876 - val_loss: 204.9251 - val_mse: 204.9250 - val_mae: 11.2092\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.9581 - mse: 294.9581 - mae: 13.1816 - val_loss: 204.3854 - val_mse: 204.3854 - val_mae: 11.1711\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.5896 - mse: 294.5896 - mae: 13.1450 - val_loss: 204.9766 - val_mse: 204.9766 - val_mae: 11.2315\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.5300 - mse: 294.5300 - mae: 13.1461 - val_loss: 205.3429 - val_mse: 205.3429 - val_mae: 11.2361\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 294.1361 - mse: 294.1361 - mae: 13.2019 - val_loss: 204.1421 - val_mse: 204.1421 - val_mae: 11.1621\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.8338 - mse: 293.8338 - mae: 13.1444 - val_loss: 203.8333 - val_mse: 203.8333 - val_mae: 11.1504\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.7010 - mse: 293.7010 - mae: 13.1372 - val_loss: 203.3143 - val_mse: 203.3143 - val_mae: 11.1159\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.3846 - mse: 293.3846 - mae: 13.1257 - val_loss: 203.2783 - val_mse: 203.2783 - val_mae: 11.1153\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 293.2720 - mse: 293.2720 - mae: 13.1457 - val_loss: 202.9237 - val_mse: 202.9237 - val_mae: 11.1002\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.9888 - mse: 292.9888 - mae: 13.1033 - val_loss: 202.6938 - val_mse: 202.6938 - val_mae: 11.0917\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 293.2392 - mse: 293.2392 - mae: 13.0926 - val_loss: 202.6678 - val_mse: 202.6678 - val_mae: 11.0789\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.6843 - mse: 292.6843 - mae: 13.0881 - val_loss: 202.9057 - val_mse: 202.9057 - val_mae: 11.0869\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.3763 - mse: 292.3763 - mae: 13.0770 - val_loss: 203.2654 - val_mse: 203.2654 - val_mae: 11.1118\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.3336 - mse: 292.3336 - mae: 13.0983 - val_loss: 202.4821 - val_mse: 202.4821 - val_mae: 11.0640\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.0554 - mse: 292.0554 - mae: 13.0533 - val_loss: 202.7181 - val_mse: 202.7181 - val_mae: 11.0744\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 292.1488 - mse: 292.1488 - mae: 13.0984 - val_loss: 202.1922 - val_mse: 202.1922 - val_mae: 11.0508\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.8912 - mse: 291.8912 - mae: 13.0520 - val_loss: 202.3618 - val_mse: 202.3618 - val_mae: 11.0630\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.7351 - mse: 291.7351 - mae: 13.1039 - val_loss: 200.8876 - val_mse: 200.8876 - val_mae: 10.9641\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.6424 - mse: 291.6424 - mae: 13.0043 - val_loss: 201.6896 - val_mse: 201.6896 - val_mae: 11.0197\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.3185 - mse: 291.3185 - mae: 13.0437 - val_loss: 201.3369 - val_mse: 201.3369 - val_mae: 10.9997\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.1462 - mse: 291.1462 - mae: 13.0210 - val_loss: 201.2362 - val_mse: 201.2362 - val_mae: 10.9961\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.1686 - mse: 291.1686 - mae: 13.0214 - val_loss: 201.6578 - val_mse: 201.6578 - val_mae: 11.0162\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.0408 - mse: 291.0408 - mae: 13.0656 - val_loss: 200.3662 - val_mse: 200.3662 - val_mae: 10.9373\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.8531 - mse: 290.8531 - mae: 12.9996 - val_loss: 200.6329 - val_mse: 200.6329 - val_mae: 10.9573\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 291.0935 - mse: 291.0935 - mae: 13.0067 - val_loss: 200.4590 - val_mse: 200.4590 - val_mae: 10.9471\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.4638 - mse: 290.4638 - mae: 13.0206 - val_loss: 199.8082 - val_mse: 199.8082 - val_mae: 10.9024\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.5026 - mse: 290.5026 - mae: 12.9863 - val_loss: 199.8605 - val_mse: 199.8605 - val_mae: 10.9080\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.3683 - mse: 290.3683 - mae: 12.9778 - val_loss: 200.4169 - val_mse: 200.4169 - val_mae: 10.9518\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 290.3226 - mse: 290.3226 - mae: 12.9941 - val_loss: 199.8199 - val_mse: 199.8199 - val_mae: 10.9056\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.9890 - mse: 289.9890 - mae: 12.9781 - val_loss: 199.5317 - val_mse: 199.5317 - val_mae: 10.8954\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 289.6098 - mse: 289.6098 - mae: 12.9600 - val_loss: 199.2010 - val_mse: 199.2010 - val_mae: 10.8913\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 287.4714 - mse: 287.4714 - mae: 12.9356 - val_loss: 196.2352 - val_mse: 196.2352 - val_mae: 10.8036\n",
      "Epoch 69/100\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 297.8040 - mse: 297.8040 - mae: 13.1159"
     ]
    }
   ],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(keras_reg,param_distribs,n_iter=30,cv=3,verbose=1)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d980e-4b5b-42a2-b87b-2121263c7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87532508-d7ae-4bca-b138-f8c2f5eec359",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnd_search.pkl','wb') as f:\n",
    "    pickle.dump(rnd_search_cv.best_params_,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76774f-4546-4377-86a5-1f3aff6e73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4551c82-34e0-4784-96bd-f16deb4a477e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
